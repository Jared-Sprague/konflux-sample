<!DOCTYPE html>
<html>
    <head>
        
        <link rel="stylesheet" type="text/css" media="screen" href="/mimir/styles/mimir.min.css">
        




    <link rel="stylesheet" type="text/css" media="screen" href="/sites/dxp-docs/files/css/css_87GMcmxT1ib8ziQiU2KUAnTDFtZQV6iP-KGslA9LigM.css" />

    <link rel="stylesheet" type="text/css" media="screen" href="/sites/dxp-docs/files/css/css_Bwzy34i9pSdDlzGALvqVwG4fpgGp08KLMAkClGNY9M4.css" />



<link rel="stylesheet" type="text/css" media="screen" href="/mimir/styles/mimir-doc-toc.min.css" />



        
        <!--#include virtual="/includes/head/index.html" -->

        
        <title>
Installing OpenShift Container Platform with the Assisted Installer - Assisted Installer for OpenShift Container Platform 2024
</title>
        
<meta name="product" content="Assisted Installer for OpenShift Container Platform" />
<meta name="documentation_version" content="2024" />
<meta name="documentKind" content="documentation" />
<meta name="portal_content_subtype" content="title" />
<meta name="lastModifiedDate" content="2024-08-15T19:58:04.000Z" />


        
            
        
        
            
                
                
                <!-- mimir_solr_yesindex -->
                <meta name="mimir_solr_yesindex" content="true" />
            
        
    </head>

    <body class="mimir-body">

        
        

        
        <div id="page-wrap" class="page-wrap">
            <div id="pers-top-page-wrap" class="top-page-wrap pers-loader-bg">
                <div id="hero-bg-top-left" class="summit-bg-shapes"></div>
                <div id="hero-bg-top-right" class="summit-bg-shapes"></div>

                <header class="masthead" id="masthead">

                    
                    <!--#include virtual="/includes/header/index.html" -->

                    
                    
                    
                        
                    
                    
                        <div class="breadcrumbs">
                            <div id="breadcrumbs" class="container">
                                
                                <a href="/">Home</a>
                                
                                <a href="/products/">Product Documentation</a>
                                
                                <a href="/documentation/en-us/assisted_installer_for_openshift_container_platform/2024/">Assisted Installer for OpenShift Container Platform</a>
                                
                                <a href="/documentation/en-us/assisted_installer_for_openshift_container_platform/2024/">2024</a>
                                
                                
                                Installing OpenShift Container Platform with the Assisted Installer
                                
                            </div>
                        </div>
                    
                </header>

                <main id="cp-main" class="portal-content-area">
                    <div id="cp-content" class="main-content">
                        
                        <main class="container mimir-docs">
                            
                            <bdo>
                                

<script type="module" src="/mimir/scripts/mimir-doc-toc.min.js"></script>






<div class="docs-grid">

    <nav id="mimir-doc-toc" class="mimir-doc-toc">
      <div class="mimir-doc-toc-inner">
          <!-- single-page -->
        
            
              <ol>
                <li>
                        
                        <a href="#">Installing OpenShift Container Platform with the Assisted Installer</a>
                    </li><li>
                        <a href="#idm139800925707712">
                            Preface
                        </a>
                    </li><li>
                        <a href="#about-ai">
                            1. About the Assisted Installer
                        </a><ol>
                <li>
                        <a href="#ai-features_about-ai">
                            1.1. Features
                        </a>
                    </li><li>
                        <a href="#customizing-installation_about-ai">
                            1.2. Customizing your installation
                        </a>
                    </li><li>
                        <a href="#api-support-policy">
                            1.3. API support policy
                        </a>
                    </li>
            </ol>
                    </li><li>
                        <a href="#prerequisites">
                            2. Prerequisites
                        </a><ol>
                <li>
                        <a href="#supported_cpu_architectures">
                            2.1. Supported CPU architectures
                        </a>
                    </li><li>
                        <a href="#resource_requirements">
                            2.2. Resource requirements
                        </a><ol>
                <li>
                        <a href="#multi-node-cluster-requirements_prerequisites">
                            2.2.1. Multi-node cluster resource requirements
                        </a>
                    </li><li>
                        <a href="#sno-requirements_prerequisites">
                            2.2.2. Single-node OpenShift resource requirements
                        </a>
                    </li>
            </ol>
                    </li><li>
                        <a href="#networking_requirements">
                            2.3. Networking requirements
                        </a><ol>
                <li>
                        <a href="#general_networking_requirements">
                            2.3.1. General networking requirements
                        </a><ol>
                <li>
                        <a href="#example_dns_configuration">
                            2.3.1.1. Example DNS configuration
                        </a>
                    </li><li>
                        <a href="#example_dns_a_record_configuration">
                            2.3.1.2. Example DNS A record configuration
                        </a>
                    </li><li>
                        <a href="#example_dns_ptr_record_configuration">
                            2.3.1.3. Example DNS PTR record configuration
                        </a>
                    </li>
            </ol>
                    </li><li>
                        <a href="#networking_requirements_ibmz">
                            2.3.2. Networking requirements for IBM Z
                        </a><ol>
                <li>
                        <a href="#configuring-network-overrides-ibmz_prerequisites">
                            2.3.2.1. Configuring network overrides in IBM Z
                        </a>
                    </li>
            </ol>
                    </li>
            </ol>
                    </li><li>
                        <a href="#preflight_validations">
                            2.4. Preflight validations
                        </a>
                    </li>
            </ol>
                    </li><li>
                        <a href="#installing-with-ui">
                            3. Installing with the Assisted Installer web console
                        </a><ol>
                <li>
                        <a href="#preinstallation-considerations_installing-with-ui">
                            3.1. Preinstallation considerations
                        </a>
                    </li><li>
                        <a href="#setting-the-cluster-details_installing-with-ui">
                            3.2. Setting the cluster details
                        </a>
                    </li><li>
                        <a href="#configuring-static-networks_installing-with-ui">
                            3.3. Optional: Configuring static networks
                        </a>
                    </li><li>
                        <a href="#configuring-operators_installing-with-ui">
                            3.4. Optional: Installing Operators
                        </a>
                    </li><li>
                        <a href="#adding-hosts-to-the-cluster_installing-with-ui">
                            3.5. Adding hosts to the cluster
                        </a>
                    </li><li>
                        <a href="#configuring-hosts_installing-with-ui">
                            3.6. Configuring hosts
                        </a>
                    </li><li>
                        <a href="#configuring-storage_installing-with-ui">
                            3.7. Configuring storage disks
                        </a>
                    </li><li>
                        <a href="#configuring-networking_installing-with-ui">
                            3.8. Configuring networking
                        </a>
                    </li><li>
                        <a href="#adding-custom-manifests_installing-with-ui">
                            3.9. Adding custom manifests
                        </a>
                    </li><li>
                        <a href="#preinstallation-validations_ui">
                            3.10. Preinstallation validations
                        </a>
                    </li><li>
                        <a href="#installing-the-cluster_installing-with-ui">
                            3.11. Installing the cluster
                        </a>
                    </li><li>
                        <a href="#completing-the-installation_installing-with-ui">
                            3.12. Completing the installation
                        </a>
                    </li>
            </ol>
                    </li><li>
                        <a href="#installing-with-api">
                            4. Installing with the Assisted Installer API
                        </a><ol>
                <li>
                        <a href="#generating-the-offline-token-cli_installing-with-api">
                            4.1. Generating the offline token
                        </a>
                    </li><li>
                        <a href="#authenticating-with-the-rest-api_installing-with-api">
                            4.2. Authenticating with the REST API
                        </a>
                    </li><li>
                        <a href="#configuring-the-pull-secret_installing-with-api">
                            4.3. Configuring the pull secret
                        </a>
                    </li><li>
                        <a href="#generating-the-ssh-public-key_installing-with-api">
                            4.4. Optional: Generating the SSH public key
                        </a>
                    </li><li>
                        <a href="#registering-a-new-cluster_installing-with-api">
                            4.5. Registering a new cluster
                        </a><ol>
                <li>
                        <a href="#installing-operators-api_installing-with-api">
                            4.5.1. Optional: Installing Operators
                        </a>
                    </li>
            </ol>
                    </li><li>
                        <a href="#modifying-a-cluster_installing-with-api">
                            4.6. Modifying a cluster
                        </a><ol>
                <li>
                        <a href="#modifying-operators-api_installing-with-api">
                            4.6.1. Modifying Operators
                        </a>
                    </li>
            </ol>
                    </li><li>
                        <a href="#registering-a-new-infrastructure-environment_installing-with-api">
                            4.7. Registering a new infrastructure environment
                        </a>
                    </li><li>
                        <a href="#modifying-an-infrastructure-environment_installing-with-api">
                            4.8. Modifying an infrastructure environment
                        </a><ol>
                <li>
                        <a href="#adding-kernel-arguments_installing-with-api">
                            4.8.1. Optional: Adding kernel arguments
                        </a>
                    </li>
            </ol>
                    </li><li>
                        <a href="#adding-hosts_installing-with-api">
                            4.9. Adding hosts
                        </a>
                    </li><li>
                        <a href="#modifying-hosts_installing-with-api">
                            4.10. Modifying hosts
                        </a><ol>
                <li>
                        <a href="#modifying-storage-configuration_installing-with-api">
                            4.10.1. Modifying storage disk configuration
                        </a>
                    </li>
            </ol>
                    </li><li>
                        <a href="#adding-custom-manifests_installing-with-api">
                            4.11. Adding custom manifests
                        </a>
                    </li><li>
                        <a href="#preinstallation-validations_api">
                            4.12. Preinstallation validations
                        </a>
                    </li><li>
                        <a href="#installing-the-cluster_installing-with-api">
                            4.13. Installing the cluster
                        </a>
                    </li>
            </ol>
                    </li><li>
                        <a href="#assembly_enabling-disk-encryption">
                            5. Optional: Enabling disk encryption
                        </a><ol>
                <li>
                        <a href="#proc_enabling-tpm-v2-encryption_enabling-disk-encryption">
                            5.1. Enabling TPM v2 encryption
                        </a>
                    </li><li>
                        <a href="#proc_enabling-tang-encryption_enabling-disk-encryption">
                            5.2. Enabling Tang encryption
                        </a>
                    </li><li>
                        <a href="#additional_resources">
                            5.3. Additional resources
                        </a>
                    </li>
            </ol>
                    </li><li>
                        <a href="#assembly_configuring-schedulable-control-planes">
                            6. Optional: Configuring schedulable control plane nodes
                        </a><ol>
                <li>
                        <a href="#configuring-schedulable-control-planes-ui_role-assignment">
                            6.1. Configuring schedulable control planes using the web console
                        </a>
                    </li><li>
                        <a href="#configuring-schedulable-control-planes-api_role-assignment">
                            6.2. Configuring schedulable control planes using the API
                        </a>
                    </li><li>
                        <a href="#additional_resources_2">
                            6.3. Additional resources
                        </a>
                    </li>
            </ol>
                    </li><li>
                        <a href="#assembly_configuring-the-discovery-image">
                            7. Configuring the discovery image
                        </a><ol>
                <li>
                        <a href="#creating-an-ignition-configuration-file_configuring-the-discovery-image">
                            7.1. Creating an Ignition configuration file
                        </a>
                    </li><li>
                        <a href="#modifying-the-discovery-image-with-ignition_configuring-the-discovery-image">
                            7.2. Modifying the discovery image with Ignition
                        </a>
                    </li>
            </ol>
                    </li><li>
                        <a href="#assembly_booting-hosts-with-the-discovery-image">
                            8. Booting hosts with the discovery image
                        </a><ol>
                <li>
                        <a href="#creating-an-iso-image-on-a-usb-drive_booting-hosts-with-the-discovery-image">
                            8.1. Creating an ISO image on a USB drive
                        </a>
                    </li><li>
                        <a href="#booting-with-a-usb-drive_booting-hosts-with-the-discovery-image">
                            8.2. Booting with a USB drive
                        </a>
                    </li><li>
                        <a href="#install-booting-from-an-iso-over-http-redfish_booting-hosts-with-the-discovery-image">
                            8.3. Booting from an HTTP-hosted ISO image using the Redfish API
                        </a>
                    </li><li>
                        <a href="#booting-hosts-using-ipxe_booting-hosts-with-the-discovery-image">
                            8.4. Booting hosts using iPXE
                        </a>
                    </li>
            </ol>
                    </li><li>
                        <a href="#assembly_role-assignment">
                            9. Assigning roles to hosts
                        </a><ol>
                <li>
                        <a href="#selecting-role-web_role-assignment">
                            9.1. Selecting a role by using the web console
                        </a>
                    </li><li>
                        <a href="#selecting-a-role-using-the-api_role-assignment">
                            9.2. Selecting a role by using the API
                        </a>
                    </li><li>
                        <a href="#con_auto-assign-role_role-assignment">
                            9.3. Auto-assigning roles
                        </a>
                    </li><li>
                        <a href="#additional_resources_3">
                            9.4. Additional resources
                        </a>
                    </li>
            </ol>
                    </li><li>
                        <a href="#assembly_preinstallation-validations">
                            10. Preinstallation validations
                        </a><ol>
                <li>
                        <a href="#definition-of-preinstallation-validations_preinstallation-validations">
                            10.1. Definition of preinstallation validations
                        </a>
                    </li><li>
                        <a href="#blocking-and-non-blocking-validations_preinstallation-validations">
                            10.2. Blocking and non-blocking validations
                        </a>
                    </li><li>
                        <a href="#validation-types_preinstallation-validations">
                            10.3. Validation types
                        </a>
                    </li><li>
                        <a href="#host_validations">
                            10.4. Host validations
                        </a><ol>
                <li>
                        <a href="#getting-host-validations-by-using-rest-api_preinstallation-validations">
                            10.4.1. Getting host validations by using the REST API
                        </a>
                    </li><li>
                        <a href="#host-validations-in-detail_preinstallation-validations">
                            10.4.2. Host validations in detail
                        </a>
                    </li>
            </ol>
                    </li><li>
                        <a href="#cluster_validations">
                            10.5. Cluster validations
                        </a><ol>
                <li>
                        <a href="#getting-cluster-validations-by-using-rest-api_preinstallation-validations">
                            10.5.1. Getting cluster validations by using the REST API
                        </a>
                    </li><li>
                        <a href="#cluster-validations-in-detail_preinstallation-validations">
                            10.5.2. Cluster validations in detail
                        </a>
                    </li>
            </ol>
                    </li>
            </ol>
                    </li><li>
                        <a href="#assembly_network-configuration">
                            11. Network configuration
                        </a><ol>
                <li>
                        <a href="#cluster-networking_network-configuration">
                            11.1. Cluster networking
                        </a><ol>
                <li>
                        <a href="#limitations">
                            11.1.1. Limitations
                        </a><ol>
                <li>
                        <a href="#sdn">
                            11.1.1.1. SDN
                        </a>
                    </li><li>
                        <a href="#ovn_kubernetes">
                            11.1.1.2. OVN-Kubernetes
                        </a>
                    </li>
            </ol>
                    </li><li>
                        <a href="#cluster_network">
                            11.1.2. Cluster network
                        </a>
                    </li><li>
                        <a href="#machine_network">
                            11.1.3. Machine network
                        </a>
                    </li><li>
                        <a href="#single_node_openshift_compared_to_multi_node_cluster">
                            11.1.4. Single-node OpenShift compared to multi-node cluster
                        </a>
                    </li><li>
                        <a href="#air_gapped_environments">
                            11.1.5. Air-gapped environments
                        </a>
                    </li>
            </ol>
                    </li><li>
                        <a href="#vip-dhcp-allocation_network-configuration">
                            11.2. VIP DHCP allocation
                        </a><ol>
                <li>
                        <a href="#example_payload_to_enable_autoallocation">
                            11.2.1. Example payload to enable autoallocation
                        </a>
                    </li><li>
                        <a href="#example_payload_to_disable_autoallocation">
                            11.2.2. Example payload to disable autoallocation
                        </a>
                    </li>
            </ol>
                    </li><li>
                        <a href="#additional_resources_4">
                            11.3. Additional resources
                        </a>
                    </li><li>
                        <a href="#understanding-differences-between-user-and-cluster-managed-networking_network-configuration">
                            11.4. Understanding differences between user- and cluster-managed networking
                        </a><ol>
                <li>
                        <a href="#validations">
                            11.4.1. Validations
                        </a>
                    </li>
            </ol>
                    </li><li>
                        <a href="#static-network-configuration_network-configuration">
                            11.5. Static network configuration
                        </a><ol>
                <li>
                        <a href="#prerequisites-1">
                            11.5.1. Prerequisites
                        </a>
                    </li><li>
                        <a href="#nmstate_configuration">
                            11.5.2. NMState configuration
                        </a><ol>
                <li>
                        <a href="#example_of_nmstate_configuration">
                            11.5.2.1. Example of NMState configuration
                        </a>
                    </li>
            </ol>
                    </li><li>
                        <a href="#mac_interface_mapping">
                            11.5.3. MAC interface mapping
                        </a><ol>
                <li>
                        <a href="#example_of_mac_interface_mapping">
                            11.5.3.1. Example of MAC interface mapping
                        </a>
                    </li>
            </ol>
                    </li><li>
                        <a href="#additional_nmstate_configuration_examples">
                            11.5.4. Additional NMState configuration examples
                        </a><ol>
                <li>
                        <a href="#tagged_vlan">
                            11.5.4.1. Tagged VLAN
                        </a>
                    </li><li>
                        <a href="#network_bond">
                            11.5.4.2. Network bond
                        </a>
                    </li>
            </ol>
                    </li>
            </ol>
                    </li><li>
                        <a href="#applying-static-network-configuration_network-configuration">
                            11.6. Applying a static network configuration with the API
                        </a>
                    </li><li>
                        <a href="#additional_resources_5">
                            11.7. Additional resources
                        </a>
                    </li><li>
                        <a href="#converting-to-dual-stack-networking_network-configuration">
                            11.8. Converting to dual-stack networking
                        </a><ol>
                <li>
                        <a href="#prerequisites_2">
                            11.8.1. Prerequisites
                        </a>
                    </li><li>
                        <a href="#example_payload_for_single_node_openshift">
                            11.8.2. Example payload for single-node OpenShift
                        </a>
                    </li><li>
                        <a href="#example_payload_for_an_openshift_container_platform_cluster_consisting_of_many_nodes">
                            11.8.3. Example payload for an OpenShift Container Platform cluster consisting of many nodes
                        </a>
                    </li><li>
                        <a href="#limitations_2">
                            11.8.4. Limitations
                        </a>
                    </li>
            </ol>
                    </li><li>
                        <a href="#additional_resources_6">
                            11.9. Additional resources
                        </a>
                    </li>
            </ol>
                    </li><li>
                        <a href="#expanding-the-cluster">
                            12. Expanding the cluster
                        </a><ol>
                <li>
                        <a href="#checking-multi-architecture-support">
                            12.1. Checking for multi-architecture support
                        </a>
                    </li><li>
                        <a href="#installing-mixed-architecture-clusters_expanding-the-cluster">
                            12.2. Installing a multi-architecture cluster
                        </a>
                    </li><li>
                        <a href="#adding-hosts-with-the-ui_expanding-the-cluster">
                            12.3. Adding hosts with the web console
                        </a>
                    </li><li>
                        <a href="#adding-hosts-with-the-api_expanding-the-cluster">
                            12.4. Adding hosts with the API
                        </a>
                    </li><li>
                        <a href="#installing-primary-control-plane-node-healthy-cluster_expanding-the-cluster">
                            12.5. Installing a primary control plane node on a healthy cluster
                        </a>
                    </li><li>
                        <a href="#installing-primary-control-plane-node-unhealthy-cluster_expanding-the-cluster">
                            12.6. Installing a primary control plane node on an unhealthy cluster
                        </a>
                    </li><li>
                        <a href="#additional_resources_7">
                            12.7. Additional resources
                        </a>
                    </li>
            </ol>
                    </li><li>
                        <a href="#assembly_installing-on-nutanix">
                            13. Optional: Installing on Nutanix
                        </a><ol>
                <li>
                        <a href="#adding-hosts-on-nutanix-with-the-ui_installing-on-nutanix">
                            13.1. Adding hosts on Nutanix with the UI
                        </a>
                    </li><li>
                        <a href="#adding-hosts-on-nutanix-with-the-api_installing-on-nutanix">
                            13.2. Adding hosts on Nutanix with the API
                        </a>
                    </li><li>
                        <a href="#nutanix-post-installation-configuration_installing-on-nutanix">
                            13.3. Nutanix postinstallation configuration
                        </a><ol>
                <li>
                        <a href="#updating-the-nutanix-configuration-settings_installing-on-nutanix">
                            13.3.1. Updating the Nutanix configuration settings
                        </a>
                    </li><li>
                        <a href="#creating-the-nutanix-csi-operator-group_installing-on-nutanix">
                            13.3.2. Creating the Nutanix CSI Operator group
                        </a>
                    </li><li>
                        <a href="#installing-the-nutanix-csi-operator_installing-on-nutanix">
                            13.3.3. Installing the Nutanix CSI Operator
                        </a>
                    </li><li>
                        <a href="#deploying-the-nutanix-csi-storage-driver_installing-on-nutanix">
                            13.3.4. Deploying the Nutanix CSI storage driver
                        </a>
                    </li><li>
                        <a href="#validating-the-post-installation-configurations_installing-on-nutanix">
                            13.3.5. Validating the postinstallation configurations
                        </a>
                    </li>
            </ol>
                    </li>
            </ol>
                    </li><li>
                        <a href="#installing-on-vsphere">
                            14. Optional: Installing on vSphere
                        </a><ol>
                <li>
                        <a href="#adding-hosts-on-vsphere_installing-on-vsphere">
                            14.1. Adding hosts on vSphere
                        </a>
                    </li><li>
                        <a href="#vsphere-post-installation-configuration_installing-on-vsphere">
                            14.2. vSphere postinstallation configuration using the CLI
                        </a>
                    </li><li>
                        <a href="#vsphere-post-installation-configuration-console_installing-on-vsphere">
                            14.3. vSphere postinstallation configuration using the web console
                        </a>
                    </li>
            </ol>
                    </li><li>
                        <a href="#installing-on-oci">
                            15. Optional: Installing on Oracle Cloud Infrastructure (OCI)
                        </a><ol>
                <li>
                        <a href="#oci_generating-discovery-iso_installing-on-oci">
                            15.1. Generating an OCI-compatible discovery ISO image
                        </a>
                    </li><li>
                        <a href="#oci-assigning-roles-and-manifests_installing-on-oci">
                            15.2. Assigning node roles and custom manifests
                        </a>
                    </li>
            </ol>
                    </li><li>
                        <a href="#assembly_troubleshooting">
                            16. Troubleshooting
                        </a><ol>
                <li>
                        <a href="#troubleshooting-discovery-iso-issues">
                            16.1. Troubleshooting discovery ISO issues
                        </a><ol>
                <li>
                        <a href="#proc_verify-the-discovery-agent-is-running_troubleshooting">
                            16.1.1. Verify the discovery agent is running
                        </a>
                    </li><li>
                        <a href="#proc_verify-the-discovery-agent-can-access-the-assisted-service_troubleshooting">
                            16.1.2. Verify the agent can access the assisted-service
                        </a>
                    </li>
            </ol>
                    </li><li>
                        <a href="#con_minimal-iso-image_troubleshooting">
                            16.2. Troubleshooting minimal discovery ISO issues
                        </a><ol>
                <li>
                        <a href="#interrupting-boot-before-mount-rootfs_troubleshooting">
                            16.2.1. Troubleshooting minimal ISO boot failure by interrupting the boot process
                        </a>
                    </li>
            </ol>
                    </li><li>
                        <a href="#proc_incorrect-boot-order_troubleshooting">
                            16.3. Correcting a host’s boot order
                        </a>
                    </li><li>
                        <a href="#proc_partially-successful-installations_troubleshooting">
                            16.4. Rectifying partially-successful installations
                        </a>
                    </li><li>
                        <a href="#api-connectivity-failure_troubleshooting">
                            16.5. API connectivity failure when adding nodes to a cluster
                        </a>
                    </li>
            </ol>
                    </li><li>
                        <a href="#idm139800925236160">
                            Legal Notice
                        </a>
                    </li>
            </ol>
            
        
      </div>
    </nav>

    
    <div class="pvof-doc__wrapper" id="doc-wrapper">
        <section class="mimir-doc-title" id="mimir-doc--installing_openshift_container_platform_with_the_assisted_installer">
            <h1 class="title">Installing OpenShift Container Platform with the Assisted Installer</h1>
        </section>
        <body><div xml:lang="en-US" class="book" id="idm139800917139104"><div class="titlepage"><div><div class="producttitle"><span class="productname">Assisted Installer for OpenShift Container Platform</span> <span class="productnumber">2024</span></div><div><h2 class="subtitle">User Guide</h2></div><div><div xml:lang="en-US" class="authorgroup"><span class="orgname">Red Hat Customer Content Services</span></div></div><div><a href="#idm139800925236160">Legal Notice</a></div><div><div class="abstract"><p class="title"><strong>Abstract</strong></p><div class="para">
				Information about the Assisted Installer and its usage
			</div></div></div></div><hr/></div><section class="preface" id="idm139800925707712"><div class="titlepage"><div><div><h1 class="title">Preface</h1></div></div></div><h2 id="making-open-source-more-inclusive">Making open source more inclusive</h2><p>
			Red Hat is committed to replacing problematic language in our code, documentation, and web properties. Because of the enormity of this endeavor, these changes are being updated gradually and where possible. For more details, see <a class="link mimir-link-warn" href="https://www.redhat.com/en/blog/making-open-source-more-inclusive-eradicating-problematic-language" title="Mimir does not include content from: www.redhat.com">our CTO Chris Wright’s message</a>.
		</p><h2 id="proc_providing-feedback-on-red-hat-documentation">Providing feedback on Red Hat documentation</h2><p>
			You can give feedback or report an error in the documentation by creating a Jira issue. You must have a Red Hat Jira account.
		</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
					Log in to <a class="link mimir-link-warn" href="https://issues.redhat.com" title="Mimir does not include content from: issues.redhat.com">Jira</a>.
				</li><li class="listitem">
					Click <a class="link mimir-link-warn" href="https://issues.redhat.com/secure/CreateIssueDetails!init.jspa?pid=12341520&amp;summary=Documentation+feedback&amp;issuetype=1&amp;description=Details:%0A%0ADocumentation+URL:%0A%0A&amp;priority=10200&amp;labels=hcidocs-feedback&amp;components=12393573" title="Mimir does not include content from: issues.redhat.com"><span class="strong strong"><strong>Create Issue</strong></span></a> to launch the form in a browser.
				</li><li class="listitem">
					Complete the <span class="strong strong"><strong>Summary</strong></span>, <span class="strong strong"><strong>Description</strong></span>, and <span class="strong strong"><strong>Reporter</strong></span> fields.
				</li><li class="listitem"><p class="simpara">
					Click <span class="strong strong"><strong>Create</strong></span> to submit the form.
				</p><p class="simpara">
					The form creates an issue in the Red Hat Hybrid Cloud Infrastructure (HCIDOCS) Jira project.
				</p></li></ol></div></section><section class="chapter" id="about-ai"><div class="titlepage"><div><div><h1 class="title">Chapter 1. About the Assisted Installer</h1></div></div></div><p>
			The Assisted Installer for Red Hat OpenShift Container Platform is a user-friendly installation solution offered on the <a class="link mimir-link-warn" href="http://console.redhat.com" title="Mimir does not include content from: console.redhat.com">Red Hat Hybrid Cloud Console</a>. The Assisted Installer supports various deployment platforms with a focus on bare metal, Nutanix, vSphere, and Oracle Cloud Infrastructure.
		</p><p>
			You can install OpenShift Container Platform on premises in a connected environment, with an optional HTTP/S proxy, for the following platforms:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					Highly available OpenShift Container Platform or single-node OpenShift cluster
				</li><li class="listitem">
					OpenShift Container Platform on bare metal or vSphere with full platform integration, or other virtualization platforms without integration
				</li><li class="listitem">
					Optionally, OpenShift Virtualization and Red Hat OpenShift Data Foundation
				</li></ul></div><section class="section" id="ai-features_about-ai"><div class="titlepage"><div><div><h2 class="title">1.1. Features</h2></div></div></div><p>
				The Assisted Installer provides installation functionality as a service. This software-as-a-service (SaaS) approach has the following features:
			</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Web interface</span></dt><dd><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									You can install your cluster by using the <a class="link mimir-link-warn" href="https://console.redhat.com/openshift/assisted-installer/clusters/~new" title="Mimir does not include content from: console.redhat.com">Hybrid Cloud Console</a> instead of creating installation configuration files manually.
								</li></ul></div></dd><dt><span class="term">No bootstrap node</span></dt><dd><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									You do not need a bootstrap node because the bootstrapping process runs on a node within the cluster.
								</li></ul></div></dd><dt><span class="term">Streamlined installation workflow</span></dt><dd><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									You do not need in-depth knowledge of OpenShift Container Platform to deploy a cluster. The Assisted Installer provides reasonable default configurations.
								</li><li class="listitem">
									You do not need to run the OpenShift Container Platform installer locally.
								</li><li class="listitem">
									You have access to the latest Assisted Installer for the latest tested z-stream releases.
								</li></ul></div></dd><dt><span class="term">Advanced networking options</span></dt><dd><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									The Assisted Installer supports IPv4 networking with SDN and OVN, IPv6 and dual stack networking with OVN only, NMState-based static IP addressing, and an HTTP/S proxy.
								</li><li class="listitem">
									OVN is the default Container Network Interface (CNI) for OpenShift Container Platform 4.12 and later.
								</li><li class="listitem">
									SDN is supported up to OpenShift Container Platform 4.14 and deprecated in OpenShift Container Platform 4.15.
								</li></ul></div></dd><dt><span class="term">Preinstallation validation</span></dt><dd><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									Before installing, the Assisted Installer checks the following configurations:
								</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
											Network connectivity
										</li><li class="listitem">
											Network bandwidth
										</li><li class="listitem">
											Connectivity to the registry
										</li><li class="listitem">
											Upstream DNS resolution of the domain name
										</li><li class="listitem">
											Time synchronization between cluster nodes
										</li><li class="listitem">
											Cluster node hardware
										</li><li class="listitem">
											Installation configuration parameters
										</li></ul></div></li></ul></div></dd><dt><span class="term">REST API</span></dt><dd><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									You can automate the installation process by using the Assisted Installer REST API.
								</li></ul></div></dd></dl></div></section><section class="section" id="customizing-installation_about-ai"><div class="titlepage"><div><div><h2 class="title">1.2. Customizing your installation</h2></div></div></div><p>
				You can customize your installation by selecting one or more options.
			</p><p>
				These options are installed as Operators, which are used to package, deploy, and manage services and applications on the control plane.
			</p><p>
				You can deploy these Operators after the installation if you require advanced configuration options.
			</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">OpenShift Virtualization</span></dt><dd><p class="simpara">
							You can deploy OpenShift Virtualization to perform the following tasks:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Create and manage Linux and Windows virtual machines (VMs).
								</li><li class="listitem">
									Run pod and VM workloads alongside each other in a cluster.
								</li><li class="listitem">
									Connect to VMs through a variety of consoles and CLI tools.
								</li><li class="listitem">
									Import and clone existing VMs.
								</li><li class="listitem">
									Manage network interface controllers and storage disks attached to VMs.
								</li><li class="listitem">
									Live migrate VMs between nodes.
								</li></ul></div></dd><dt><span class="term">Multicluster engine for Kubernetes</span></dt><dd><p class="simpara">
							You can deploy the multicluster engine for Kubernetes to perform the following tasks in a large, multi-cluster environment:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Provision and manage additional Kubernetes clusters from your initial cluster.
								</li><li class="listitem">
									Use hosted control planes to reduce management costs and optimize cluster deployment by decoupling the control and data planes.
								</li><li class="listitem"><p class="simpara">
									Use GitOps Zero Touch Provisioning to manage remote edge sites at scale.
								</p><p class="simpara">
									You can deploy the multicluster engine with Red Hat OpenShift Data Foundation on all OpenShift Container Platform clusters.
								</p></li></ul></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								Deploying multicluster engine <span class="emphasis"><em>without</em></span> OpenShift Data Foundation results in the following scenarios:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										Multi-node cluster: No storage is configured. You must configure storage after the installation process.
									</li><li class="listitem">
										Single-node OpenShift: LVM Storage is installed.
									</li></ul></div><p>
								You must review the prerequisites to ensure that your environment has sufficient additional resources for the multicluster engine.
							</p></div></div></dd><dt><span class="term">Logical Volume Manager Storage</span></dt><dd>
							You can use Logical Volume Manager Storage (LVM Storage) to dynamically provision block storage on a limited resources cluster.
						</dd><dt><span class="term">Red Hat OpenShift Data Foundation</span></dt><dd>
							You can use Red Hat OpenShift Data Foundation for file, block, and object storage. This storage option is recommended for all OpenShift Container Platform clusters. OpenShift Data Foundation requires a separate subscription.
						</dd></dl></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						<a class="link mimir-link-warn" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.16/html/operators/index#operators" title="This content is not included in Mimir."><span class="emphasis"><em>Operators</em></span></a>.
					</li><li class="listitem">
						<a class="link mimir-link-warn" href="https://www.redhat.com/en/technologies/cloud-computing/openshift/virtualization" title="Mimir does not include content from: www.redhat.com">OpenShift Virtualization product overview</a>.
					</li><li class="listitem">
						<a class="link mimir-link-warn" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.16/html/virtualization/index#virtualization" title="This content is not included in Mimir.">OpenShift Virtualization documentation</a>.
					</li><li class="listitem">
						<a class="link mimir-link-warn" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.16/html/architecture/index#about-the-multicluster-engine-for-kubernetes-operator" title="This content is not included in Mimir.">"About the multicluster engine for Kubernetes Operator"</a> in <span class="emphasis"><em>Architecture</em></span>.
					</li><li class="listitem">
						<a class="link mimir-link-warn" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.16/html/architecture/index#hosted-control-planes-overview_control-plane" title="This content is not included in Mimir.">"Introduction to hosted control planes"</a> in <span class="emphasis"><em>Architecture</em></span>.
					</li><li class="listitem">
						<a class="link mimir-link-warn" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.16/html/edge_computing/index#edge_computing" title="This content is not included in Mimir."><span class="emphasis"><em>Edge computing</em></span></a>.
					</li><li class="listitem">
						<a class="link mimir-link-warn" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.16/html/storage/index#persistent-storage-using-lvms" title="This content is not included in Mimir.">"Persistent storage using Logical Volume Manager Storage"</a> in <span class="emphasis"><em>Storage</em></span>.
					</li><li class="listitem">
						<a class="link mimir-link-warn" href="https://www.redhat.com/en/resources/openshift-data-foundation-datasheet" title="Mimir does not include content from: www.redhat.com">OpenShift Data Foundation datasheet</a>.
					</li><li class="listitem">
						<a class="link mimir-link-warn" href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation" title="This content is not included in Mimir.">OpenShift Data Foundation documentation</a>.
					</li></ul></div></section><section class="section" id="api-support-policy"><div class="titlepage"><div><div><h2 class="title">1.3. API support policy</h2></div></div></div><p>
				Assisted Installer APIs are supported for a minimum of three months from the announcement of deprecation.
			</p></section></section><section class="chapter" id="prerequisites"><div class="titlepage"><div><div><h1 class="title">Chapter 2. Prerequisites</h1></div></div></div><p>
			The Assisted Installer validates the following prerequisites to ensure successful installation.
		</p><p>
			If you use a firewall, you must configure it so that Assisted Installer can access the resources it requires to function.
		</p><section class="section" id="supported_cpu_architectures"><div class="titlepage"><div><div><h2 class="title">2.1. Supported CPU architectures</h2></div></div></div><p>
				The Assisted Installer is supported on the following CPU architectures:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						x86_64
					</li><li class="listitem">
						arm64
					</li><li class="listitem">
						ppc64le
					</li><li class="listitem">
						s390x
					</li></ul></div></section><section class="section" id="resource_requirements"><div class="titlepage"><div><div><h2 class="title">2.2. Resource requirements</h2></div></div></div><p>
				This section describes the resource requirements for different clusters and installation options.
			</p><p>
				The multicluster engine for Kubernetes requires additional resources.
			</p><p>
				If you deploy the multicluster engine with storage, such as OpenShift Data Foundation or LVM Storage, you must also assign additional resources to each node.
			</p><section class="section" id="multi-node-cluster-requirements_prerequisites"><div class="titlepage"><div><div><h3 class="title">2.2.1. Multi-node cluster resource requirements</h3></div></div></div><p>
					The resource requirements of a multi-node cluster depend on the installation options.
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Multi-node cluster basic installation</span></dt><dd><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
										Control plane nodes:
									</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
												4 CPU cores
											</li><li class="listitem">
												16 GB RAM
											</li><li class="listitem"><p class="simpara">
												100 GB storage
											</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
													The disks must be reasonably fast, with an etcd <code class="literal">wal_fsync_duration_seconds</code> p99 duration that is less than 10 ms. For more information, see the Red Hat Knowledgebase solution <a class="link mimir-link-warn" href="https://access.redhat.com/solutions/4885641" title="This content is not included in Mimir.">How to Use 'fio' to Check Etcd Disk Performance in OCP</a>.
												</p></div></div></li></ul></div></li><li class="listitem"><p class="simpara">
										Compute nodes:
									</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
												2 CPU cores
											</li><li class="listitem">
												8 GB RAM
											</li><li class="listitem">
												100 GB storage
											</li></ul></div></li></ul></div></dd><dt><span class="term">Multi-node cluster + multicluster engine</span></dt><dd><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										Additional 4 CPU cores
									</li><li class="listitem"><p class="simpara">
										Additional 16 GB RAM
									</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
											If you deploy multicluster engine without OpenShift Data Foundation, no storage is configured. You configure the storage after the installation.
										</p></div></div></li></ul></div></dd><dt><span class="term">Multi-node cluster + multicluster engine + OpenShift Data Foundation or LVM Storage</span></dt><dd><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										Additional 75 GB storage
									</li></ul></div></dd></dl></div></section><section class="section" id="sno-requirements_prerequisites"><div class="titlepage"><div><div><h3 class="title">2.2.2. Single-node OpenShift resource requirements</h3></div></div></div><p>
					The resource requirements for single-node OpenShift depend on the installation options.
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Single-node OpenShift basic installation</span></dt><dd><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										8 CPU cores
									</li><li class="listitem">
										16 GB RAM
									</li><li class="listitem">
										100 GB storage
									</li></ul></div></dd><dt><span class="term">Single-node OpenShift + multicluster engine</span></dt><dd><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										Additional 8 CPU cores
									</li><li class="listitem"><p class="simpara">
										Additional 32 GB RAM
									</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
											If you deploy multicluster engine without OpenShift Data Foundation, LVM Storage is enabled.
										</p></div></div></li></ul></div></dd><dt><span class="term">Single-node OpenShift + multicluster engine + OpenShift Data Foundation or LVM Storage</span></dt><dd><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										Additional 95 GB storage
									</li></ul></div></dd></dl></div></section></section><section class="section" id="networking_requirements"><div class="titlepage"><div><div><h2 class="title">2.3. Networking requirements</h2></div></div></div><p>
				For hosts of type <code class="literal">VMware</code>, set <code class="literal">clusterSet disk.enableUUID</code> to <code class="literal">true</code>, even when the platform is not vSphere.
			</p><section class="section" id="general_networking_requirements"><div class="titlepage"><div><div><h3 class="title">2.3.1. General networking requirements</h3></div></div></div><p>
					The network must meet the following requirements:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							A DHCP server unless using static IP addressing.
						</li><li class="listitem"><p class="simpara">
							A base domain name. You must ensure that the following requirements are met:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									There is no wildcard, such as <code class="literal">*.&lt;cluster_name&gt;.&lt;base_domain&gt;</code>, or the installation will not proceed.
								</li><li class="listitem">
									A DNS A/AAAA record for <code class="literal">api.&lt;cluster_name&gt;.&lt;base_domain&gt;</code>.
								</li><li class="listitem">
									A DNS A/AAAA record with a wildcard for <code class="literal">*.apps.&lt;cluster_name&gt;.&lt;base_domain&gt;</code>.
								</li></ul></div></li><li class="listitem">
							Port <code class="literal">6443</code> is open for the API URL to allow users outside the firewall to access the cluster by using the <code class="literal">oc</code> CLI tool.
						</li><li class="listitem">
							Port <code class="literal">443</code> is open for the console to allow users outside the firewall to access the console.
						</li><li class="listitem">
							A DNS A/AAAA record for each node in the cluster when using User Managed Networking, or the installation will not proceed. DNS A/AAAA records are required for each node in the cluster when using Cluster Managed Networking after installation is complete to connect to the cluster, but installation can proceed without the A/AAAA records when using Cluster Managed Networking.
						</li><li class="listitem">
							A DNS PTR record for each node in the cluster if you want to boot with the preset hostname when using static IP addressing. Otherwise, the Assisted Installer has an automatic node renaming feature when using static IP addressing that will rename the nodes to their network interface MAC address.
						</li></ul></div><div class="admonition important"><div class="admonition_header">Important</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								DNS A/AAAA record settings at top-level domain registrars can take significant time to update. Ensure the A/AAAA record DNS settings are working before installation to prevent installation delays.
							</li><li class="listitem">
								For DNS record examples, see <span class="emphasis"><em>Example DNS configuration</em></span>.
							</li></ul></div></div></div><p>
					The OpenShift Container Platform cluster’s network must also meet the following requirements:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Connectivity between all cluster nodes
						</li><li class="listitem">
							Connectivity for each node to the internet
						</li><li class="listitem">
							Access to an NTP server for time synchronization between the cluster nodes
						</li></ul></div><section class="section" id="example_dns_configuration"><div class="titlepage"><div><div><h4 class="title">2.3.1.1. Example DNS configuration</h4></div></div></div><p>
						This section provides A and PTR record configuration examples that meet the DNS requirements for deploying OpenShift Container Platform using the Assisted Installer. The examples are not meant to provide advice for choosing one DNS solution over another.
					</p><p>
						In the examples, the cluster name is <code class="literal">ocp4</code> and the base domain is <code class="literal">example.com</code>.
					</p></section><section class="section" id="example_dns_a_record_configuration"><div class="titlepage"><div><div><h4 class="title">2.3.1.2. Example DNS A record configuration</h4></div></div></div><p>
						The following example is a BIND zone file that shows sample A records for name resolution in a cluster installed using the Assisted Installer.
					</p><div class="formalpara"><p class="title"><strong>Example DNS zone database</strong></p><p>
							
<pre class="programlisting language-terminal">$TTL 1W
@	IN	SOA	ns1.example.com.	root (
			2019070700	; serial
			3H		; refresh (3 hours)
			30M		; retry (30 minutes)
			2W		; expiry (2 weeks)
			1W )		; minimum (1 week)
	IN	NS	ns1.example.com.
	IN	MX 10	smtp.example.com.
;
;
ns1.example.com.		IN	A	192.168.1.1
smtp.example.com.		IN	A	192.168.1.5
;
helper.example.com.		IN	A	192.168.1.5
;
api.ocp4.example.com.		IN	A	192.168.1.5 <span id="CO1-1"><!--Empty--></span><span class="callout">1</span>
api-int.ocp4.example.com.	IN	A	192.168.1.5 <span id="CO1-2"><!--Empty--></span><span class="callout">2</span>
;
*.apps.ocp4.example.com.	IN	A	192.168.1.5 <span id="CO1-3"><!--Empty--></span><span class="callout">3</span>
;
control-plane0.ocp4.example.com.	IN	A	192.168.1.97 <span id="CO1-4"><!--Empty--></span><span class="callout">4</span>
control-plane1.ocp4.example.com.	IN	A	192.168.1.98
control-plane2.ocp4.example.com.	IN	A	192.168.1.99
;
worker0.ocp4.example.com.	IN	A	192.168.1.11 <span id="CO1-5"><!--Empty--></span><span class="callout">5</span>
worker1.ocp4.example.com.	IN	A	192.168.1.7
;
;EOF</pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO1-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Provides name resolution for the Kubernetes API. The record refers to the IP address of the API load balancer.
							</div></dd><dt><a href="#CO1-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Provides name resolution for the Kubernetes API. The record refers to the IP address of the API load balancer and is used for internal cluster communications.
							</div></dd><dt><a href="#CO1-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Provides name resolution for the wildcard routes. The record refers to the IP address of the application ingress load balancer. The application ingress load balancer targets the machines that run the Ingress Controller pods. The Ingress Controller pods run on the worker machines by default.
							</div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									In the example, the same load balancer is used for the Kubernetes API and application ingress traffic. In production scenarios, you can deploy the API and application ingress load balancers separately so that you can scale the load balancer infrastructure for each in isolation.
								</p></div></div></dd><dt><a href="#CO1-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								Provides name resolution for the control plane machines.
							</div></dd><dt><a href="#CO1-5"><span class="callout">5</span></a> </dt><dd><div class="para">
								Provides name resolution for the worker machines.
							</div></dd></dl></div></section><section class="section" id="example_dns_ptr_record_configuration"><div class="titlepage"><div><div><h4 class="title">2.3.1.3. Example DNS PTR record configuration</h4></div></div></div><p>
						The following example is a BIND zone file that shows sample PTR records for reverse name resolution in a cluster installed using the Assisted Installer.
					</p><div class="formalpara"><p class="title"><strong>Example DNS zone database for reverse records</strong></p><p>
							
<pre class="programlisting language-terminal">$$TTL 1W
@	IN	SOA	ns1.example.com.	root (
			2019070700	; serial
			3H		; refresh (3 hours)
			30M		; retry (30 minutes)
			2W		; expiry (2 weeks)
			1W )		; minimum (1 week)
	IN	NS	ns1.example.com.
;
5.1.168.192.in-addr.arpa.	IN	PTR	api.ocp4.example.com. <span id="CO2-1"><!--Empty--></span><span class="callout">1</span>
5.1.168.192.in-addr.arpa.	IN	PTR	api-int.ocp4.example.com. <span id="CO2-2"><!--Empty--></span><span class="callout">2</span>
;
97.1.168.192.in-addr.arpa.	IN	PTR	control-plane0.ocp4.example.com. <span id="CO2-3"><!--Empty--></span><span class="callout">3</span>
98.1.168.192.in-addr.arpa.	IN	PTR	control-plane1.ocp4.example.com.
99.1.168.192.in-addr.arpa.	IN	PTR	control-plane2.ocp4.example.com.
;
11.1.168.192.in-addr.arpa.	IN	PTR	worker0.ocp4.example.com. <span id="CO2-4"><!--Empty--></span><span class="callout">4</span>
7.1.168.192.in-addr.arpa.	IN	PTR	worker1.ocp4.example.com.
;
;EOF</pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO2-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Provides reverse DNS resolution for the Kubernetes API. The PTR record refers to the record name of the API load balancer.
							</div></dd><dt><a href="#CO2-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Provides reverse DNS resolution for the Kubernetes API. The PTR record refers to the record name of the API load balancer and is used for internal cluster communications.
							</div></dd><dt><a href="#CO2-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Provides reverse DNS resolution for the control plane machines.
							</div></dd><dt><a href="#CO2-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								Provides reverse DNS resolution for the worker machines.
							</div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									A PTR record is not required for the OpenShift Container Platform application wildcard.
								</p></div></div></dd></dl></div></section></section><section class="section" id="networking_requirements_ibmz"><div class="titlepage"><div><div><h3 class="title">2.3.2. Networking requirements for IBM Z</h3></div></div></div><p>
					In IBM Z® environments, advanced networking technologies like Original Storage Architecture (OSA), HiperSockets, and Remote Direct Memory Access (RDMA) over Converged Ethernet (RoCE) require specific configurations that deviate from the standard settings used in Assisted Installer deployments. These overrides are necessary to accommodate their unique requirements and ensure a successful and efficient deployment on IBM Z®.
				</p><p>
					The following table lists the network devices that are supported for the network configuration override functionality:
				</p><div class="informaltable"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 27%; " class="col_1"><!--Empty--></col><col style="width: 18%; " class="col_2"><!--Empty--></col><col style="width: 18%; " class="col_3"><!--Empty--></col><col style="width: 18%; " class="col_4"><!--Empty--></col><col style="width: 18%; " class="col_5"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139800924328864" scope="col">Network device</th><th align="left" valign="top" id="idm139800924327776" scope="col">z/VM</th><th align="left" valign="top" id="idm139800923847376" scope="col">KVM</th><th align="left" valign="top" id="idm139800923846288" scope="col">LPAR Classic</th><th align="left" valign="top" id="idm139800923845200" scope="col">LPAR Dynamic Partition Manager (DPM)</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139800924328864"> <p>
									Original Storage Architecture (OSA) virtual switch
								</p>
								 </td><td align="left" valign="top" headers="idm139800924327776"> <p>
									Not supported
								</p>
								 </td><td align="left" valign="top" headers="idm139800923847376"> <p>
									 — 
								</p>
								 </td><td align="left" valign="top" headers="idm139800923846288"> <p>
									Not supported
								</p>
								 </td><td align="left" valign="top" headers="idm139800923845200"> <p>
									Not supported
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800924328864"> <p>
									Direct attached OSA
								</p>
								 </td><td align="left" valign="top" headers="idm139800924327776"> <p>
									Supported
								</p>
								 </td><td align="left" valign="top" headers="idm139800923847376"> <p>
									Only through a Linux bridge
								</p>
								 </td><td align="left" valign="top" headers="idm139800923846288"> <p>
									Supported
								</p>
								 </td><td align="left" valign="top" headers="idm139800923845200"> <p>
									Not supported
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800924328864"> <p>
									RDMA over Converged Ethernet (RoCE)
								</p>
								 </td><td align="left" valign="top" headers="idm139800924327776"> <p>
									Not supported
								</p>
								 </td><td align="left" valign="top" headers="idm139800923847376"> <p>
									Only through a Linux bridge
								</p>
								 </td><td align="left" valign="top" headers="idm139800923846288"> <p>
									Not supported
								</p>
								 </td><td align="left" valign="top" headers="idm139800923845200"> <p>
									Not supported
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800924328864"> <p>
									HiperSockets
								</p>
								 </td><td align="left" valign="top" headers="idm139800924327776"> <p>
									Supported
								</p>
								 </td><td align="left" valign="top" headers="idm139800923847376"> <p>
									Only through a Linux bridge
								</p>
								 </td><td align="left" valign="top" headers="idm139800923846288"> <p>
									Supported
								</p>
								 </td><td align="left" valign="top" headers="idm139800923845200"> <p>
									Not supported
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800924328864"> <p>
									Linux bridge
								</p>
								 </td><td align="left" valign="top" headers="idm139800924327776"> <p>
									Not supported
								</p>
								 </td><td align="left" valign="top" headers="idm139800923847376"> <p>
									Supported
								</p>
								 </td><td align="left" valign="top" headers="idm139800923846288"> <p>
									Not supported
								</p>
								 </td><td align="left" valign="top" headers="idm139800923845200"> <p>
									Not supported
								</p>
								 </td></tr></tbody></table></div><section class="section" id="configuring-network-overrides-ibmz_prerequisites"><div class="titlepage"><div><div><h4 class="title">2.3.2.1. Configuring network overrides in IBM Z</h4></div></div></div><p>
						You can specify a static IP address on IBM Z® machines that uses Logical Partition (LPAR) and z/VM. This is specially useful when the network devices do not have a static MAC address assigned to them.
					</p><p>
						If you have an existing <code class="literal">.parm</code> file, edit it to include the following entry:
					</p><pre class="programlisting language-terminal">ai.ip_cfg_override=1</pre><p>
						This parameter allows the file to add the network settings to the CoreOS installer.
					</p><div class="formalpara"><p class="title"><strong>Example of the <code class="literal">.parm</code> file</strong></p><p>
							
<pre class="programlisting language-terminal">rd.neednet=1 cio_ignore=all,!condev
console=ttysclp0
coreos.live.rootfs_url=&lt;coreos_url&gt; <span id="CO3-1"><!--Empty--></span><span class="callout">1</span>
ip=&lt;ip&gt;::&lt;gateway&gt;:&lt;netmask&gt;:&lt;hostname&gt;::none nameserver=&lt;dns&gt;
rd.znet=qeth,&lt;network_adaptor_range&gt;,layer2=1
rd.&lt;disk_type&gt;=&lt;adapter&gt; <span id="CO3-2"><!--Empty--></span><span class="callout">2</span>
rd.zfcp=&lt;adapter&gt;,&lt;wwpn&gt;,&lt;lun&gt; random.trust_cpu=on <span id="CO3-3"><!--Empty--></span><span class="callout">3</span>
zfcp.allow_lun_scan=0
ai.ip_cfg_override=1 <span id="CO3-4"><!--Empty--></span><span class="callout">4</span>
rd.luks.options=discard ignition.firstboot ignition.platform.id=metal
random.trust_cpu=on rd.luks.options=discard</pre>

						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO3-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								For the <code class="literal">coreos.live.rootfs_url</code> artifact, specify the matching <code class="literal">rootfs</code> artifact for the <code class="literal">kernel</code> and <code class="literal">initramfs</code> that you are booting. Only HTTP and HTTPS protocols are supported.
							</div></dd><dt><a href="#CO3-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								For installations on direct access storage devices (DASD) type disks, use <code class="literal">rd.</code> to specify the DASD where Red Hat Enterprise Linux (RHEL) is to be installed. For installations on FCP-type disks, use <code class="literal">rd.zfcp=&lt;adapter&gt;,&lt;wwpn&gt;,&lt;lun&gt;</code> to specify the FCP disk where RHEL is to be installed.
							</div></dd><dt><a href="#CO3-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Specify values for <code class="literal">adapter</code>, <code class="literal">wwpn</code>, and <code class="literal">lun</code> as in the following example: <code class="literal">rd.zfcp=0.0.8002,0x500507630400d1e3,0x4000404600000000</code>.
							</div></dd><dt><a href="#CO3-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								Specify this parameter when using an OSA network adapter or HiperSockets.
							</div></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The <code class="literal">override</code> parameter overrides the host’s network configuration settings.
						</p></div></div></section></section></section><section class="section" id="preflight_validations"><div class="titlepage"><div><div><h2 class="title">2.4. Preflight validations</h2></div></div></div><p>
				The Assisted Installer ensures the cluster meets the prerequisites before installation, because it eliminates complex postinstallation troubleshooting, thereby saving significant amounts of time and effort. Before installing software on the nodes, the Assisted Installer conducts the following validations:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Ensures network connectivity
					</li><li class="listitem">
						Ensures sufficient network bandwidth
					</li><li class="listitem">
						Ensures connectivity to the registry
					</li><li class="listitem">
						Ensures that any upstream DNS can resolve the required domain name
					</li><li class="listitem">
						Ensures time synchronization between cluster nodes
					</li><li class="listitem">
						Verifies that the cluster nodes meet the minimum hardware requirements
					</li><li class="listitem">
						Validates the installation configuration parameters
					</li></ul></div><p>
				If the Assisted Installer does not successfully validate the foregoing requirements, installation will not proceed.
			</p></section></section><section class="chapter" id="installing-with-ui"><div class="titlepage"><div><div><h1 class="title">Chapter 3. Installing with the Assisted Installer web console</h1></div></div></div><p>
			After you ensure the cluster nodes and network requirements are met, you can begin installing the cluster.
		</p><section class="section" id="preinstallation-considerations_installing-with-ui"><div class="titlepage"><div><div><h2 class="title">3.1. Preinstallation considerations</h2></div></div></div><p>
				Before installing OpenShift Container Platform with the Assisted Installer, you must consider the following configuration choices:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Which base domain to use
					</li><li class="listitem">
						Which OpenShift Container Platform product version to install
					</li><li class="listitem">
						Whether to install a full cluster or single-node OpenShift
					</li><li class="listitem">
						Whether to use a DHCP server or a static network configuration
					</li><li class="listitem">
						Whether to use IPv4 or dual-stack networking
					</li><li class="listitem">
						Whether to install OpenShift Virtualization
					</li><li class="listitem">
						Whether to install Red Hat OpenShift Data Foundation
					</li><li class="listitem">
						Whether to install multicluster engine for Kubernetes
					</li><li class="listitem">
						Whether to integrate with the platform when installing on vSphere or Nutanix
					</li><li class="listitem">
						Whether to install a mixed-cluster architecture
					</li></ul></div></section><section class="section" id="setting-the-cluster-details_installing-with-ui"><div class="titlepage"><div><div><h2 class="title">3.2. Setting the cluster details</h2></div></div></div><p>
				To create a cluster with the Assisted Installer web user interface, use the following procedure.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Log in to the <a class="link mimir-link-warn" href="https://console.redhat.com" title="Mimir does not include content from: console.redhat.com">Red Hat Hybrid Cloud Console</a>.
					</li><li class="listitem">
						In the <span class="strong strong"><strong>Red Hat OpenShift</strong></span> tile, click <span class="strong strong"><strong>Scale your applications</strong></span>.
					</li><li class="listitem">
						In the menu, click <span class="strong strong"><strong>Clusters</strong></span>.
					</li><li class="listitem">
						Click <span class="strong strong"><strong>Create cluster</strong></span>.
					</li><li class="listitem">
						Click the <span class="strong strong"><strong>Datacenter</strong></span> tab.
					</li><li class="listitem">
						Under <span class="strong strong"><strong>Assisted Installer</strong></span>, click <span class="strong strong"><strong>Create cluster</strong></span>.
					</li><li class="listitem">
						Enter a name for the cluster in the <span class="strong strong"><strong>Cluster name</strong></span> field.
					</li><li class="listitem"><p class="simpara">
						Enter a base domain for the cluster in the <span class="strong strong"><strong>Base domain</strong></span> field. All subdomains for the cluster will use this base domain.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The base domain must be a valid DNS name. You must not have a wildcard domain set up for the base domain.
						</p></div></div></li><li class="listitem"><p class="simpara">
						From the <span class="strong strong"><strong>OpenShift version</strong></span> dropdown list, select the version that you want to install and click <span class="strong strong"><strong>Select</strong></span>. By default, the latest OpenShift versions are listed. If you need an older version that is not displayed, click <span class="strong strong"><strong>Show all available versions</strong></span> at the bottom of the list, and use the search box to find it.
					</p><div class="admonition important"><div class="admonition_header">Important</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									For a mixed-architecture cluster installation, select OpenShift Container Platform 4.12 or later, and use the <code class="literal">-multi</code> option. For instructions on installing a mixed-architecture cluster, see <span class="emphasis"><em>Additional resources</em></span>.
								</li><li class="listitem">
									For IBM Power® and IBM Z® platforms, only OpenShift Container Platform 4.13 and later is supported.
								</li></ul></div></div></div></li><li class="listitem"><p class="simpara">
						Optional: Select <span class="strong strong"><strong>Install single-node OpenShift (SNO)</strong></span> if you want to install OpenShift Container Platform on a single node.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							Currently, single-node OpenShift is not supported on IBM Z® and IBM Power® platforms.
						</p></div></div></li><li class="listitem">
						Optional: The Assisted Installer already has the pull secret associated to your account. If you want to use a different pull secret, select <span class="strong strong"><strong>Edit pull secret</strong></span>.
					</li><li class="listitem"><p class="simpara">
						Optional: If you are installing OpenShift Container Platform on a third-party platform, select the platform from the <span class="strong strong"><strong>Integrate with external parter platforms</strong></span> list. Valid values are <code class="literal">Nutanix</code>, <code class="literal">vSphere</code> or <code class="literal">Oracle Cloud Infrastructure</code>. Assisted Installer defaults to having no platform integration.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Assisted Installer supports Oracle Cloud Infrastructure (OCI) integration from OpenShift Container Platform 4.14 and later.
								</li><li class="listitem">
									For details on each of the external partner integrations, see <span class="emphasis"><em>Additional Resources</em></span>.
								</li></ul></div></div></div></li><li class="listitem"><p class="simpara">
						Optional: Assisted Installer defaults to using <code class="literal">x86_64</code> CPU architecture. If you are installing OpenShift Container Platform on a different architecture, select the architecture to use. Valid values are <code class="literal">arm64</code>, <code class="literal">ppc64le</code>, and <code class="literal">s390x</code>. Remember that some features are not available with <code class="literal">arm64</code>, <code class="literal">ppc64le</code>, and <code class="literal">s390x</code> CPU architectures.
					</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							For a mixed-architecture cluster installation, use the default <code class="literal">x86_64</code> architecture. For instructions on installing a mixed-architecture cluster, see <span class="emphasis"><em>Additional resources</em></span>.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Optional: Select <span class="strong strong"><strong>Include custom manifests</strong></span> if you have at least one custom manifest to include in the installation. A custom manifest contains additional configurations not currently supported in the Assisted Installer. Selecting the checkbox adds the <span class="strong strong"><strong>Custom manifests</strong></span> page to the wizard, where you upload the manifests.
					</p><div class="admonition important"><div class="admonition_header">Important</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									If you are installing OpenShift Container Platform on the Oracle Cloud Infrastructure (OCI) third-party platform, it is mandatory to add the custom manifests provided by Oracle.
								</li><li class="listitem">
									If you have already added custom manifests, unchecking the <span class="strong strong"><strong>Include custom manifests</strong></span> box automatically deletes them all. You will be asked to confirm the deletion.
								</li></ul></div></div></div></li><li class="listitem"><p class="simpara">
						Optional: The Assisted Installer defaults to DHCP networking. If you are using a static IP configuration, bridges or bonds for the cluster nodes instead of DHCP reservations, select <span class="strong strong"><strong>Static IP, bridges, and bonds</strong></span>.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							A static IP configuration is not supported for OpenShift Container Platform installations on Oracle Cloud Infrastructure.
						</p></div></div></li><li class="listitem">
						Optional: If you want to enable encryption of the installation disks, under <span class="strong strong"><strong>Enable encryption of installation disks</strong></span> you can select <span class="strong strong"><strong>Control plane node, worker</strong></span> for single-node OpenShift. For multi-node clusters, you can select <span class="strong strong"><strong>Control plane nodes</strong></span> to encrypt the control plane node installation disks and select <span class="strong strong"><strong>Workers</strong></span> to encrypt worker node installation disks.
					</li></ol></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					You cannot change the base domain, the single-node OpenShift checkbox, the CPU architecture, the host’s network configuration, or the disk-encryption after installation begins.
				</p></div></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						<a class="link" href="#assembly_installing-on-nutanix" title="Chapter 13. Optional: Installing on Nutanix">Optional: Installing on Nutanix</a>
					</li><li class="listitem">
						<a class="link" href="#installing-on-vsphere" title="Chapter 14. Optional: Installing on vSphere">Optional: Installing on vSphere</a>
					</li><li class="listitem">
						<a class="link" href="#installing-on-oci" title="Chapter 15. Optional: Installing on Oracle Cloud Infrastructure (OCI)">Optional: Installing on Oracle Cloud Infrastructure (OCI)</a>
					</li></ul></div></section><section class="section" id="configuring-static-networks_installing-with-ui"><div class="titlepage"><div><div><h2 class="title">3.3. Optional: Configuring static networks</h2></div></div></div><p>
				The Assisted Installer supports IPv4 networking with SDN up to OpenShift Container Platform 4.14 and OVN, and supports IPv6 and dual stack networking with OVN only. The Assisted Installer supports configuring the network with static network interfaces with IP address/MAC address mapping. The Assisted Installer also supports configuring host network interfaces with the NMState library, a declarative network manager API for hosts. You can use NMState to deploy hosts with static IP addressing, bonds, VLANs and other advanced networking features. First, you must set network-wide configurations. Then, you must create a host-specific configuration for each host.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					For installations on IBM Z® with z/VM, ensure that the z/VM nodes and vSwitches are properly configured for static networks and NMState. Also, the z/VM nodes must have a fixed MAC address assigned as the pool MAC addresses might cause issues with NMState.
				</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Select the internet protocol version. Valid options are <span class="strong strong"><strong>IPv4</strong></span> and <span class="strong strong"><strong>Dual stack</strong></span>.
					</li><li class="listitem">
						If the cluster hosts are on a shared VLAN, enter the VLAN ID.
					</li><li class="listitem"><p class="simpara">
						Enter the network-wide IP addresses. If you selected <span class="strong strong"><strong>Dual stack</strong></span> networking, you must enter both IPv4 and IPv6 addresses.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
								Enter the cluster network’s IP address range in CIDR notation.
							</li><li class="listitem">
								Enter the default gateway IP address.
							</li><li class="listitem">
								Enter the DNS server IP address.
							</li></ol></div></li><li class="listitem"><p class="simpara">
						Enter the host-specific configuration.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
								If you are only setting a static IP address that uses a single network interface, use the form view to enter the IP address and the MAC address for each host.
							</li><li class="listitem">
								If you use multiple interfaces, bonding, or other advanced networking features, use the YAML view and enter the desired network state for each host that uses NMState syntax. Then, add the MAC address and interface name for each host interface used in your network configuration.
							</li></ol></div></li></ol></div><div class="itemizedlist _additional_resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional_resources" type="disc"><li class="listitem">
						<a class="link mimir-link-warn" href="http://nmstate.io" title="Mimir does not include content from: nmstate.io">NMState version 2.1.4</a>
					</li></ul></div></section><section class="section" id="configuring-operators_installing-with-ui"><div class="titlepage"><div><div><h2 class="title">3.4. Optional: Installing Operators</h2></div></div></div><p>
				This step is optional.
			</p><p>
				See the product documentation for prerequisites and configuration options:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link mimir-link-warn" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html-single/virtualization/index#virt-getting-started" title="This content is not included in Mimir.">OpenShift Virtualization</a>
					</li><li class="listitem">
						<a class="link mimir-link-warn" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html-single/architecture/index#mce-overview-ocp" title="This content is not included in Mimir.">Multicluster Engine for Kubernetes</a>
					</li><li class="listitem">
						<a class="link mimir-link-warn" href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation" title="This content is not included in Mimir.">Red Hat OpenShift Data Foundation</a>
					</li><li class="listitem">
						<a class="link mimir-link-warn" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html-single/storage/index#persistent-storage-using-lvms" title="This content is not included in Mimir.">Logical Volume Manager Storage</a>
					</li></ul></div><p>
				If you require advanced options, install the Operators after you have installed the cluster.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Select one or more from the following options:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								<span class="strong strong"><strong>Install OpenShift Virtualization</strong></span>
							</li><li class="listitem"><p class="simpara">
								<span class="strong strong"><strong>Install multicluster engine</strong></span>
							</p><p class="simpara">
								You can deploy the multicluster engine with OpenShift Data Foundation on all OpenShift Container Platform clusters.
							</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
									Deploying the multicluster engine <span class="emphasis"><em>without</em></span> OpenShift Data Foundation results in the following storage configurations:
								</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
											Multi-node cluster: No storage is configured. You must configure storage after the installation.
										</li><li class="listitem">
											Single-node OpenShift: LVM Storage is installed.
										</li></ul></div></div></div></li><li class="listitem">
								<span class="strong strong"><strong>Install Logical Volume Manager Storage</strong></span>
							</li><li class="listitem">
								<span class="strong strong"><strong>Install OpenShift Data Foundation</strong></span>
							</li></ul></div></li><li class="listitem">
						Click <span class="strong strong"><strong>Next</strong></span>.
					</li></ol></div></section><section class="section" id="adding-hosts-to-the-cluster_installing-with-ui"><div class="titlepage"><div><div><h2 class="title">3.5. Adding hosts to the cluster</h2></div></div></div><p>
				You must add one or more hosts to the cluster. Adding a host to the cluster involves generating a discovery ISO. The discovery ISO runs Red Hat Enterprise Linux CoreOS (RHCOS) in-memory with an agent.
			</p><p>
				Use the following table to identify the image file type for your IBM Z® architecture:
			</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139800922253840" scope="col">Architecture</th><th align="left" valign="top" id="idm139800923411520" scope="col">Boot method</th><th align="left" valign="top" id="idm139800923410432" scope="col">Image type</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139800922253840"> <p>
								Logical Partition-Classic
							</p>
							 </td><td align="left" valign="top" headers="idm139800923411520"> <p>
								iPXE
							</p>
							 </td><td align="left" valign="top" headers="idm139800923410432"> <p>
								Full image file: Download a self-contained ISO image
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139800922253840"> <p>
								Logical Partition-Data Protection Manager
							</p>
							 </td><td align="left" valign="top" headers="idm139800923411520"> <p>
								ISO or iPXE
							</p>
							 </td><td align="left" valign="top" headers="idm139800923410432"> <p>
								Minimal image file: Download an ISO image that fetches content when booting up
							</p>
							 </td></tr></tbody></table></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					ISO images are not supported for installations on IBM Z (<code class="literal">s390x</code>) with z/VM or logical partitioning (LPAR) nodes; use the "Booting hosts with iPXE" procedure. ISO images and iPXE are supported for installations on RHEL KVM.
				</p></div></div><p>
				Perform the following procedure for each host on the cluster.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Click the <span class="strong strong"><strong>Add hosts</strong></span> button and select the provisioning type.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
								Select <span class="strong strong"><strong>Minimal image file: Provision with virtual media</strong></span> to download a smaller image that will fetch the data needed to boot. The nodes must have virtual media capability. This is the recommended method for <code class="literal">x86_64</code> and <code class="literal">arm64</code> architectures.
							</li><li class="listitem">
								Select <span class="strong strong"><strong>Full image file: Provision with physical media</strong></span> to download the larger full image. This is the recommended method for the <code class="literal">ppc64le</code> architecture and for the <code class="literal">s390x</code> architecture when installing with RHEL KVM.
							</li><li class="listitem">
								Select <span class="strong strong"><strong>iPXE: Provision from your network server</strong></span> to boot the hosts using iPXE. This is the recommended method on IBM Z® with z/VM nodes and LPAR (both static and DPM). ISO boot is the recommended method on the RHEL KVM installation.
							</li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									If you install on RHEL KVM, in some circumstances, the VMs on the KVM host are not rebooted on first boot and need to be restarted manually.
								</li><li class="listitem">
									If you install OpenShift Container Platform on Oracle Cloud Infrastructure, select <span class="strong strong"><strong>Minimal image file: Provision with virtual media</strong></span> only.
								</li></ul></div></div></div></li><li class="listitem"><p class="simpara">
						Optional: Activate the <span class="strong strong"><strong>Run workloads on control plane nodes</strong></span> switch to schedule workloads to run on control plane nodes, in addition to the default worker nodes.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							This option is available for clusters of five or more nodes. For clusters of under five nodes, the system runs workloads on the control plane nodes only, by default. For more details, see <span class="emphasis"><em>Configuring schedulable control plane nodes</em></span> in <span class="emphasis"><em>Additional Resources</em></span>.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Optional: If the cluster hosts are behind a firewall that requires the use of a proxy, select <span class="strong strong"><strong>Configure cluster-wide proxy settings</strong></span>. Enter the username, password, IP address and port for the HTTP and HTTPS URLs of the proxy server.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The proxy username and password must be URL-encoded.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Optional: Add an SSH public key so that you can connect to the cluster nodes as the <code class="literal">core</code> user. Having a login to the cluster nodes can provide you with debugging information during the installation.
					</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							Do not skip this procedure in production environments, where disaster recovery and debugging is required.
						</p></div></div><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
								If you do not have an existing SSH key pair on your local machine, follow the steps in <a class="link mimir-link-warn" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html-single/installing/index#ssh-agent-using_installing-platform-agnostic" title="This content is not included in Mimir.">Generating a key pair for cluster node SSH access</a>.
							</li><li class="listitem">
								In the <span class="strong strong"><strong>SSH public key</strong></span> field, click <span class="strong strong"><strong>Browse</strong></span> to upload the <code class="literal">id_rsa.pub</code> file containing the SSH public key. Alternatively, drag and drop the file into the field from the file manager. To see the file in the file manager, select <span class="strong strong"><strong>Show hidden files</strong></span> in the menu.
							</li></ol></div></li><li class="listitem">
						Optional: If the cluster hosts are in a network with a re-encrypting man-in-the-middle (MITM) proxy, or if the cluster needs to trust certificates for other purposes such as container image registries, select <span class="strong strong"><strong>Configure cluster-wide trusted certificates</strong></span>. Add additional certificates in X.509 format.
					</li><li class="listitem">
						Configure the discovery image if needed.
					</li><li class="listitem">
						Optional: If you are installing on a platform and want to integrate with the platform, select <span class="strong strong"><strong>Integrate with your virtualization platform</strong></span>. You must boot all hosts and ensure they appear in the host inventory. All the hosts must be on the same platform.
					</li><li class="listitem">
						Click <span class="strong strong"><strong>Generate Discovery ISO</strong></span> or <span class="strong strong"><strong>Generate Script File</strong></span>.
					</li><li class="listitem">
						Download the discovery ISO or iPXE script.
					</li><li class="listitem">
						Boot the host(s) with the discovery image or iPXE script.
					</li></ol></div><div class="itemizedlist _additional_resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional_resources" type="disc"><li class="listitem">
						<a class="link" href="#assembly_configuring-the-discovery-image" title="Chapter 7. Configuring the discovery image">Configuring the discovery image</a> for additional details.
					</li><li class="listitem">
						<a class="link" href="#assembly_booting-hosts-with-the-discovery-image" title="Chapter 8. Booting hosts with the discovery image">Booting hosts with the discovery image</a> for additional details.
					</li><li class="listitem">
						<a class="link mimir-link-warn" href="https://access.redhat.com/documentation/de-de/red_hat_enterprise_linux/9/html-single/configuring_and_managing_virtualization/index#assembly_creating-virtual-machines_virt-getting-started" title="This content is not included in Mimir.">Red Hat Enterprise Linux 9 - Configuring and managing virtualization</a> for additional details.
					</li><li class="listitem">
						<a class="link mimir-link-warn" href="https://www.ibm.com/support/pages/how-configure-vios-media-repositoryvirtual-media-library-ex-aix-installrestore" title="Mimir does not include content from: www.ibm.com">How to configure a VIOS Media Repository/Virtual Media Library</a> for additional details.
					</li><li class="listitem">
						<a class="link" href="#adding-hosts-on-nutanix-with-the-ui_installing-on-nutanix" title="13.1. Adding hosts on Nutanix with the UI">Adding hosts on Nutanix with the web console</a>
					</li><li class="listitem">
						<a class="link" href="#adding-hosts-on-vsphere_installing-on-vsphere" title="14.1. Adding hosts on vSphere">Adding hosts on vSphere</a>
					</li><li class="listitem">
						<a class="link" href="#assembly_configuring-schedulable-control-planes" title="Chapter 6. Optional: Configuring schedulable control plane nodes">Configurng schedulable control plane nodes</a>
					</li></ul></div></section><section class="section" id="configuring-hosts_installing-with-ui"><div class="titlepage"><div><div><h2 class="title">3.6. Configuring hosts</h2></div></div></div><p>
				After booting the hosts with the discovery ISO, the hosts will appear in the table at the bottom of the page. You can optionally configure the hostname and role for each host. You can also delete a host if necessary.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						From the <span class="strong strong"><strong>Options</strong></span> (⋮) menu for a host, select <span class="strong strong"><strong>Change hostname</strong></span>. If necessary, enter a new name for the host and click <span class="strong strong"><strong>Change</strong></span>. You must ensure that each host has a valid and unique hostname.
					</p><p class="simpara">
						Alternatively, from the <span class="strong strong"><strong>Actions</strong></span> list, select <span class="strong strong"><strong>Change hostname</strong></span> to rename multiple selected hosts. In the <span class="strong strong"><strong>Change Hostname</strong></span> dialog, type the new name and include <code class="literal">{{n}}</code> to make each hostname unique. Then click <span class="strong strong"><strong>Change</strong></span>.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							You can see the new names appearing in the <span class="strong strong"><strong>Preview</strong></span> pane as you type. The name will be identical for all selected hosts, with the exception of a single-digit increment per host.
						</p></div></div></li><li class="listitem"><p class="simpara">
						From the <span class="strong strong"><strong>Options</strong></span> (⋮) menu, you can select <span class="strong strong"><strong>Delete host</strong></span> to delete a host. Click <span class="strong strong"><strong>Delete</strong></span> to confirm the deletion.
					</p><p class="simpara">
						Alternatively, from the <span class="strong strong"><strong>Actions</strong></span> list, select <span class="strong strong"><strong>Delete</strong></span> to delete multiple selected hosts at the same time. Then click <span class="strong strong"><strong>Delete hosts</strong></span>.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							In a regular deployment, a cluster can have three or more hosts, and three of these must be control plane hosts. If you delete a host that is also a control plane, or if you are left with only two hosts, you will get a message saying that the system is not ready. To restore a host, you will need to reboot it from the discovery ISO.
						</p></div></div></li><li class="listitem">
						From the <span class="strong strong"><strong>Options</strong></span> (⋮) menu for the host, optionally select <span class="strong strong"><strong>View host events</strong></span>. The events in the list are presented chronologically.
					</li><li class="listitem"><p class="simpara">
						For multi-host clusters, in the <span class="strong strong"><strong>Role</strong></span> column next to the host name, you can click on the menu to change the role of the host.
					</p><p class="simpara">
						If you do not select a role, the Assisted Installer will assign the role automatically. The minimum hardware requirements for control plane nodes exceed that of worker nodes. If you assign a role to a host, ensure that you assign the control plane role to hosts that meet the minimum hardware requirements.
					</p></li><li class="listitem">
						Click the <span class="strong strong"><strong>Status</strong></span> link to view hardware, network and operator validations for the host.
					</li><li class="listitem">
						Click the arrow to the left of a host name to expand the host details.
					</li></ol></div><p>
				Once all cluster hosts appear with a status of <span class="strong strong"><strong>Ready</strong></span>, proceed to the next step.
			</p></section><section class="section" id="configuring-storage_installing-with-ui"><div class="titlepage"><div><div><h2 class="title">3.7. Configuring storage disks</h2></div></div></div><p>
				Each of the hosts retrieved during host discovery can have multiple storage disks. The storage disks are listed for the host on the <span class="strong strong"><strong>Storage</strong></span> page of the Assisted Installer wizard.
			</p><p>
				You can optionally modify the default configurations for each disk.
			</p><h4 id="changing_the_installation_disk">Changing the installation disk</h4><p>
				The Assisted Installer randomly assigns an installation disk by default. If there are multiple storage disks for a host, you can select a different disk to be the installation disk. This automatically unassigns the previous disk.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Navigate to the <span class="strong strong"><strong>Storage</strong></span> page of the wizard.
					</li><li class="listitem">
						Expand a host to display the associated storage disks.
					</li><li class="listitem">
						Select <span class="strong strong"><strong>Installation disk</strong></span> from the <span class="strong strong"><strong>Role</strong></span> list.
					</li><li class="listitem">
						When all storage disks return to <span class="strong strong"><strong>Ready</strong></span> status, proceed to the next step.
					</li></ol></div><h4 id="disabling_disk_formatting">Disabling disk formatting</h4><p>
				The Assisted Installer marks all bootable disks for formatting during the installation process by default, regardless of whether or not they have been defined as the installation disk. Formatting causes data loss.
			</p><p>
				You can choose to disable the formatting of a specific disk. This should be performed with caution, as bootable disks may interfere with the installation process, mainly in terms of boot order.
			</p><p>
				You cannot disable formatting for the installation disk.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Navigate to the <span class="strong strong"><strong>Storage</strong></span> page of the wizard.
					</li><li class="listitem">
						Expand a host to display the associated storage disks.
					</li><li class="listitem">
						Clear <span class="strong strong"><strong>Format</strong></span> for a disk.
					</li><li class="listitem">
						When all storage disks return to <span class="strong strong"><strong>Ready</strong></span> status, proceed to the next step.
					</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						<a class="link" href="#configuring-hosts_installing-with-ui" title="3.6. Configuring hosts">Configuring hosts</a>
					</li></ul></div></section><section class="section" id="configuring-networking_installing-with-ui"><div class="titlepage"><div><div><h2 class="title">3.8. Configuring networking</h2></div></div></div><p>
				Before installing OpenShift Container Platform, you must configure the cluster network.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						In the <span class="strong strong"><strong>Networking</strong></span> page, select one of the following if it is not already selected for you:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								<span class="strong strong"><strong>Cluster-Managed Networking:</strong></span> Selecting cluster-managed networking means that the Assisted Installer will configure a standard network topology, including <code class="literal">keepalived</code> and Virtual Router Redundancy Protocol (VRRP) for managing the API and Ingress VIP addresses.
							</p><div class="admonition note"><div class="admonition_header">Note</div><div><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
											Currently, Cluster-Managed Networking is not supported on IBM Z® and IBM Power® in OpenShift Container Platform version 4.13.
										</li><li class="listitem">
											Oracle Cloud Infrastructure (OCI) is available for OpenShift Container Platform 4.14 with a user-managed networking configuration only.
										</li></ul></div></div></div></li><li class="listitem">
								<span class="strong strong"><strong>User-Managed Networking</strong></span>: Selecting user-managed networking allows you to deploy OpenShift Container Platform with a non-standard network topology. For example, if you want to deploy with an external load balancer instead of <code class="literal">keepalived</code> and VRRP, or if you intend to deploy the cluster nodes across many distinct L2 network segments.
							</li></ul></div></li><li class="listitem"><p class="simpara">
						For cluster-managed networking, configure the following settings:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
								Define the <span class="strong strong"><strong>Machine network</strong></span>. You can use the default network or select a subnet.
							</li><li class="listitem">
								Define an <span class="strong strong"><strong>API virtual IP</strong></span>. An API virtual IP provides an endpoint for all users to interact with, and configure the platform.
							</li><li class="listitem">
								Define an <span class="strong strong"><strong>Ingress virtual IP</strong></span>. An Ingress virtual IP provides an endpoint for application traffic flowing from outside the cluster.
							</li></ol></div></li><li class="listitem"><p class="simpara">
						For user-managed networking, configure the following settings:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Select your <span class="strong strong"><strong>Networking stack type</strong></span>:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										<span class="strong strong"><strong>IPv4</strong></span>: Select this type when your hosts are only using IPv4.
									</li><li class="listitem">
										<span class="strong strong"><strong>Dual-stack</strong></span>: You can select dual-stack when your hosts are using IPv4 together with IPv6.
									</li></ul></div></li><li class="listitem">
								Define the <span class="strong strong"><strong>Machine network</strong></span>. You can use the default network or select a subnet.
							</li><li class="listitem">
								Define an <span class="strong strong"><strong>API virtual IP</strong></span>. An API virtual IP provides an endpoint for all users to interact with, and configure the platform.
							</li><li class="listitem">
								Define an <span class="strong strong"><strong>Ingress virtual IP</strong></span>. An Ingress virtual IP provides an endpoint for application traffic flowing from outside the cluster.
							</li><li class="listitem">
								Optional: You can select <span class="strong strong"><strong>Allocate IPs via DHCP server</strong></span> to automatically allocate the <span class="strong strong"><strong>API IP</strong></span> and <span class="strong strong"><strong>Ingress IP</strong></span> using the DHCP server.
							</li></ol></div></li><li class="listitem"><p class="simpara">
						Optional: Select <span class="strong strong"><strong>Use advanced networking</strong></span> to configure the following advanced networking properties:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								<span class="strong strong"><strong>Cluster network CIDR</strong></span>: Define an IP address block from which Pod IP addresses are allocated.
							</li><li class="listitem">
								<span class="strong strong"><strong>Cluster network host prefix</strong></span>: Define a subnet prefix length to assign to each node.
							</li><li class="listitem">
								<span class="strong strong"><strong>Service network CIDR</strong></span>: Define an IP address to use for service IP addresses.
							</li><li class="listitem">
								<span class="strong strong"><strong>Network type</strong></span>: Select either <span class="strong strong"><strong>Software-Defined Networking (SDN)</strong></span> for standard networking or <span class="strong strong"><strong>Open Virtual Networking (OVN)</strong></span> for IPv6, dual-stack networking, and telco features. In OpenShift Container Platform 4.12 and later releases, OVN is the default Container Network Interface (CNI). In OpenShift Container Platform 4.15 and later releases, <span class="strong strong"><strong>Software-Defined Networking (SDN)</strong></span> is not supported.
							</li></ul></div></li></ol></div><div class="itemizedlist _additional_resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional_resources" type="disc"><li class="listitem">
						<a class="link" href="#assembly_network-configuration" title="Chapter 11. Network configuration">Network configuration</a>
					</li></ul></div></section><section class="section" id="adding-custom-manifests_installing-with-ui"><div class="titlepage"><div><div><h2 class="title">3.9. Adding custom manifests</h2></div></div></div><p>
				A custom manifest is a JSON or YAML file that contains advanced configurations not currently supported in the Assisted Installer user interface. You can create a custom manifest or use one provided by a third party.
			</p><p>
				You can upload a custom manifest from your file system to either the <code class="literal">openshift</code> folder or the <code class="literal">manifests</code> folder. There is no limit to the number of custom manifest files permitted.
			</p><p>
				Only one file can be uploaded at a time. However, each uploaded YAML file can contain multiple custom manifests. Uploading a multi-document YAML manifest is faster than adding the YAML files individually.
			</p><p>
				For a file containing a single custom manifest, accepted file extensions include <code class="literal">.yaml</code>, <code class="literal">.yml</code>, or <code class="literal">.json</code>.
			</p><div class="formalpara"><p class="title"><strong>Single custom manifest example</strong></p><p>
					
<pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: master
  name: 99-openshift-machineconfig-master-kargs
spec:
  kernelArguments:
    - loglevel=7</pre>

				</p></div><p>
				For a file containing multiple custom manifests, accepted file types include <code class="literal">.yaml</code> or <code class="literal">.yml</code>.
			</p><div class="formalpara"><p class="title"><strong>Multiple custom manifest example</strong></p><p>
					
<pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: master
  name: 99-openshift-machineconfig-master-kargs
spec:
  kernelArguments:
    - loglevel=7
---
apiVersion: machineconfiguration.openshift.io/v2
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
  name: 98-openshift-machineconfig-worker-kargs
spec:
  kernelArguments:
    - loglevel=5</pre>

				</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							When you install OpenShift Container Platform on the Oracle Cloud Infrastructure (OCI) external platform, you must add the custom manifests provided by Oracle. For additional external partner integrations such as vSphere or Nutanix, this step is optional.
						</li><li class="listitem">
							For more information about custom manifests, see <span class="emphasis"><em>Additional Resources</em></span>.
						</li></ul></div></div></div><h4 id="uploading_a_custom_manifest_in_the_assisted_installer_user_interface">Uploading a custom manifest in the Assisted Installer user interface</h4><p>
				When uploading a custom manifest, enter the manifest filename and select a destination folder.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have at least one custom manifest file saved in your file system.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						On the <span class="strong strong"><strong>Cluster details</strong></span> page of the wizard, select the <span class="strong strong"><strong>Include custom manifests</strong></span> checkbox.
					</li><li class="listitem">
						On the <span class="strong strong"><strong>Custom manifest</strong></span> page, in the <span class="strong strong"><strong>folder</strong></span> field, select the Assisted Installer folder where you want to save the custom manifest file. Options include <span class="strong strong"><strong>openshift</strong></span> or <span class="strong strong"><strong>manifest</strong></span>.
					</li><li class="listitem">
						In the <span class="strong strong"><strong>Filename</strong></span> field, enter a name for the manifest file, including the extension. For example, <span class="strong strong"><strong>manifest1.json</strong></span> or <span class="strong strong"><strong>multiple1.yaml</strong></span>.
					</li><li class="listitem">
						Under <span class="strong strong"><strong>Content</strong></span>, click the <span class="strong strong"><strong>Upload</strong></span> icon or <span class="strong strong"><strong>Browse</strong></span> button to upload a file. Alternatively, drag the file into the <span class="strong strong"><strong>Content</strong></span> field from your file system.
					</li><li class="listitem">
						To upload another manifest, click <span class="strong strong"><strong>Add another manifest</strong></span> and repeat the process. This saves the previously uploaded manifest.
					</li><li class="listitem">
						Click <span class="strong strong"><strong>Next</strong></span> to save all manifests and proceed to the <span class="strong strong"><strong>Review and create</strong></span> page. The uploaded custom manifests are listed under <span class="strong strong"><strong>Custom manifests</strong></span>.
					</li></ol></div><h4 id="modifying_a_custom_manifest_in_the_assisted_installer_user_interface">Modifying a custom manifest in the Assisted Installer user interface</h4><p>
				You can change the folder and file name of an uploaded custom manifest. You can also copy the content of an existing manifest, or download it to the folder defined in the Chrome download settings.
			</p><p>
				It is not possible to modify the content of an uploaded manifest. However, you can overwrite the file.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have uploaded at least one custom manifest file.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						To change the folder, select a different folder for the manifest from the <span class="strong strong"><strong>Folder</strong></span> list.
					</li><li class="listitem">
						To modify the file name, type the new name for the manifest in the <span class="strong strong"><strong>File name</strong></span> field.
					</li><li class="listitem">
						To overwrite a manifest, save the new manifest in the same folder with the same file name.
					</li><li class="listitem">
						To save a manifest as a file in your file system, click the <span class="strong strong"><strong>Download</strong></span> icon.
					</li><li class="listitem">
						To copy the manifest, click the <span class="strong strong"><strong>Copy to clipboard</strong></span> icon.
					</li><li class="listitem">
						To apply the changes, click either <span class="strong strong"><strong>Add another manifest</strong></span> or <span class="strong strong"><strong>Next</strong></span>.
					</li></ol></div><h4 id="removing_custom_manifests_in_the_assisted_installer_user_interface">Removing custom manifests in the Assisted Installer user interface</h4><p>
				You can remove uploaded custom manifests before installation in one of two ways:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Removing one or more manifests individually.
					</li><li class="listitem">
						Removing all manifests at once.
					</li></ul></div><p>
				Once you have removed a manifest you cannot undo the action. The workaround is to upload the manifest again.
			</p><h5 id="removing_a_single_manifest">Removing a single manifest</h5><p>
				You can delete one manifest at a time. This option does not allow you to delete the last remaining manifest.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have uploaded at least two custom manifest files.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Navigate to the <span class="strong strong"><strong>Custom manifests</strong></span> page.
					</li><li class="listitem">
						Hover over the manifest name to display the <span class="strong strong"><strong>Delete</strong></span> (minus) icon.
					</li><li class="listitem">
						Click the icon and then click <span class="strong strong"><strong>Delete</strong></span> in the dialog box.
					</li></ol></div><h5 id="removing_all_manifests">Removing all manifests</h5><p>
				You can remove all custom manifests at once. This also hides the <span class="strong strong"><strong>Custom manifest</strong></span> page.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have uploaded at least one custom manifest file.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Navigate to the <span class="strong strong"><strong>Cluster details</strong></span> page of the wizard.
					</li><li class="listitem">
						Clear the <span class="strong strong"><strong>Include custom manifests</strong></span> checkbox.
					</li><li class="listitem">
						In the <span class="strong strong"><strong>Remove custom manifests</strong></span> dialog box, click <span class="strong strong"><strong>Remove</strong></span>.
					</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						<a class="link mimir-link-warn-approx" href="/products/#letter-O" title="Mimir does not include this page, but the link has been rewritten to point to the nearest parent document.">Manifest configuration files</a>
					</li><li class="listitem">
						<a class="link mimir-link-warn" href="https://www.yaml.info/learn/document.html" title="Mimir does not include content from: www.yaml.info">Multi-document YAML files</a>
					</li></ul></div></section><section class="section" id="preinstallation-validations_ui"><div class="titlepage"><div><div><h2 class="title">3.10. Preinstallation validations</h2></div></div></div><p>
				The Assisted Installer ensures the cluster meets the prerequisites before installation, because it eliminates complex postinstallation troubleshooting, thereby saving significant amounts of time and effort. Before installing the cluster, ensure the cluster and each host pass preinstallation validation.
			</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						<a class="link" href="#assembly_preinstallation-validations" title="Chapter 10. Preinstallation validations">Preinstallation validation</a>
					</li></ul></div></section><section class="section" id="installing-the-cluster_installing-with-ui"><div class="titlepage"><div><div><h2 class="title">3.11. Installing the cluster</h2></div></div></div><p>
				After you have completed the configuration and all the nodes are <span class="strong strong"><strong>Ready</strong></span>, you can begin installation. The installation process takes a considerable amount of time, and you can monitor the installation from the Assisted Installer web console. Nodes will reboot during the installation, and they will initialize after installation.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Press <span class="strong strong"><strong>Begin installation</strong></span>.
					</li><li class="listitem">
						Click the link in the <span class="strong strong"><strong>Status</strong></span> column of the <span class="strong strong"><strong>Host Inventory</strong></span> list to see the installation status of a particular host.
					</li></ol></div></section><section class="section" id="completing-the-installation_installing-with-ui"><div class="titlepage"><div><div><h2 class="title">3.12. Completing the installation</h2></div></div></div><p>
				After the cluster is installed and initialized, the Assisted Installer indicates that the installation is finished. The Assisted Installer provides the console URL, the <code class="literal">kubeadmin</code> username and password, and the <code class="literal">kubeconfig</code> file. Additionally, the Assisted Installer provides cluster details including the OpenShift Container Platform version, base domain, CPU architecture, API and Ingress IP addresses, and the cluster and service network IP addresses.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have installed the <code class="literal">oc</code> CLI tool.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Make a copy of the <code class="literal">kubeadmin</code> username and password.
					</li><li class="listitem"><p class="simpara">
						Download the <code class="literal">kubeconfig</code> file and copy it to the <code class="literal">auth</code> directory under your working directory:
					</p><pre class="programlisting language-terminal">$ mkdir -p &lt;working_directory&gt;/auth</pre><pre class="programlisting language-terminal">$ cp kubeconfig &lt;working_directory&gt;/auth</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The <code class="literal">kubeconfig</code> file is available for download for 20 days after completing the installation.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Add the <code class="literal">kubeconfig</code> file to your environment:
					</p><pre class="programlisting language-terminal">$ export KUBECONFIG=&lt;your working directory&gt;/auth/kubeconfig</pre></li><li class="listitem"><p class="simpara">
						Login with the <code class="literal">oc</code> CLI tool:
					</p><pre class="programlisting language-terminal">$ oc login -u kubeadmin -p &lt;password&gt;</pre><p class="simpara">
						Replace <code class="literal">&lt;password&gt;</code> with the password of the <code class="literal">kubeadmin</code> user.
					</p></li><li class="listitem">
						Click the web console URL or click <span class="strong strong"><strong>Launch OpenShift Console</strong></span> to open the console.
					</li><li class="listitem">
						Enter the <code class="literal">kubeadmin</code> username and password. Follow the instructions in the OpenShift Container Platform console to configure an identity provider and configure alert receivers.
					</li><li class="listitem">
						Add a bookmark of the OpenShift Container Platform console.
					</li><li class="listitem">
						Complete any postinstallation platform integration steps.
					</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						<a class="link" href="#nutanix-post-installation-configuration_installing-on-nutanix" title="13.3. Nutanix postinstallation configuration">Nutanix postinstallation configuration</a>
					</li><li class="listitem">
						<a class="link" href="#vsphere-post-installation-configuration_installing-on-vsphere" title="14.2. vSphere postinstallation configuration using the CLI">vSphere postinstallation configuration</a>
					</li></ul></div></section></section><section class="chapter" id="installing-with-api"><div class="titlepage"><div><div><h1 class="title">Chapter 4. Installing with the Assisted Installer API</h1></div></div></div><p>
			After you ensure the cluster nodes and network requirements are met, you can begin installing the cluster by using the Assisted Installer API. To use the API, you must perform the following procedures:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					Set up the API authentication.
				</li><li class="listitem">
					Configure the pull secret.
				</li><li class="listitem">
					Register a new cluster definition.
				</li><li class="listitem">
					Create an infrastructure environment for the cluster.
				</li></ul></div><p>
			Once you perform these steps, you can modify the cluster definition, create discovery ISOs, add hosts to the cluster, and install the cluster. This document does not cover every endpoint of the <a class="link mimir-link-warn" href="https://api.openshift.com/?urls.primaryName=assisted-service%20service" title="Mimir does not include content from: api.openshift.com">Assisted Installer API</a>, but you can review all of the endpoints in the <a class="link mimir-link-warn" href="https://api.openshift.com/?urls.primaryName=assisted-service%20service" title="Mimir does not include content from: api.openshift.com">API viewer</a> or the <a class="link mimir-link-warn" href="https://github.com/openshift/assisted-service/blob/master/swagger.yaml" title="Mimir does not include content from: github.com">swagger.yaml</a> file.
		</p><section class="section" id="generating-the-offline-token-cli_installing-with-api"><div class="titlepage"><div><div><h2 class="title">4.1. Generating the offline token</h2></div></div></div><p>
				Download the offline token from the Assisted Installer web console. You will use the offline token to set the API token.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Install <code class="literal">jq</code>.
					</li><li class="listitem">
						Log in to the <a class="link mimir-link-warn" href="https://console.redhat.com/openshift/assisted-installer/clusters" title="Mimir does not include content from: console.redhat.com">OpenShift Cluster Manager</a> as a user with cluster creation privileges.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						In the menu, click <span class="strong strong"><strong>Downloads</strong></span>.
					</li><li class="listitem">
						In the <span class="strong strong"><strong>Tokens</strong></span> section under <span class="strong strong"><strong>OpenShift Cluster Manager API Token</strong></span>, click <span class="strong strong"><strong>View API Token</strong></span>.
					</li><li class="listitem"><p class="simpara">
						Click <span class="strong strong"><strong>Load Token</strong></span>.
					</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							Disable pop-up blockers.
						</p></div></div></li><li class="listitem">
						In the <span class="strong strong"><strong>Your API token</strong></span> section, copy the offline token.
					</li><li class="listitem"><p class="simpara">
						In your terminal, set the offline token to the <code class="literal">OFFLINE_TOKEN</code> variable:
					</p><pre class="programlisting language-terminal">$ export OFFLINE_TOKEN=&lt;copied_token&gt;</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
						To make the offline token permanent, add it to your profile.
					</p></div></div></li><li class="listitem"><p class="simpara">
						(Optional) Confirm the <code class="literal">OFFLINE_TOKEN</code> variable definition.
					</p><pre class="programlisting language-terminal">$ echo ${OFFLINE_TOKEN}</pre></li></ol></div></section><section class="section" id="authenticating-with-the-rest-api_installing-with-api"><div class="titlepage"><div><div><h2 class="title">4.2. Authenticating with the REST API</h2></div></div></div><p>
				API calls require authentication with the API token. Assuming you use <code class="literal">API_TOKEN</code> as a variable name, add <code class="literal">-H "Authorization: Bearer ${API_TOKEN}"</code> to API calls to authenticate with the REST API.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					The API token expires after 15 minutes.
				</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have generated the <code class="literal">OFFLINE_TOKEN</code> variable.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						On the command line terminal, set the <code class="literal">API_TOKEN</code> variable using the <code class="literal">OFFLINE_TOKEN</code> to validate the user.
					</p><pre class="programlisting language-terminal">$ export API_TOKEN=$( \
  curl \
  --silent \
  --header "Accept: application/json" \
  --header "Content-Type: application/x-www-form-urlencoded" \
  --data-urlencode "grant_type=refresh_token" \
  --data-urlencode "client_id=cloud-services" \
  --data-urlencode "refresh_token=${OFFLINE_TOKEN}" \
  "https://sso.redhat.com/auth/realms/redhat-external/protocol/openid-connect/token" \
  | jq --raw-output ".access_token" \
)</pre></li></ol></div><div class="orderedlist"><ol class="orderedlist" start="2" type="1"><li class="listitem"><p class="simpara">
						Confirm the <code class="literal">API_TOKEN</code> variable definition:
					</p><pre class="programlisting language-terminal">$ echo ${API_TOKEN}</pre></li><li class="listitem"><p class="simpara">
						Create a script in your path for one of the token generating methods. For example:
					</p><pre class="programlisting language-terminal">$ vim ~/.local/bin/refresh-token</pre><pre class="programlisting language-terminal">export API_TOKEN=$( \
  curl \
  --silent \
  --header "Accept: application/json" \
  --header "Content-Type: application/x-www-form-urlencoded" \
  --data-urlencode "grant_type=refresh_token" \
  --data-urlencode "client_id=cloud-services" \
  --data-urlencode "refresh_token=${OFFLINE_TOKEN}" \
  "https://sso.redhat.com/auth/realms/redhat-external/protocol/openid-connect/token" \
  | jq --raw-output ".access_token" \
)</pre><p class="simpara">
						Then, save the file.
					</p></li><li class="listitem"><p class="simpara">
						Change the file mode to make it executable:
					</p><pre class="programlisting language-terminal">$ chmod +x ~/.local/bin/refresh-token</pre></li><li class="listitem"><p class="simpara">
						Refresh the API token:
					</p><pre class="programlisting language-terminal">$ source refresh-token</pre></li><li class="listitem"><p class="simpara">
						Verify that you can access the API by running the following command:
					</p><pre class="programlisting language-terminal">$ curl -s https://api.openshift.com/api/assisted-install/v2/component-versions -H "Authorization: Bearer ${API_TOKEN}" | jq</pre><div class="white-space-pre white-space-pre"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">{
  "release_tag": "v2.11.3",
  "versions": {
    "assisted-installer": "registry.redhat.io/rhai-tech-preview/assisted-installer-rhel8:v1.0.0-211",
    "assisted-installer-controller": "registry.redhat.io/rhai-tech-preview/assisted-installer-reporter-rhel8:v1.0.0-266",
    "assisted-installer-service": "quay.io/app-sre/assisted-service:78d113a",
    "discovery-agent": "registry.redhat.io/rhai-tech-preview/assisted-installer-agent-rhel8:v1.0.0-195"
  }
}</pre>

						</p></div></li></ol></div></section><section class="section" id="configuring-the-pull-secret_installing-with-api"><div class="titlepage"><div><div><h2 class="title">4.3. Configuring the pull secret</h2></div></div></div><p>
				Many of the Assisted Installer API calls require the pull secret. Download the pull secret to a file so that you can reference it in API calls. The pull secret is a JSON object that will be included as a value within the request’s JSON object. The pull secret JSON must be formatted to escape the quotes. For example:
			</p><div class="formalpara"><p class="title"><strong>Before</strong></p><p>
					
<pre class="programlisting language-javscript">{"auths":{"cloud.openshift.com": ...</pre>

				</p></div><div class="formalpara"><p class="title"><strong>After</strong></p><p>
					
<pre class="programlisting language-javascript">{\"auths\":{\"cloud.openshift.com\": ...</pre>

				</p></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						In the menu, click <span class="strong strong"><strong>OpenShift</strong></span>.
					</li><li class="listitem">
						In the submenu, click <span class="strong strong"><strong>Downloads</strong></span>.
					</li><li class="listitem">
						In the <span class="strong strong"><strong>Tokens</strong></span> section under <span class="strong strong"><strong>Pull secret</strong></span>, click <span class="strong strong"><strong>Download</strong></span>.
					</li><li class="listitem"><p class="simpara">
						To use the pull secret from a shell variable, execute the following command:
					</p><pre class="programlisting language-terminal white-space-pre white-space-pre">$ export PULL_SECRET=$(cat ~/Downloads/pull-secret.txt | jq -R .)</pre></li><li class="listitem"><p class="simpara">
						To slurp the pull secret file using <code class="literal">jq</code>, reference it in the <code class="literal">pull_secret</code> variable, piping the value to <code class="literal">tojson</code> to ensure that it is properly formatted as escaped JSON. For example:
					</p><pre class="programlisting language-terminal white-space-pre white-space-pre">$ curl https://api.openshift.com/api/assisted-install/v2/clusters \
-H "Authorization: Bearer ${API_TOKEN}" \
-H "Content-Type: application/json" \
-d "$(jq --null-input \
        --slurpfile pull_secret ~/Downloads/pull-secret.txt ' <span id="CO4-1"><!--Empty--></span><span class="callout">1</span>
    {
        "name": "testcluster",
        "high_availability_mode": "None",
        "openshift_version": "4.11",
        "pull_secret": $pull_secret[0] | tojson, <span id="CO4-2"><!--Empty--></span><span class="callout">2</span>
        "base_dns_domain": "example.com"
    }
')"</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO4-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Slurp the pull secret file.
							</div></dd><dt><a href="#CO4-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Format the pull secret to escaped JSON format.
							</div></dd></dl></div></li><li class="listitem"><p class="simpara">
						Confirm the <code class="literal">PULL_SECRET</code> variable definition:
					</p><pre class="programlisting language-terminal white-space-pre white-space-pre">$ echo ${PULL_SECRET}</pre></li></ol></div></section><section class="section" id="generating-the-ssh-public-key_installing-with-api"><div class="titlepage"><div><div><h2 class="title">4.4. Optional: Generating the SSH public key</h2></div></div></div><p>
				During the installation of OpenShift Container Platform, you can optionally provide an SSH public key to the installation program. This is useful for initiating an SSH connection to a remote node when troubeshooting an installation error.
			</p><p>
				If you do not have an existing SSH key pair on your local machine to use for the authentication, create one now.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Generate the <code class="literal">OFFLINE_TOKEN</code> and <code class="literal">API_TOKEN</code> variables.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						From the root user in your terminal, get the SSH public key:
					</p><pre class="programlisting language-terminal">$ cat /root/.ssh/id_rsa.pub</pre></li><li class="listitem"><p class="simpara">
						Set the SSH public key to the <code class="literal">CLUSTER_SSHKEY</code> variable:
					</p><pre class="programlisting language-terminal">$ CLUSTER_SSHKEY=&lt;downloaded_ssh_key&gt;</pre></li><li class="listitem"><p class="simpara">
						Confirm the <code class="literal">CLUSTER_SSHKEY</code> variable definition:
					</p><pre class="programlisting language-terminal">$ echo ${CLUSTER_SSHKEY}</pre></li></ol></div></section><section class="section" id="registering-a-new-cluster_installing-with-api"><div class="titlepage"><div><div><h2 class="title">4.5. Registering a new cluster</h2></div></div></div><p>
				To register a new cluster definition with the API, use the <a class="link mimir-link-warn" href="https://api.openshift.com/?urls.primaryName=assisted-service%20service#/installer/v2RegisterCluster" title="Mimir does not include content from: api.openshift.com">/v2/clusters</a> endpoint.
			</p><p>
				The following parameters are mandatory:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<code class="literal">name</code>
					</li><li class="listitem">
						<code class="literal">openshift-version</code>
					</li><li class="listitem">
						<code class="literal">pull_secret</code>
					</li><li class="listitem">
						<code class="literal">cpu_architecture</code>
					</li></ul></div><p>
				See the <code class="literal">cluster-create-params</code> model in the <a class="link mimir-link-warn" href="https://api.openshift.com/?urls.primaryName=assisted-service%20service" title="Mimir does not include content from: api.openshift.com">API viewer</a> for details on the fields you can set when registering a new cluster. When setting the <code class="literal">olm_operators</code> field, see <span class="emphasis"><em>Additional Resources</em></span> for details on installing Operators.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have generated a valid <code class="literal">API_TOKEN</code>. Tokens expire every 15 minutes.
					</li><li class="listitem">
						You have downloaded the pull secret.
					</li><li class="listitem">
						Optional: You have assigned the pull secret to the <code class="literal">$PULL_SECRET</code> variable.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Refresh the API token:
					</p><pre class="programlisting language-terminal">$ source refresh-token</pre></li><li class="listitem"><p class="simpara">
						Register a new cluster by using one of the following methods:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Register the cluster by referencing the pull secret file in the request:
							</p><pre class="programlisting language-terminal">$ curl -s -X POST https://api.openshift.com/api/assisted-install/v2/clusters \
  -H "Authorization: Bearer ${API_TOKEN}" \
  -H "Content-Type: application/json" \
  -d "$(jq --null-input \
      --slurpfile pull_secret ~/Downloads/pull-secret.txt ' \
  { \
      "name": "testcluster", \
      "openshift_version": "4.16", \ <span id="CO5-1"><!--Empty--></span><span class="callout">1</span>
      "cpu_architecture" : "&lt;architecture_name&gt;", \ <span id="CO5-2"><!--Empty--></span><span class="callout">2</span>
      "high_availability_mode": "&lt;mode&gt;", \ <span id="CO5-3"><!--Empty--></span><span class="callout">3</span>
      "base_dns_domain": "example.com", \
      "pull_secret": $pull_secret[0] | tojson \
  } \
  ')" | jq '.id'</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO5-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										To install the latest OpenShift version, use the <code class="literal">x.y</code> format, such as <code class="literal">4.16</code> for version 4.16.10. To install a specific OpenShift version, use the <code class="literal">x.y.z</code> format, such as <code class="literal">4.16.3</code> for version 4.16.3. To install a mixed architecture cluster, add the <code class="literal">-multi</code> extension, such as <code class="literal">4.16-multi</code> for the latest version or <code class="literal">4.16.3-multi</code> for a specific version.
									</div></dd><dt><a href="#CO5-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Valid values are <code class="literal">x86_64</code>, <code class="literal">arm64</code>, <code class="literal">ppc64le</code>, <code class="literal">s390x</code>, <code class="literal">multi</code>.
									</div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
											Specify <code class="literal">multi</code> for a mixed-architecture cluster.
										</p></div></div></dd><dt><a href="#CO5-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										Set the value to <code class="literal">Full</code> for a high-availability multi-node cluster or <code class="literal">None</code> for a single-node OpenShift cluster.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Register the cluster by writing the configuration to a JSON file:
							</p><pre class="programlisting language-terminal">$ cat &lt;&lt; EOF &gt; cluster.json
{
  "name": "testcluster",
  "openshift_version": "4.16", <span id="CO6-1"><!--Empty--></span><span class="callout">1</span>
  "high_availability_mode": "&lt;mode&gt;", <span id="CO6-2"><!--Empty--></span><span class="callout">2</span>
  "base_dns_domain": "example.com",
  "network_type": "examplenetwork",
  "cluster_network_cidr":"11.111.1.0/14"
  "cluster_network_host_prefix": 11,
  "service_network_cidr": "111.11.1.0/16",
  "api_vips":[{"ip": ""}],
  "ingress_vips": [{"ip": ""}],
  "vip_dhcp_allocation": false,
  "additional_ntp_source": "clock.redhat.com,clock2.redhat.com",
  "ssh_public_key": "$CLUSTER_SSHKEY",
  "pull_secret": $PULL_SECRET
}
EOF</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO6-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										To install the latest OpenShift version, use the <code class="literal">x.y</code> format, such as <code class="literal">4.16</code> for version 4.16.10. To install a specific OpenShift version, use the <code class="literal">x.y.z</code> format, such as <code class="literal">4.16.3</code> for version 4.16.3. To install a mixed architecture cluster, add the <code class="literal">-multi</code> extension, such as <code class="literal">4.16-multi</code> for the latest version or <code class="literal">4.16.3-multi</code> for a specific version.
									</div></dd><dt><a href="#CO6-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Set the value to <code class="literal">Full</code> for a high-availability multi-node cluster or <code class="literal">None</code> for a single-node OpenShift cluster.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Apply the configuration:
							</p><pre class="programlisting language-terminal">$ curl -s -X POST "https://api.openshift.com/api/assisted-install/v2/clusters" \
  -d @./cluster.json \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $API_TOKEN" \
  | jq '.id'</pre></li></ul></div></li><li class="listitem"><p class="simpara">
						Assign the returned <code class="literal">cluster_id</code> to the <code class="literal">CLUSTER_ID</code> variable and export it:
					</p><pre class="programlisting language-terminal">$ export CLUSTER_ID=&lt;cluster_id&gt;</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							If you close your terminal session, you need to export the <code class="literal">CLUSTER_ID</code> variable again in a new terminal session.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Check the status of the new cluster:
					</p><pre class="programlisting language-terminal">$ curl -s -X GET "https://api.openshift.com/api/assisted-install/v2/clusters/$CLUSTER_ID" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $API_TOKEN" \
  | jq</pre></li></ol></div><p>
				Once you register a new cluster definition, create the infrastructure environment for the cluster.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					You cannot see the cluster configuration settings in the Assisted Installer user interface until you create the infrastructure environment.
				</p></div></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						<a class="link" href="#modifying-a-cluster_installing-with-api" title="4.6. Modifying a cluster">Modifying a cluster</a>
					</li><li class="listitem">
						<a class="link" href="#installing-mixed-architecture-clusters_expanding-the-cluster" title="12.2. Installing a multi-architecture cluster">Installing a mixed-architecture cluster</a>
					</li><li class="listitem">
						<a class="link" href="#assembly_installing-on-nutanix" title="Chapter 13. Optional: Installing on Nutanix">Optional: Installing on Nutanix</a>
					</li><li class="listitem">
						<a class="link" href="#installing-on-vsphere" title="Chapter 14. Optional: Installing on vSphere">Optional: Installing on vSphere</a>
					</li><li class="listitem">
						<a class="link" href="#installing-on-oci" title="Chapter 15. Optional: Installing on Oracle Cloud Infrastructure (OCI)">Optional: Installing on Oracle Cloud Infrastructure</a>
					</li></ul></div><section class="section" id="installing-operators-api_installing-with-api"><div class="titlepage"><div><div><h3 class="title">4.5.1. Optional: Installing Operators</h3></div></div></div><p>
					You can install the following Operators when you register a new cluster:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							OpenShift Virtualization Operator
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Currently, OpenShift Virtualization is not supported on IBM Z® and IBM Power®.
							</p></div></div></li><li class="listitem">
							Multicluster engine Operator
						</li><li class="listitem">
							OpenShift Data Foundation Operator
						</li><li class="listitem">
							LVM Storage Operator
						</li></ul></div><p>
					If you require advanced options, install the Operators after you have installed the cluster.
				</p><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Run the following command:
						</p><pre class="programlisting language-terminal">$ curl -s -X POST https://api.openshift.com/api/assisted-install/v2/clusters \
-H "Authorization: Bearer ${API_TOKEN}" \
-H "Content-Type: application/json" \
-d "$(jq --null-input \
   --slurpfile pull_secret ~/Downloads/pull-secret.txt '
{
   "name": "testcluster",
   "openshift_version": "4.15",
   "cpu_architecture" : "x86_64",
   "base_dns_domain": "example.com",
  "olm_operators": [
  { "name": "mce" } <span id="CO7-1"><!--Empty--></span><span class="callout">1</span>
  ,
  { "name": "odf" } <span id="CO7-2"><!--Empty--></span><span class="callout">2</span>
    ]
   "pull_secret": $pull_secret[0] | tojson
}
')" | jq '.id'</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO7-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify <code class="literal">cnv</code> for OpenShift Virtualization, <code class="literal">mce</code> for multicluster engine, <code class="literal">odf</code> for OpenShift Data Foundation, or <code class="literal">lvm</code> for LVM Storage.
								</div></dd><dt><a href="#CO7-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									This example installs multicluster engine and OpenShift Data Foundation on a multi-node cluster. Specify <code class="literal">mce</code> and <code class="literal">lvm</code> for a single-node OpenShift cluster.
								</div></dd></dl></div></li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link mimir-link-warn" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html-single/virtualization/index#virt-getting-started" title="This content is not included in Mimir.">OpenShift Virtualization documentation</a>
						</li><li class="listitem">
							<a class="link mimir-link-warn" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html-single/architecture/index#ocm-overview-ocp" title="This content is not included in Mimir.">Red Hat OpenShift Cluster Manager documentation</a>
						</li><li class="listitem">
							<a class="link mimir-link-warn" href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation" title="This content is not included in Mimir.">Red Hat OpenShift Data Foundation documentation</a>
						</li><li class="listitem">
							<a class="link mimir-link-warn" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html-single/storage/index#persistent-storage-using-lvms" title="This content is not included in Mimir.">Logical Volume Manager Storage documentation</a>
						</li></ul></div></section></section><section class="section" id="modifying-a-cluster_installing-with-api"><div class="titlepage"><div><div><h2 class="title">4.6. Modifying a cluster</h2></div></div></div><p>
				To modify a cluster definition with the API, use the <a class="link mimir-link-warn" href="https://api.openshift.com/?urls.primaryName=assisted-service%20service#/installer/V2UpdateCluster" title="Mimir does not include content from: api.openshift.com">/v2/clusters/{cluster_id}</a> endpoint. Modifying a cluster resource is a common operation for adding settings such as changing the network type or enabling user-managed networking. See the <code class="literal">v2-cluster-update-params</code> model in the <a class="link mimir-link-warn" href="https://api.openshift.com/?urls.primaryName=assisted-service%20service" title="Mimir does not include content from: api.openshift.com">API viewer</a> for details on the fields you can set when modifying a cluster definition.
			</p><p>
				You can add or remove Operators from a cluster resource that has already been registered.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					To create partitions on nodes, see <a class="link mimir-link-warn" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html/installing/deploying-installer-provisioned-clusters-on-bare-metal#configuring-storage-on-nodes_ipi-install-installation-workflow" title="This content is not included in Mimir.">Configuring storage on nodes</a> in the OpenShift Container Platform documentation.
				</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have created a new cluster resource.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Refresh the API token:
					</p><pre class="programlisting language-terminal">$ source refresh-token</pre></li><li class="listitem"><p class="simpara">
						Modify the cluster. For example, change the SSH key:
					</p><pre class="programlisting language-terminal">$ curl https://api.openshift.com/api/assisted-install/v2/clusters/${CLUSTER_ID} \
-X PATCH \
-H "Authorization: Bearer ${API_TOKEN}" \
-H "Content-Type: application/json" \
-d '
{
    "ssh_public_key": "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDZrD4LMkAEeoU2vShhF8VM+cCZtVRgB7tqtsMxms2q3TOJZAgfuqReKYWm+OLOZTD+DO3Hn1pah/mU3u7uJfTUg4wEX0Le8zBu9xJVym0BVmSFkzHfIJVTn6SfZ81NqcalisGWkpmkKXVCdnVAX6RsbHfpGKk9YPQarmRCn5KzkelJK4hrSWpBPjdzkFXaIpf64JBZtew9XVYA3QeXkIcFuq7NBuUH9BonroPEmIXNOa41PUP1IWq3mERNgzHZiuU8Ks/pFuU5HCMvv4qbTOIhiig7vidImHPpqYT/TCkuVi5w0ZZgkkBeLnxWxH0ldrfzgFBYAxnpTU8Ih/4VhG538Ix1hxPaM6cXds2ic71mBbtbSrk+zjtNPaeYk1O7UpcCw4jjHspU/rVV/DY51D5gSiiuaFPBMucnYPgUxy4FMBFfGrmGLIzTKiLzcz0DiSz1jBeTQOX++1nz+KDLBD8CPdi5k4dq7lLkapRk85qdEvgaG5RlHMSPSS3wDrQ51fD8= user@hostname"
}
' | jq</pre></li></ol></div><section class="section" id="modifying-operators-api_installing-with-api"><div class="titlepage"><div><div><h3 class="title">4.6.1. Modifying Operators</h3></div></div></div><p>
					You can add or remove Operators from a cluster resource that has already been registered as part of a previous installation. This is only possible before you start the OpenShift Container Platform installation.
				</p><p>
					You set the required Operator definition by using the PATCH method for the <a class="link mimir-link-warn" href="https://api.openshift.com/?urls.primaryName=assisted-service%20service#/installer/V2UpdateCluster" title="Mimir does not include content from: api.openshift.com">/v2/clusters/{cluster_id}</a> endpoint.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have refreshed the API token.
						</li><li class="listitem">
							You have exported the <code class="literal">CLUSTER_ID</code> as an environment variable.
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Run the following command to modify the Operators:
						</p><pre class="programlisting language-terminal">$ curl https://api.openshift.com/api/assisted-install/v2/clusters/${CLUSTER_ID} \
-X PATCH \
-H "Authorization: Bearer ${API_TOKEN}" \
-H "Content-Type: application/json" \
-d '
{
    "olm_operators": [{"name": "mce"}, {"name": "cnv"}], <span id="CO8-1"><!--Empty--></span><span class="callout">1</span>
}
' | jq '.id'</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO8-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify <code class="literal">cnv</code> for OpenShift Virtualization, <code class="literal">mce</code> for multicluster engine, <code class="literal">odf</code> for Red Hat OpenShift Data Foundation, or <code class="literal">lvm</code> for Logical Volume Manager Storage. To remove a previously installed Operator, exclude it from the list of values. To remove all previously installed Operators, specify an empty array: <code class="literal">"olm_operators": []</code>.
								</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Sample output</strong></p><p>
								
<pre class="programlisting language-terminal">{
  &lt;various cluster properties&gt;,
  "monitored_operators": [
    {
      "cluster_id": "b5259f97-be09-430e-b5eb-d78420ee509a",
      "name": "console",
      "operator_type": "builtin",
      "status_updated_at": "0001-01-01T00:00:00.000Z",
      "timeout_seconds": 3600
    },
    {
      "cluster_id": "b5259f97-be09-430e-b5eb-d78420ee509a",
      "name": "cvo",
      "operator_type": "builtin",
      "status_updated_at": "0001-01-01T00:00:00.000Z",
      "timeout_seconds": 3600
    },
    {
      "cluster_id": "b5259f97-be09-430e-b5eb-d78420ee509a",
      "name": "mce",
      "namespace": "multicluster-engine",
      "operator_type": "olm",
      "status_updated_at": "0001-01-01T00:00:00.000Z",
      "subscription_name": "multicluster-engine",
      "timeout_seconds": 3600
    },
    {
      "cluster_id": "b5259f97-be09-430e-b5eb-d78420ee509a",
      "name": "cnv",
      "namespace": "openshift-cnv",
      "operator_type": "olm",
      "status_updated_at": "0001-01-01T00:00:00.000Z",
      "subscription_name": "hco-operatorhub",
      "timeout_seconds": 3600
    },
    {
      "cluster_id": "b5259f97-be09-430e-b5eb-d78420ee509a",
      "name": "lvm",
      "namespace": "openshift-local-storage",
      "operator_type": "olm",
      "status_updated_at": "0001-01-01T00:00:00.000Z",
      "subscription_name": "local-storage-operator",
      "timeout_seconds": 4200
    }
  ],
  &lt;more cluster properties&gt;</pre>

							</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The output is the description of the new cluster state. The <code class="literal">monitored_operators</code> property in the output contains Operators of two types:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
										<code class="literal">"operator_type": "builtin"</code>: Operators of this type are an integral part of OpenShift Container Platform.
									</li><li class="listitem">
										<code class="literal">"operator_type": "olm"</code>: Operators of this type are added manually by a user or automatically, as a dependency. In this example, the LVM Storage Operator is added automatically as a dependency of OpenShift Virtualization.
									</li></ul></div></div></div></li></ul></div></section></section><section class="section" id="registering-a-new-infrastructure-environment_installing-with-api"><div class="titlepage"><div><div><h2 class="title">4.7. Registering a new infrastructure environment</h2></div></div></div><p>
				Once you register a new cluster definition with the Assisted Installer API, create an infrastructure environment using the <a class="link mimir-link-warn" href="https://api.openshift.com/?urls.primaryName=assisted-service%20service#/installer/RegisterInfraEnv" title="Mimir does not include content from: api.openshift.com">v2/infra-envs</a> endpoint. Registering a new infrastructure environment requires the following settings:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<code class="literal">name</code>
					</li><li class="listitem">
						<code class="literal">pull_secret</code>
					</li><li class="listitem">
						<code class="literal">cpu_architecture</code>
					</li></ul></div><p>
				See the <code class="literal">infra-env-create-params</code> model in the <a class="link mimir-link-warn" href="https://api.openshift.com/?urls.primaryName=assisted-service%20service" title="Mimir does not include content from: api.openshift.com">API viewer</a> for details on the fields you can set when registering a new infrastructure environment. You can modify an infrastructure environment after you create it. As a best practice, consider including the <code class="literal">cluster_id</code> when creating a new infrastructure environment. The <code class="literal">cluster_id</code> will associate the infrastructure environment with a cluster definition. When creating the new infrastructure environment, the Assisted Installer will also generate a discovery ISO.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have generated a valid <code class="literal">API_TOKEN</code>. Tokens expire every 15 minutes.
					</li><li class="listitem">
						You have downloaded the pull secret.
					</li><li class="listitem">
						Optional: You have registered a new cluster definition and exported the <code class="literal">cluster_id</code>.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Refresh the API token:
					</p><pre class="programlisting language-terminal">$ source refresh-token</pre></li><li class="listitem"><p class="simpara">
						Register a new infrastructure environment. Provide a name, preferably something including the cluster name. This example provides the cluster ID to associate the infrastructure environment with the cluster resource. The following example specifies the <code class="literal">image_type</code>. You can specify either <code class="literal">full-iso</code> or <code class="literal">minimal-iso</code>. The default value is <code class="literal">minimal-iso</code>.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Optional: You can register a new infrastructure environment by slurping the pull secret file in the request:
							</p><pre class="programlisting language-terminal">$ curl https://api.openshift.com/api/assisted-install/v2/infra-envs \
-H "Authorization: Bearer ${API_TOKEN}" \
-H "Content-Type: application/json" \
-d "$(jq --null-input \
  --slurpfile pull_secret ~/Downloads/pull-secret.txt \
  --arg cluster_id ${CLUSTER_ID} '
    {
      "name": "testcluster-infra-env",
      "image_type":"full-iso",
      "cluster_id": $cluster_id,
      "cpu_architecture" : "&lt;architecture_name&gt;", <span id="CO9-1"><!--Empty--></span><span class="callout">1</span>
      "pull_secret": $pull_secret[0] | tojson
    }
')" | jq '.id'</pre><div class="admonition note"><div class="admonition_header">Note</div><div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO9-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Indicates the valid values. They are: <code class="literal">x86_64</code>, <code class="literal">arm64</code>, <code class="literal">ppc64le</code>, <code class="literal">s390x</code>, <code class="literal">multi</code>
										</div></dd></dl></div></div></div></li><li class="listitem"><p class="simpara">
								Optional: You can register a new infrastructure environment by writing the configuration to a JSON file and then referencing it in the request:
							</p><pre class="programlisting language-terminal">$ cat &lt;&lt; EOF &gt; infra-envs.json
{
 "name": "testcluster",
 "pull_secret": $PULL_SECRET,
 "proxy": {
    "http_proxy": "",
    "https_proxy": "",
    "no_proxy": ""
  },
  "ssh_authorized_key": "$CLUSTER_SSHKEY",
  "image_type": "full-iso",
  "cluster_id": "${CLUSTER_ID}",
  "openshift_version": "4.11"
}
EOF</pre><pre class="programlisting language-terminal white-space-pre white-space-pre">$ curl -s -X POST "https://api.openshift.com/api/assisted-install/v2/infra-envs"
 -d @./infra-envs.json
 -H "Content-Type: application/json"
 -H "Authorization: Bearer $API_TOKEN"
 | jq '.id'</pre></li></ol></div></li><li class="listitem"><p class="simpara">
						Assign the returned <code class="literal">id</code> to the <code class="literal">INFRA_ENV_ID</code> variable and export it:
					</p><pre class="programlisting language-terminal">$ export INFRA_ENV_ID=&lt;id&gt;</pre></li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Once you create an infrastructure environment and associate it to a cluster definition via the <code class="literal">cluster_id</code>, you can see the cluster settings in the Assisted Installer web user interface. If you close your terminal session, you need to re-export the <code class="literal">id</code> in a new terminal session.
				</p></div></div></section><section class="section" id="modifying-an-infrastructure-environment_installing-with-api"><div class="titlepage"><div><div><h2 class="title">4.8. Modifying an infrastructure environment</h2></div></div></div><p>
				You can modify an infrastructure environment using the <a class="link mimir-link-warn" href="https://api.openshift.com/?urls.primaryName=assisted-service%20service#/installer/UpdateInfraEnv" title="Mimir does not include content from: api.openshift.com">/v2/infra-envs/{infra_env_id}</a> endpoint. Modifying an infrastructure environment is a common operation for adding settings such as networking, SSH keys, or ignition configuration overrides.
			</p><p>
				See the <code class="literal">infra-env-update-params</code> model in the <a class="link mimir-link-warn" href="https://api.openshift.com/?urls.primaryName=assisted-service%20service" title="Mimir does not include content from: api.openshift.com">API viewer</a> for details on the fields you can set when modifying an infrastructure environment. When modifying the new infrastructure environment, the Assisted Installer will also re-generate the discovery ISO.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have created a new infrastructure environment.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Refresh the API token:
					</p><pre class="programlisting language-terminal">$ source refresh-token</pre></li><li class="listitem"><p class="simpara">
						Modify the infrastructure environment:
					</p><pre class="programlisting language-terminal">$ curl https://api.openshift.com/api/assisted-install/v2/infra-envs/${INFRA_ENV_ID} \
-X PATCH \
-H "Authorization: Bearer ${API_TOKEN}" \
-H "Content-Type: application/json" \
-d "$(jq --null-input \
  --slurpfile pull_secret ~/Downloads/pull-secret.txt '
    {
      "image_type":"minimal-iso",
      "pull_secret": $pull_secret[0] | tojson
    }
')" | jq</pre></li></ol></div><section class="section" id="adding-kernel-arguments_installing-with-api"><div class="titlepage"><div><div><h3 class="title">4.8.1. Optional: Adding kernel arguments</h3></div></div></div><p>
					Providing kernel arguments to the Red Hat Enterprise Linux CoreOS (RHCOS) kernel via the Assisted Installer means passing specific parameters or options to the kernel at boot time, particularly when you cannot customize the kernel parameters of the discovery ISO. Kernel parameters can control various aspects of the kernel’s behavior and the operating system’s configuration, affecting hardware interaction, system performance, and functionality. Kernel arguments are used to customize or inform the node’s RHCOS kernel about the hardware configuration, debugging preferences, system services, and other low-level settings.
				</p><p>
					The RHCOS installer <code class="literal">kargs modify</code> command supports the <code class="literal">append</code>, <code class="literal">delete</code>, and <code class="literal">replace</code> options.
				</p><p>
					You can modify an infrastructure environment using the <a class="link mimir-link-warn" href="https://api.openshift.com/?urls.primaryName=assisted-service%20service#/installer/UpdateInfraEnv" title="Mimir does not include content from: api.openshift.com">/v2/infra-envs/{infra_env_id}</a> endpoint. When modifying the new infrastructure environment, the Assisted Installer will also re-generate the discovery ISO.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Refresh the API token:
						</p><pre class="programlisting language-terminal">$ source refresh-token</pre></li><li class="listitem"><p class="simpara">
							Modify the kernel arguments:
						</p><pre class="programlisting language-terminal">$ curl https://api.openshift.com/api/assisted-install/v2/infra-envs/${INFRA_ENV_ID} \
-X PATCH \
-H "Authorization: Bearer ${API_TOKEN}" \
-H "Content-Type: application/json" \
-d "$(jq --null-input \
  --slurpfile pull_secret ~/Downloads/pull-secret.txt '
    {
      "kernel_arguments": [{ "operation": "append", "value": "&lt;karg&gt;=&lt;value&gt;" }], <span id="CO10-1"><!--Empty--></span><span class="callout">1</span>
      "image_type":"minimal-iso",
      "pull_secret": $pull_secret[0] | tojson
    }
')" | jq</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO10-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Replace <code class="literal">&lt;karg&gt;</code> with the the kernel argument and <code class="literal">&lt;value&gt;</code> with the kernal argument value. For example: <code class="literal">rd.net.timeout.carrier=60</code>. You can specify multiple kernel arguments by adding a JSON object for each kernel argument.
								</div></dd></dl></div></li></ol></div></section></section><section class="section" id="adding-hosts_installing-with-api"><div class="titlepage"><div><div><h2 class="title">4.9. Adding hosts</h2></div></div></div><p>
				After configuring the cluster resource and infrastructure environment, download the discovery ISO image. You can choose from two images:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<span class="strong strong"><strong>Full ISO image:</strong></span> Use the full ISO image when booting must be self-contained. The image includes everything needed to boot and start the Assisted Installer agent. The ISO image is about 1GB in size. This is the recommended method for the <code class="literal">s390x</code> architecture when installing with RHEL KVM.
					</li><li class="listitem">
						<span class="strong strong"><strong>Minimal ISO image:</strong></span> Use the minimal ISO image when bandwidth over the virtual media connection is limited. This is the default setting. The image includes only what is required to boot a host with networking. The majority of the content is downloaded upon boot. The ISO image is about 100MB in size.
					</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Currently, ISO images are supported on IBM Z® (<code class="literal">s390x</code>) with KVM, iPXE with z/VM, and LPAR (both static and DPM). For details, see <a class="link" href="#booting-hosts-using-ipxe_booting-hosts-with-the-discovery-image" title="8.4. Booting hosts using iPXE">Booting hosts using iPXE</a>.
				</p></div></div><p>
				You can boot hosts with the discovery image using three methods. For details, see <a class="link" href="#assembly_booting-hosts-with-the-discovery-image" title="Chapter 8. Booting hosts with the discovery image">Booting hosts with the discovery image</a>.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have created a cluster.
					</li><li class="listitem">
						You have created an infrastructure environment.
					</li><li class="listitem">
						You have completed the configuration.
					</li><li class="listitem"><p class="simpara">
						If the cluster hosts are behind a firewall that requires the use of a proxy, you have configured the username, password, IP address and port for the HTTP and HTTPS URLs of the proxy server.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The proxy username and password must be URL-encoded.
						</p></div></div></li><li class="listitem">
						You have selected an image type or will use the default <code class="literal">minimal-iso</code>.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Configure the discovery image if needed. For details, see <a class="link" href="#assembly_configuring-the-discovery-image" title="Chapter 7. Configuring the discovery image">Configuring the discovery image</a>.
					</li><li class="listitem"><p class="simpara">
						Refresh the API token:
					</p><pre class="programlisting language-terminal">$ source refresh-token</pre></li><li class="listitem"><p class="simpara">
						Get the download URL:
					</p><pre class="programlisting language-terminal">$ curl -H "Authorization: Bearer ${API_TOKEN}" \
https://api.openshift.com/api/assisted-install/v2/infra-envs/${INFRA_ENV_ID}/downloads/image-url</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">{
  "expires_at": "2024-02-07T20:20:23.000Z",
  "url": "https://api.openshift.com/api/assisted-images/bytoken/&lt;TOKEN&gt;/&lt;OCP_VERSION&gt;/&lt;CPU_ARCHITECTURE&gt;/&lt;FULL_OR_MINIMAL_IMAGE&gt;.iso"
}</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Download the discovery image:
					</p><pre class="programlisting language-terminal">$ wget -O discovery.iso &lt;url&gt;</pre><p class="simpara">
						Replace <code class="literal">&lt;url&gt;</code> with the download URL from the previous step.
					</p></li><li class="listitem">
						Boot the host(s) with the discovery image.
					</li><li class="listitem">
						Assign a role to host(s).
					</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						<a class="link" href="#assembly_configuring-the-discovery-image" title="Chapter 7. Configuring the discovery image">Configuring the discovery image</a>
					</li><li class="listitem">
						<a class="link" href="#assembly_booting-hosts-with-the-discovery-image" title="Chapter 8. Booting hosts with the discovery image">Booting hosts with the discovery image</a>
					</li><li class="listitem">
						<a class="link" href="#adding-hosts-on-nutanix-with-the-api_installing-on-nutanix" title="13.2. Adding hosts on Nutanix with the API">Adding hosts on Nutanix with the API</a>
					</li><li class="listitem">
						<a class="link" href="#adding-hosts-on-vsphere_installing-on-vsphere" title="14.1. Adding hosts on vSphere">Adding hosts on vSphere</a>
					</li><li class="listitem">
						<a class="link" href="#assembly_role-assignment" title="Chapter 9. Assigning roles to hosts">Assigning roles to hosts</a>
					</li><li class="listitem">
						<a class="link" href="#booting-hosts-using-ipxe_booting-hosts-with-the-discovery-image" title="8.4. Booting hosts using iPXE">Booting hosts using iPXE</a>
					</li></ul></div></section><section class="section" id="modifying-hosts_installing-with-api"><div class="titlepage"><div><div><h2 class="title">4.10. Modifying hosts</h2></div></div></div><p>
				After adding hosts, modify the hosts as needed. The most common modifications are to the <code class="literal">host_name</code> and the <code class="literal">host_role</code> parameters.
			</p><p>
				You can modify a host by using the <a class="link mimir-link-warn" href="https://api.openshift.com/?urls.primaryName=assisted-service%20service#/installer/v2UpdateHost" title="Mimir does not include content from: api.openshift.com">/v2/infra-envs/{infra_env_id}/hosts/{host_id}</a> endpoint. See the <code class="literal">host-update-params</code> model in the <a class="link mimir-link-warn" href="https://api.openshift.com/?urls.primaryName=assisted-service%20service" title="Mimir does not include content from: api.openshift.com">API viewer</a> for details on the fields you can set when modifying a host.
			</p><p>
				A host might be one of two roles:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<code class="literal">master</code>: A host with the <code class="literal">master</code> role will operate as a control plane host.
					</li><li class="listitem">
						<code class="literal">worker</code>: A host with the <code class="literal">worker</code> role will operate as a worker host.
					</li></ul></div><p>
				By default, the Assisted Installer sets a host to <code class="literal">auto-assign</code>, which means the installation program determines whether the host is a <code class="literal">master</code> or <code class="literal">worker</code> role automatically. Use the following procedure to set the host’s role:
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have added hosts to the cluster.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Refresh the API token:
					</p><pre class="programlisting language-terminal">$ source refresh-token</pre></li><li class="listitem"><p class="simpara">
						Get the host IDs:
					</p><pre class="programlisting language-terminal">$ curl -s -X GET "https://api.openshift.com/api/assisted-install/v2/clusters/$CLUSTER_ID" \
--header "Content-Type: application/json" \
  -H "Authorization: Bearer $API_TOKEN" \
| jq '.host_networks[].host_ids'</pre></li><li class="listitem"><p class="simpara">
						Modify the host:
					</p><pre class="programlisting language-terminal">$ curl https://api.openshift.com/api/assisted-install/v2/infra-envs/${INFRA_ENV_ID}/hosts/&lt;host_id&gt; \ <span id="CO11-1"><!--Empty--></span><span class="callout">1</span>
-X PATCH \
-H "Authorization: Bearer ${API_TOKEN}" \
-H "Content-Type: application/json" \
-d '
    {
      "host_role":"worker"
      "host_name" : "worker-1"
    }
' | jq</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO11-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Replace <code class="literal">&lt;host_id&gt;</code> with the ID of the host.
							</div></dd></dl></div></li></ol></div><section class="section" id="modifying-storage-configuration_installing-with-api"><div class="titlepage"><div><div><h3 class="title">4.10.1. Modifying storage disk configuration</h3></div></div></div><p>
					Each host retrieved during host discovery can have multiple storage disks. You can optionally modify the default configurations for each disk.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Configure the cluster and discover the hosts. For details, see <span class="emphasis"><em>Additional resources</em></span>.
						</li></ul></div><h5 id="viewing_the_storage_disks">Viewing the storage disks</h5><p>
					You can view the hosts in your cluster, and the disks on each host. This enables you to perform actions on a specific disk.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Refresh the API token:
						</p><pre class="programlisting language-terminal">$ source refresh-token</pre></li><li class="listitem"><p class="simpara">
							Get the host IDs for the cluster:
						</p><pre class="programlisting language-terminal">$ curl -s "https://api.openshift.com/api/assisted-install/v2/clusters/$CLUSTER_ID" \
  -H "Authorization: Bearer $API_TOKEN" \
| jq '.host_networks[].host_ids'</pre><div class="formalpara"><p class="title"><strong>Sample output</strong></p><p>
								
<pre class="programlisting language-terminal">$ "1022623e-7689-8b2d-7fbd-e6f4d5bb28e5"</pre>

							</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								This is the ID of a single host. Multiple host IDs are separated by commas.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Get the disks for a specific host:
						</p><pre class="programlisting language-terminal">$ curl https://api.openshift.com/api/assisted-install/v2/infra-envs/${INFRA_ENV_ID}/hosts/&lt;host_id&gt; \ <span id="CO12-1"><!--Empty--></span><span class="callout">1</span>
-H "Authorization: Bearer ${API_TOKEN}" \
| jq '.inventory | fromjson | .disks'</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO12-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Replace <code class="literal">&lt;host_id&gt;</code> with the ID of the relevant host.
								</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Sample output</strong></p><p>
								
<pre class="programlisting language-terminal">$ [
  {
    "by_id": "/dev/disk/by-id/wwn-0x6c81f660f98afb002d3adc1a1460a506",
    "by_path": "/dev/disk/by-path/pci-0000:03:00.0-scsi-0:2:0:0",
    "drive_type": "HDD",
    "has_uuid": true,
    "hctl": "1:2:0:0",
    "id": "/dev/disk/by-id/wwn-0x6c81f660f98afb002d3adc1a1460a506",
    "installation_eligibility": {
      "eligible": true,
      "not_eligible_reasons": null
    },
    "model": "PERC_H710P",
    "name": "sda",
    "path": "/dev/sda",
    "serial": "0006a560141adc3a2d00fb8af960f681",
    "size_bytes": 6595056500736,
    "vendor": "DELL",
    "wwn": "0x6c81f660f98afb002d3adc1a1460a506"
  }
]</pre>

							</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								This is the output for one disk. It contains the <code class="literal">disk_id</code> and <code class="literal">installation_eligibility</code> properties for the disk.
							</p></div></div></li></ol></div><h5 id="changing_the_installation_disk_2">Changing the installation disk</h5><p>
					The Assisted Installer randomly assigns an installation disk by default. If there are multiple storage disks for a host, you can select a different disk to be the installation disk. This automatically unassigns the previous disk.
				</p><p>
					You can select any disk whose <code class="literal">installation_eligibility</code> property is <code class="literal">eligible: true</code> to be the installation disk.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Get the host and storage disk IDs. For details, see <span class="emphasis"><em>Viewing the storage disks</em></span>.
						</li><li class="listitem"><p class="simpara">
							Optional: Identify the current installation disk:
						</p><pre class="programlisting language-terminal">$ curl https://api.openshift.com/api/assisted-install/v2/infra-envs/${INFRA_ENV_ID}/hosts/&lt;host_id&gt; \ <span id="CO13-1"><!--Empty--></span><span class="callout">1</span>
-H "Authorization: Bearer ${API_TOKEN}" \
| jq '.installation_disk_id'</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO13-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Replace <code class="literal">&lt;host_id&gt;</code> with the ID of the relevant host.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Assign a new installation disk:
						</p><pre class="programlisting language-terminal">$ curl https://api.openshift.com/api/assisted-install/v2/infra-envs/${INFRA_ENV_ID}/hosts/&lt;host_id&gt; \ <span id="CO14-1"><!--Empty--></span><span class="callout">1</span>
-X PATCH \
-H "Content-Type: application/json" \
-H "Authorization: Bearer ${API_TOKEN}" \

{
  "disks_selected_config": [
    {
      "id": "&lt;disk_id&gt;", <span id="CO14-2"><!--Empty--></span><span class="callout">2</span>
      "role": "install"
    }
  ]
}</pre><div class="admonition note"><div class="admonition_header">Note</div><div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO14-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Replace <code class="literal">&lt;host_id&gt;</code> with the ID of the host.
									</div></dd><dt><a href="#CO14-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Replace <code class="literal">&lt;disk_id&gt;</code> with the ID of the new installation disk.
									</div></dd></dl></div></div></div></li></ol></div><h5 id="disabling_disk_formatting_2">Disabling disk formatting</h5><p>
					The Assisted Installer marks all bootable disks for formatting during the installation process by default, regardless of whether or not they have been defined as the installation disk. Formatting causes data loss.
				</p><p>
					You can choose to disable the formatting of a specific disk. This should be performed with caution, as bootable disks may interfere with the installation process, mainly in terms of boot order.
				</p><p>
					You cannot disable formatting for the installation disk.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Get the host and storage disk IDs. For details, see <span class="emphasis"><em>Viewing the storage disks</em></span>.
						</li><li class="listitem"><p class="simpara">
							Run the following command:
						</p><pre class="programlisting language-terminal">$  curl https://api.openshift.com/api/assisted-install/v2/infra-envs/${INFRA_ENV_ID}/hosts/&lt;host_id&gt; \ <span id="CO15-1"><!--Empty--></span><span class="callout">1</span>
-X PATCH \
-H "Content-Type: application/json" \
-H "Authorization: Bearer ${API_TOKEN}" \

{
 "disks_skip_formatting": [
   {
     "disk_id": "&lt;disk_id&gt;", <span id="CO15-2"><!--Empty--></span><span class="callout">2</span>
     "skip_formatting": true <span id="CO15-3"><!--Empty--></span><span class="callout">3</span>
   }
 ]
}</pre><div class="admonition note"><div class="admonition_header">Note</div><div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO15-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Replace <code class="literal">&lt;host_id&gt;</code> with the ID of the host.
									</div></dd><dt><a href="#CO15-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Replace <code class="literal">&lt;disk_id&gt;</code> with the ID of the disk. If there is more than one disk, separate the IDs with a comma.
									</div></dd><dt><a href="#CO15-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										To re-enable formatting, change the value to <code class="literal">false</code>.
									</div></dd></dl></div></div></div></li></ol></div></section></section><section class="section" id="adding-custom-manifests_installing-with-api"><div class="titlepage"><div><div><h2 class="title">4.11. Adding custom manifests</h2></div></div></div><p>
				A custom manifest is a JSON or YAML file that contains advanced configurations not currently supported in the Assisted Installer user interface. You can create a custom manifest or use one provided by a third party. To create a custom manifest with the API, use the <a class="link mimir-link-warn" href="https://api.openshift.com/?urls.primaryName=assisted-service%20service#/manifests/V2CreateClusterManifest" title="Mimir does not include content from: api.openshift.com">/v2/clusters/$CLUSTER_ID/manifests</a> endpoint.
			</p><p>
				You can upload a base64-encoded custom manifest to either the <code class="literal">openshift</code> folder or the <code class="literal">manifests</code> folder with the Assisted Installer API. There is no limit to the number of custom manifests permitted.
			</p><p>
				Only one base64-encoded JSON manifest can be uploaded at a time. However, each uploaded base64-encoded YAML file can contain multiple custom manifests. Uploading a multi-document YAML manifest is faster than adding the YAML files individually.
			</p><p>
				For a file containing a single custom manifest, accepted file extensions include <code class="literal">.yaml</code>, <code class="literal">.yml</code>, or <code class="literal">.json</code>.
			</p><div class="formalpara"><p class="title"><strong>Single custom manifest example</strong></p><p>
					
<pre class="programlisting language-json">{
    "apiVersion": "machineconfiguration.openshift.io/v1",
    "kind": "MachineConfig",
    "metadata": {
        "labels": {
            "machineconfiguration.openshift.io/role": "primary"
        },
        "name": "10_primary_storage_config"
    },
    "spec": {
        "config": {
            "ignition": {
                "version": "3.2.0"
            },
            "storage": {
                "disks": [
                    {
                        "device": "&lt;/dev/xxyN&gt;",
                        "partitions": [
                            {
                                "label": "recovery",
                                "startMiB": 32768,
                                "sizeMiB": 16384
                            }
                        ]
                    }
                ],
                "filesystems": [
                    {
                        "device": "/dev/disk/by-partlabel/recovery",
                        "label": "recovery",
                        "format": "xfs"
                    }
                ]
            }
        }
    }
}</pre>

				</p></div><p>
				For a file containing multiple custom manifests, accepted file types include <code class="literal">.yaml</code> or <code class="literal">.yml</code>.
			</p><div class="formalpara"><p class="title"><strong>Multiple custom manifest example</strong></p><p>
					
<pre class="programlisting language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: master
  name: 99-openshift-machineconfig-master-kargs
spec:
  kernelArguments:
    - loglevel=7
---
apiVersion: machineconfiguration.openshift.io/v2
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
  name: 98-openshift-machineconfig-worker-kargs
spec:
  kernelArguments:
    - loglevel=5</pre>

				</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							When you install OpenShift Container Platform on the Oracle Cloud Infrastructure (OCI) external platform, you must add the custom manifests provided by Oracle. For additional external partner integrations such as vSphere or Nutanix, this step is optional.
						</li><li class="listitem">
							For more information about custom manifests, see <span class="emphasis"><em>Additional Resources</em></span>.
						</li></ul></div></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have generated a valid <code class="literal">API_TOKEN</code>. Tokens expire every 15 minutes.
					</li><li class="listitem">
						You have registered a new cluster definition and exported the <code class="literal">cluster_id</code> to the <code class="literal">$CLUSTER_ID</code> BASH variable.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Create a custom manifest file.
					</li><li class="listitem">
						Save the custom manifest file using the appropriate extension for the file format.
					</li><li class="listitem"><p class="simpara">
						Refresh the API token:
					</p><pre class="programlisting language-terminal">$ source refresh-token</pre></li><li class="listitem"><p class="simpara">
						Add the custom manifest to the cluster by executing the following command:
					</p><pre class="programlisting language-terminal">$ curl -X POST "https://api.openshift.com/api/assisted-install/v2/clusters/$CLUSTER_ID/manifests" \
    -H "Authorization: Bearer $API_TOKEN" \
    -H "Content-Type: application/json" \
    -d '{
            "file_name":"manifest.json",
            "folder":"manifests",
            "content":"'"$(base64 -w 0 ~/manifest.json)"'"
    }' | jq</pre><p class="simpara">
						Replace <code class="literal">manifest.json</code> with the name of your manifest file. The second instance of <code class="literal">manifest.json</code> is the path to the file. Ensure the path is correct.
					</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-json">{
  "file_name": "manifest.json",
  "folder": "manifests"
}</pre>

						</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The <code class="literal">base64 -w 0</code> command base64-encodes the manifest as a string and omits carriage returns. Encoding with carriage returns will generate an exception.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Verify that the Assisted Installer added the manifest:
					</p><pre class="programlisting language-terminal">curl -X GET "https://api.openshift.com/api/assisted-install/v2/clusters/$CLUSTER_ID/manifests/files?folder=manifests&amp;file_name=manifest.json" -H "Authorization: Bearer $API_TOKEN"</pre><p class="simpara">
						Replace <code class="literal">manifest.json</code> with the name of your manifest file.
					</p></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						<a class="link mimir-link-warn-approx" href="/products/#letter-O" title="Mimir does not include this page, but the link has been rewritten to point to the nearest parent document.">Manifest configuration files</a>
					</li><li class="listitem">
						<a class="link mimir-link-warn" href="https://www.yaml.info/learn/document.html" title="Mimir does not include content from: www.yaml.info">Multi-document YAML files</a>
					</li></ul></div></section><section class="section" id="preinstallation-validations_api"><div class="titlepage"><div><div><h2 class="title">4.12. Preinstallation validations</h2></div></div></div><p>
				The Assisted Installer ensures the cluster meets the prerequisites before installation, because it eliminates complex postinstallation troubleshooting, thereby saving significant amounts of time and effort. Before installing the cluster, ensure the cluster and each host pass preinstallation validation.
			</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						<a class="link" href="#assembly_preinstallation-validations" title="Chapter 10. Preinstallation validations">Preinstallation validations</a>
					</li></ul></div></section><section class="section" id="installing-the-cluster_installing-with-api"><div class="titlepage"><div><div><h2 class="title">4.13. Installing the cluster</h2></div></div></div><p>
				Once the cluster hosts past validation, you can install the cluster.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have created a cluster and infrastructure environment.
					</li><li class="listitem">
						You have added hosts to the infrastructure environment.
					</li><li class="listitem">
						The hosts have passed validation.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Refresh the API token:
					</p><pre class="programlisting language-terminal">$ source refresh-token</pre></li><li class="listitem"><p class="simpara">
						Install the cluster:
					</p><pre class="programlisting language-terminal">$ curl -H "Authorization: Bearer $API_TOKEN" \
-X POST \
https://api.openshift.com/api/assisted-install/v2/clusters/$CLUSTER_ID/actions/install | jq</pre></li><li class="listitem">
						Complete any postinstallation platform integration steps.
					</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						<a class="link" href="#nutanix-post-installation-configuration_installing-on-nutanix" title="13.3. Nutanix postinstallation configuration">Nutanix postinstallation configuration</a>
					</li><li class="listitem">
						<a class="link" href="#vsphere-post-installation-configuration_installing-on-vsphere" title="14.2. vSphere postinstallation configuration using the CLI">vSphere postinstallation configuration</a>
					</li></ul></div></section></section><section class="chapter" id="assembly_enabling-disk-encryption"><div class="titlepage"><div><div><h1 class="title">Chapter 5. Optional: Enabling disk encryption</h1></div></div></div><p>
			You can enable encryption of installation disks using either the TPM v2 or Tang encryption modes.
		</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
				In some situations, when you enable TPM disk encryption in the firmware for a bare-metal host and then boot it from an ISO that you generate with the Assisted Installer, the cluster deployment can get stuck. This can happen if there are left-over TPM encryption keys from a previous installation on the host. For more information, see <a class="link mimir-link-warn" href="https://bugzilla.redhat.com/show_bug.cgi?id=2011634" title="Mimir does not include content from: bugzilla.redhat.com">BZ#2011634</a>. If you experience this problem, contact Red Hat support.
			</p></div></div><section class="section" id="proc_enabling-tpm-v2-encryption_enabling-disk-encryption"><div class="titlepage"><div><div><h2 class="title">5.1. Enabling TPM v2 encryption</h2></div></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Check to see if TPM v2 encryption is enabled in the BIOS on each host. Most Dell systems require this. Check the manual for your computer. The Assisted Installer will also validate that TPM is enabled in the firmware. See the <code class="literal">disk-encruption</code> model in the <a class="link mimir-link-warn" href="https://api.openshift.com/?urls.primaryName=assisted-service%20service" title="Mimir does not include content from: api.openshift.com">Assisted Installer API</a> for additional details.
					</li></ul></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					Verify that a TPM v2 encryption chip is installed on each node and enabled in the firmware.
				</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Optional: Using the web console, in the <span class="strong strong"><strong>Cluster details</strong></span> step of the user interface wizard, choose to enable TPM v2 encryption on either the control plane nodes, workers, or both.
					</li><li class="listitem"><p class="simpara">
						Optional: Using the API, follow the "Modifying hosts" procedure. Set the <code class="literal">disk_encryption.enable_on</code> setting to <code class="literal">all</code>, <code class="literal">masters</code>, or <code class="literal">workers</code>. Set the <code class="literal">disk_encryption.mode</code> setting to <code class="literal">tpmv2</code>.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Refresh the API token:
							</p><pre class="programlisting language-terminal">$ source refresh-token</pre></li><li class="listitem"><p class="simpara">
								Enable TPM v2 encryption:
							</p><pre class="programlisting language-terminal white-space-pre white-space-pre">$ curl https://api.openshift.com/api/assisted-install/v2/clusters/${CLUSTER_ID} \
-X PATCH \
-H "Authorization: Bearer ${API_TOKEN}" \
-H "Content-Type: application/json" \
-d '
{
  "disk_encryption": {
    "enable_on": "none",
    "mode": "tpmv2"
  }
}
' | jq</pre><p class="simpara">
								Valid settings for <code class="literal">enable_on</code> are <code class="literal">all</code>, <code class="literal">master</code>, <code class="literal">worker</code>, or <code class="literal">none</code>.
							</p></li></ol></div></li></ol></div></section><section class="section" id="proc_enabling-tang-encryption_enabling-disk-encryption"><div class="titlepage"><div><div><h2 class="title">5.2. Enabling Tang encryption</h2></div></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have access to a Red Hat Enterprise Linux (RHEL) 8 machine that can be used to generate a thumbprint of the Tang exchange key.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Set up a Tang server or access an existing one. See <a class="link mimir-link-warn" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/security_hardening/configuring-automated-unlocking-of-encrypted-volumes-using-policy-based-decryption_security-hardening#network-bound-disk-encryption_configuring-automated-unlocking-of-encrypted-volumes-using-policy-based-decryption" title="This content is not included in Mimir.">Network-bound disk encryption</a> for instructions. You can set multiple Tang servers, but the Assisted Installer must be able to connect to all of them during installation.
					</li><li class="listitem"><p class="simpara">
						On the Tang server, retrieve the thumbprint for the Tang server using <code class="literal">tang-show-keys</code>:
					</p><pre class="programlisting language-terminal white-space-pre white-space-pre">$ tang-show-keys &lt;port&gt;</pre><p class="simpara">
						Optional: Replace <code class="literal">&lt;port&gt;</code> with the port number. The default port number is <code class="literal">80</code>.
					</p><div class="white-space-pre white-space-pre"><p class="title"><strong>Example thumbprint</strong></p><p>
							
<pre class="programlisting language-terminal">1gYTN_LpU9ZMB35yn5IbADY5OQ0</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Optional: Retrieve the thumbprint for the Tang server using <code class="literal">jose</code>.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Ensure <code class="literal">jose</code> is installed on the Tang server:
							</p><pre class="programlisting language-terminal white-space-pre white-space-pre">$ sudo dnf install jose</pre></li><li class="listitem"><p class="simpara">
								On the Tang server, retrieve the thumbprint using <code class="literal">jose</code>:
							</p><pre class="programlisting language-terminal white-space-pre white-space-pre">$ sudo jose jwk thp -i /var/db/tang/&lt;public_key&gt;.jwk</pre><p class="simpara">
								Replace <code class="literal">&lt;public_key&gt;</code> with the public exchange key for the Tang server.
							</p><div class="white-space-pre white-space-pre"><p class="title"><strong>Example thumbprint</strong></p><p>
									
<pre class="programlisting language-terminal">1gYTN_LpU9ZMB35yn5IbADY5OQ0</pre>

								</p></div></li></ol></div></li><li class="listitem">
						Optional: In the <span class="strong strong"><strong>Cluster details</strong></span> step of the user interface wizard, choose to enable Tang encryption on either the control plane nodes, workers, or both. You will be required to enter URLs and thumbprints for the Tang servers.
					</li><li class="listitem"><p class="simpara">
						Optional: Using the API, follow the "Modifying hosts" procedure.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Refresh the API token:
							</p><pre class="programlisting language-terminal">$ source refresh-token</pre></li><li class="listitem"><p class="simpara">
								Set the <code class="literal">disk_encryption.enable_on</code> setting to <code class="literal">all</code>, <code class="literal">masters</code>, or <code class="literal">workers</code>. Set the <code class="literal">disk_encryption.mode</code> setting to <code class="literal">tang</code>. Set <code class="literal">disk_encyrption.tang_servers</code> to provide the URL and thumbprint details about one or more Tang servers:
							</p><pre class="programlisting language-terminal white-space-pre white-space-pre">$ curl https://api.openshift.com/api/assisted-install/v2/clusters/${CLUSTER_ID} \
-X PATCH \
-H "Authorization: Bearer ${API_TOKEN}" \
-H "Content-Type: application/json" \
-d '
{
  "disk_encryption": {
    "enable_on": "all",
    "mode": "tang",
    "tang_servers": "[{\"url\":\"http://tang.example.com:7500\",\"thumbprint\":\"PLjNyRdGw03zlRoGjQYMahSZGu9\"},{\"url\":\"http://tang2.example.com:7500\",\"thumbprint\":\"XYjNyRdGw03zlRoGjQYMahSZGu3\"}]"
  }
}
' | jq</pre><p class="simpara">
								Valid settings for <code class="literal">enable_on</code> are <code class="literal">all</code>, <code class="literal">master</code>, <code class="literal">worker</code>, or <code class="literal">none</code>. Within the <code class="literal">tang_servers</code> value, comment out the quotes within the object(s).
							</p></li></ol></div></li></ol></div></section><section class="section _additional-resources" id="additional_resources"><div class="titlepage"><div><div><h2 class="title">5.3. Additional resources</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link" href="#modifying-hosts_installing-with-api" title="4.10. Modifying hosts">Modifying hosts</a>
					</li></ul></div></section></section><section class="chapter" id="assembly_configuring-schedulable-control-planes"><div class="titlepage"><div><div><h1 class="title">Chapter 6. Optional: Configuring schedulable control plane nodes</h1></div></div></div><p>
			In a high availability deployment, three or more nodes comprise the control plane. The control plane nodes are used for managing OpenShift Container Platform and for running the OpenShift containers. The remaining nodes are workers, used to run the customer containers and workloads. There can be anywhere between one to thousands of worker nodes.
		</p><p>
			For a single-node OpenShift cluster or for a cluster that comprises up to four nodes, the system automatically schedules the workloads to run on the control plane nodes.
		</p><p>
			For clusters of between five to ten nodes, you can choose to schedule workloads to run on the control plane nodes in addition to the worker nodes. This option is recommended for enhancing efficiency and preventing underutilized resources. You can select this option either during the installation setup, or as part of the post-installation steps.
		</p><p>
			For larger clusters of more than ten nodes, this option is not recommended.
		</p><p>
			This section explains how to schedule workloads to run on control plane nodes using the Assisted Installer web console and API, as part of the installation setup.
		</p><p>
			For instructions on how to configure schedulable control plane nodes following an installation, see <a class="link mimir-link-warn-approx" href="/products/#letter-O" title="Mimir does not include this page, but the link has been rewritten to point to the nearest parent document.">Configuring control plane nodes as schedulable</a> in the OpenShift Container Platform documentation.
		</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
				When you configure control plane nodes from the default unschedulable to schedulable, additional subscriptions are required. This is because control plane nodes then become worker nodes.
			</p></div></div><section class="section" id="configuring-schedulable-control-planes-ui_role-assignment"><div class="titlepage"><div><div><h2 class="title">6.1. Configuring schedulable control planes using the web console</h2></div></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have set the cluster details.
					</li><li class="listitem">
						You are installing OpenShift Container Platform 4.14 or later.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Log in to the <a class="link mimir-link-warn" href="https://console.redhat.com" title="Mimir does not include content from: console.redhat.com">Red Hat Hybrid Cloud Console</a> and follow the instructions for installing OpenShift Container Platform using the Assisted Installer web console. For details, see <span class="emphasis"><em>Installing with the Assisted Installer web console</em></span> in <span class="emphasis"><em>Additional Resources</em></span>.
					</li><li class="listitem">
						When you reach the <span class="strong strong"><strong>Host discovery</strong></span> page, click <span class="strong strong"><strong>Add hosts</strong></span>.
					</li><li class="listitem">
						Optionally change the <span class="strong strong"><strong>Provisioning type</strong></span> and additional settings as required. All options are compatible with schedulable control planes.
					</li><li class="listitem">
						Click <span class="strong strong"><strong>Generate Discovery ISO</strong></span> to download the ISO.
					</li><li class="listitem"><p class="simpara">
						Set <span class="strong strong"><strong>Run workloads on control plane nodes</strong></span> to on.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							For four nodes or less, this switch is activated automatically and cannot be changed.
						</p></div></div></li><li class="listitem">
						Click <span class="strong strong"><strong>Next</strong></span>.
					</li></ol></div></section><section class="section" id="configuring-schedulable-control-planes-api_role-assignment"><div class="titlepage"><div><div><h2 class="title">6.2. Configuring schedulable control planes using the API</h2></div></div></div><p>
				Use the <code class="literal">schedulable_masters</code> attribute to enable workloads to run on control plane nodes.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have generated a valid <code class="literal">API_TOKEN</code>. Tokens expire every 15 minutes.
					</li><li class="listitem">
						You have created a <code class="literal">$PULL_SECRET</code> variable.
					</li><li class="listitem">
						You are installing OpenShift Container Platform 4.14 or later.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Follow the instructions for installing Assisted Installer using the Assisted Installer API. For details, see <span class="emphasis"><em>Installing with the Assisted Installer API</em></span> in <span class="emphasis"><em>Additional Resources</em></span>.
					</li><li class="listitem"><p class="simpara">
						When you reach the step for registering a new cluster, set the <code class="literal">schedulable_masters</code> attribute as follows:
					</p><pre class="programlisting language-terminal">$ curl https://api.openshift.com/api/assisted-install/v2/clusters/${CLUSTER_ID} \
-X PATCH \
-H "Authorization: Bearer ${API_TOKEN}" \
-H "Content-Type: application/json" \
-d '
{
  "schedulable_masters": true <span id="CO16-1"><!--Empty--></span><span class="callout">1</span>
}
' | jq</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO16-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Enables the scheduling of workloads on the control plane nodes.
							</div></dd></dl></div></li></ol></div></section><section class="section _additional-resources" id="additional_resources_2"><div class="titlepage"><div><div><h2 class="title">6.3. Additional resources</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link" href="#installing-with-ui" title="Chapter 3. Installing with the Assisted Installer web console">Installing with the Assisted Installer web console</a>
					</li><li class="listitem">
						<a class="link" href="#installing-with-api" title="Chapter 4. Installing with the Assisted Installer API">Installing with the Assisted Installer API</a>
					</li></ul></div></section></section><section class="chapter" id="assembly_configuring-the-discovery-image"><div class="titlepage"><div><div><h1 class="title">Chapter 7. Configuring the discovery image</h1></div></div></div><p>
			The Assisted Installer uses an initial image to run an agent that performs hardware and network validations before attempting to install OpenShift Container Platform. You can use <a class="link mimir-link-warn" href="https://coreos.github.io/ignition/" title="Mimir does not include content from: coreos.github.io">Ignition</a> to customize the discovery image.
		</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
				Modifications to the discovery image will not persist in the system.
			</p></div></div><section class="section" id="creating-an-ignition-configuration-file_configuring-the-discovery-image"><div class="titlepage"><div><div><h2 class="title">7.1. Creating an Ignition configuration file</h2></div></div></div><p>
				Ignition is a low-level system configuration utility, which is part of the temporary initial root filesystem, the <span class="emphasis"><em>initramfs</em></span>. When Ignition runs on the first boot, it finds configuration data in the Ignition configuration file and applies it to the host before <code class="literal">switch_root</code> is called to pivot to the host’s root filesystem.
			</p><p>
				Ignition uses a JSON <a class="link mimir-link-warn" href="https://coreos.github.io/ignition/configuration-v3_2/" title="Mimir does not include content from: coreos.github.io">configuration specification</a> file to represent the set of changes that occur on the first boot.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					Ignition versions newer than 3.2 are not supported, and will raise an error.
				</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create an Ignition file and specify the configuration specification version:
					</p><pre class="programlisting language-terminal">$ vim ~/ignition.conf</pre><pre class="programlisting language-javascript">{
  "ignition": { "version": "3.1.0" }
}</pre></li><li class="listitem"><p class="simpara">
						Add configuration data to the Ignition file. For example, add a password to the <code class="literal">core</code> user.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Generate a password hash:
							</p><pre class="programlisting language-terminal">$ openssl passwd -6</pre></li><li class="listitem"><p class="simpara">
								Add the generated password hash to the <code class="literal">core</code> user:
							</p><pre class="programlisting language-javascript">{
  "ignition": { "version": "3.1.0" },
  "passwd": {
    "users": [
      {
        "name": "core",
        "passwordHash": "$6$spam$M5LGSMGyVD.9XOboxcwrsnwNdF4irpJdAWy.1Ry55syyUiUssIzIAHaOrUHr2zg6ruD8YNBPW9kW0H8EnKXyc1"
      }
    ]
  }
}</pre></li></ol></div></li><li class="listitem"><p class="simpara">
						Save the Ignition file and export it to the <code class="literal">IGNITION_FILE</code> variable:
					</p><pre class="programlisting language-terminal">$ export IGNITION_FILE=~/ignition.conf</pre></li></ol></div></section><section class="section" id="modifying-the-discovery-image-with-ignition_configuring-the-discovery-image"><div class="titlepage"><div><div><h2 class="title">7.2. Modifying the discovery image with Ignition</h2></div></div></div><p>
				Once you create an Ignition configuration file, you can modify the discovery image by patching the infrastructure environment using the Assisted Installer API.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						If you used the web console to create the cluster, you have set up the API authentication.
					</li><li class="listitem">
						You have an infrastructure environment and you have exported the infrastructure environment <code class="literal">id</code> to the <code class="literal">INFRA_ENV_ID</code> variable.
					</li><li class="listitem">
						You have a valid Ignition file and have exported the file name as <code class="literal">$IGNITION_FILE</code>.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create an <code class="literal">ignition_config_override</code> JSON object and redirect it to a file:
					</p><pre class="programlisting language-terminal">$ jq -n \
  --arg IGNITION "$(jq -c . $IGNITION_FILE)" \
  '{ignition_config_override: $IGNITION}' \
  &gt; discovery_ignition.json</pre></li><li class="listitem"><p class="simpara">
						Refresh the API token:
					</p><pre class="programlisting language-terminal">$ source refresh-token</pre></li><li class="listitem"><p class="simpara">
						Patch the infrastructure environment:
					</p><pre class="programlisting language-terminal">$ curl \
  --header "Authorization: Bearer $API_TOKEN" \
  --header "Content-Type: application/json" \
  -XPATCH \
  -d @discovery_ignition.json \
  https://api.openshift.com/api/assisted-install/v2/infra-envs/$INFRA_ENV_ID | jq</pre><p class="simpara">
						The <code class="literal">ignition_config_override</code> object references the Ignition file.
					</p></li><li class="listitem">
						Download the updated discovery image.
					</li></ol></div></section></section><section class="chapter" id="assembly_booting-hosts-with-the-discovery-image"><div class="titlepage"><div><div><h1 class="title">Chapter 8. Booting hosts with the discovery image</h1></div></div></div><p>
			The Assisted Installer uses an initial image to run an agent that performs hardware and network validations before attempting to install OpenShift Container Platform. You can boot hosts with the discovery image using three methods:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					USB drive
				</li><li class="listitem">
					Redfish virtual media
				</li><li class="listitem">
					iPXE
				</li></ul></div><section class="section" id="creating-an-iso-image-on-a-usb-drive_booting-hosts-with-the-discovery-image"><div class="titlepage"><div><div><h2 class="title">8.1. Creating an ISO image on a USB drive</h2></div></div></div><p>
				You can install the Assisted Installer agent using a USB drive that contains the discovery ISO image. Starting the host with the USB drive prepares the host for the software installation.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						On the administration host, insert a USB drive into a USB port.
					</li><li class="listitem"><p class="simpara">
						Copy the ISO image to the USB drive, for example:
					</p><pre class="programlisting language-terminal"># dd if=&lt;path_to_iso&gt; of=&lt;path_to_usb&gt; status=progress</pre><p class="simpara">
						where:
					</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">&lt;path_to_iso&gt;</span></dt><dd>
									is the relative path to the downloaded discovery ISO file, for example, <code class="literal">discovery.iso</code>.
								</dd><dt><span class="term">&lt;path_to_usb&gt;</span></dt><dd><p class="simpara">
									is the location of the connected USB drive, for example, <code class="literal">/dev/sdb</code>.
								</p><p class="simpara">
									After the ISO is copied to the USB drive, you can use the USB drive to install the Assisted Installer agent on the cluster host.
								</p></dd></dl></div></li></ol></div></section><section class="section" id="booting-with-a-usb-drive_booting-hosts-with-the-discovery-image"><div class="titlepage"><div><div><h2 class="title">8.2. Booting with a USB drive</h2></div></div></div><p>
				To register nodes with the Assisted Installer using a bootable USB drive, use the following procedure.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Insert the RHCOS discovery ISO USB drive into the target host.
					</li><li class="listitem">
						Configure the boot drive order in the server firmware settings to boot from the attached discovery ISO, and then reboot the server.
					</li><li class="listitem"><p class="simpara">
						Wait for the host to boot up.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
								For web console installations, on the administration host, return to the browser. Wait for the host to appear in the list of discovered hosts.
							</li><li class="listitem"><p class="simpara">
								For API installations, refresh the token, check the enabled host count, and gather the host IDs:
							</p><pre class="programlisting language-terminal white-space-pre white-space-pre">$ source refresh-token</pre><pre class="programlisting language-terminal white-space-pre white-space-pre">$ curl -s -X GET "https://api.openshift.com/api/assisted-install/v2/clusters/$CLUSTER_ID" \
--header "Content-Type: application/json" \
  -H "Authorization: Bearer $API_TOKEN" \
| jq '.enabled_host_count'</pre><pre class="programlisting language-terminal white-space-pre white-space-pre">$ curl -s -X GET "https://api.openshift.com/api/assisted-install/v2/clusters/$CLUSTER_ID" \
--header "Content-Type: application/json" \
  -H "Authorization: Bearer $API_TOKEN" \
| jq '.host_networks[].host_ids'</pre><div class="white-space-pre white-space-pre"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">[
  "1062663e-7989-8b2d-7fbb-e6f4d5bb28e5"
]</pre>

								</p></div></li></ol></div></li></ol></div></section><section class="section" id="install-booting-from-an-iso-over-http-redfish_booting-hosts-with-the-discovery-image"><div class="titlepage"><div><div><h2 class="title">8.3. Booting from an HTTP-hosted ISO image using the Redfish API</h2></div></div></div><p>
				You can provision hosts in your network using ISOs that you install using the Redfish Baseboard Management Controller (BMC) API.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Download the installation Red Hat Enterprise Linux CoreOS (RHCOS) ISO.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Copy the ISO file to an HTTP server accessible in your network.
					</li><li class="listitem"><p class="simpara">
						Boot the host from the hosted ISO file, for example:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Call the redfish API to set the hosted ISO as the <code class="literal">VirtualMedia</code> boot media by running the following command:
							</p><pre class="programlisting language-terminal">$ curl -k -u &lt;bmc_username&gt;:&lt;bmc_password&gt; \
-d '{"Image":"&lt;hosted_iso_file&gt;", "Inserted": true}' \
-H "Content-Type: application/json" \
-X POST &lt;host_bmc_address&gt;/redfish/v1/Managers/iDRAC.Embedded.1/VirtualMedia/CD/Actions/VirtualMedia.InsertMedia</pre><p class="simpara">
								Where:
							</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">&lt;bmc_username&gt;:&lt;bmc_password&gt;</span></dt><dd>
											Is the username and password for the target host BMC.
										</dd><dt><span class="term">&lt;hosted_iso_file&gt;</span></dt><dd>
											Is the URL for the hosted installation ISO, for example: <code class="literal"><a class="link mimir-link-warn" href="https://example.com/rhcos-live-minimal.iso" title="Mimir does not include content from: example.com">https://example.com/rhcos-live-minimal.iso</a></code>. The ISO must be accessible from the target host machine.
										</dd><dt><span class="term">&lt;host_bmc_address&gt;</span></dt><dd>
											Is the BMC IP address of the target host machine.
										</dd></dl></div></li><li class="listitem"><p class="simpara">
								Set the host to boot from the <code class="literal">VirtualMedia</code> device by running the following command:
							</p><pre class="programlisting language-terminal">$ curl -k -u &lt;bmc_username&gt;:&lt;bmc_password&gt; \
-X PATCH -H 'Content-Type: application/json' \
-d '{"Boot": {"BootSourceOverrideTarget": "Cd", "BootSourceOverrideMode": "UEFI", "BootSourceOverrideEnabled": "Once"}}' \
&lt;host_bmc_address&gt;/redfish/v1/Systems/System.Embedded.1</pre></li><li class="listitem"><p class="simpara">
								Reboot the host:
							</p><pre class="programlisting language-terminal">$ curl -k -u &lt;bmc_username&gt;:&lt;bmc_password&gt; \
-d '{"ResetType": "ForceRestart"}' \
-H 'Content-type: application/json' \
-X POST &lt;host_bmc_address&gt;/redfish/v1/Systems/System.Embedded.1/Actions/ComputerSystem.Reset</pre></li><li class="listitem"><p class="simpara">
								Optional: If the host is powered off, you can boot it using the <code class="literal">{"ResetType": "On"}</code> switch. Run the following command:
							</p><pre class="programlisting language-terminal">$ curl -k -u &lt;bmc_username&gt;:&lt;bmc_password&gt; \
-d '{"ResetType": "On"}' -H 'Content-type: application/json' \
-X POST &lt;host_bmc_address&gt;/redfish/v1/Systems/System.Embedded.1/Actions/ComputerSystem.Reset</pre></li></ol></div></li></ol></div></section><section class="section" id="booting-hosts-using-ipxe_booting-hosts-with-the-discovery-image"><div class="titlepage"><div><div><h2 class="title">8.4. Booting hosts using iPXE</h2></div></div></div><p>
				The Assisted Installer provides an iPXE script including all the artifacts needed to boot the discovery image for an infrastructure environment. Due to the limitations of the current HTTPS implementation of iPXE, the recommendation is to download and expose the needed artifacts in an HTTP server. Currently, even if iPXE supports HTTPS protocol, the supported algorithms are old and not recommended.
			</p><p>
				The full list of supported ciphers is in <a class="link mimir-link-warn" href="https://ipxe.org/crypto" title="Mimir does not include content from: ipxe.org">https://ipxe.org/crypto</a>.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have created an infrastructure environment by using the API or you have created a cluster by using the web console.
					</li><li class="listitem">
						You have your infrastructure environment ID exported in your shell as <code class="literal">$INFRA_ENV_ID</code>.
					</li><li class="listitem">
						You have credentials to use when accessing the API and have exported a token as <code class="literal">$API_TOKEN</code> in your shell.
					</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					If you configure iPXE by using the web console, the <code class="literal">$INFRA_ENV_ID</code> and <code class="literal">$API_TOKEN</code> variables are preset.
				</p></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						You have an HTTP server to host the images.
					</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					IBM Power® only supports PXE, which has the following requirements:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							GRUB2 installed at <code class="literal">/var/lib/tftpboot</code>
						</li><li class="listitem">
							DHCP and TFTP for PXE
						</li></ul></div></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Download the iPXE script directly from the web console, or get the iPXE script from the Assisted Installer:
					</p><pre class="programlisting language-terminal">$ curl \
  --silent \
  --header "Authorization: Bearer $API_TOKEN" \
  https://api.openshift.com/api/assisted-install/v2/infra-envs/$INFRA_ENV_ID/downloads/files?file_name=ipxe-script &gt; ipxe-script</pre><div class="formalpara"><p class="title"><strong>Example</strong></p><p>
							
<pre class="programlisting language-ipxe">#!ipxe
initrd --name initrd http://api.openshift.com/api/assisted-images/images/&lt;infra_env_id&gt;/pxe-initrd?arch=x86_64&amp;image_token=&lt;token_string&gt;&amp;version=4.10
kernel http://api.openshift.com/api/assisted-images/boot-artifacts/kernel?arch=x86_64&amp;version=4.10 initrd=initrd coreos.live.rootfs_url=http://api.openshift.com/api/assisted-images/boot-artifacts/rootfs?arch=x86_64&amp;version=4.10 random.trust_cpu=on rd.luks.options=discard ignition.firstboot ignition.platform.id=metal console=tty1 console=ttyS1,115200n8 coreos.inst.persistent-kargs="console=tty1 console=ttyS1,115200n8"
boot</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Download the required artifacts by extracting URLs from the <code class="literal">ipxe-script</code>.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Download the initial RAM disk:
							</p><pre class="programlisting language-terminal">$ awk '/^initrd /{print $NF}' ipxe-script | curl -o initrd.img</pre></li><li class="listitem"><p class="simpara">
								Download the Linux kernel:
							</p><pre class="programlisting language-terminal">$ awk '/^kernel /{print $2}' ipxe-script | curl -o kernel</pre></li><li class="listitem"><p class="simpara">
								Download the root filesystem:
							</p><pre class="programlisting language-terminal">$ grep ^kernel ipxe-script | xargs -n1| grep ^coreos.live.rootfs_url | cut -d = -f 2- | curl -o rootfs.img</pre></li></ol></div></li><li class="listitem"><p class="simpara">
						Change the URLs to the different artifacts in the <code class="literal">ipxe-script`</code> to match your local HTTP server. For example:
					</p><pre class="programlisting language-ipxe">#!ipxe
set webserver http://192.168.0.1
initrd --name initrd $webserver/initrd.img
kernel $webserver/kernel initrd=initrd coreos.live.rootfs_url=$webserver/rootfs.img random.trust_cpu=on rd.luks.options=discard ignition.firstboot ignition.platform.id=metal console=tty1 console=ttyS1,115200n8 coreos.inst.persistent-kargs="console=tty1 console=ttyS1,115200n8"
boot</pre></li><li class="listitem"><p class="simpara">
						Optional: When installing with RHEL KVM on IBM Z® you must boot the host by specifying additional kernel arguments.
					</p><pre class="programlisting language-terminal">random.trust_cpu=on rd.luks.options=discard ignition.firstboot ignition.platform.id=metal console=tty1 console=ttyS1,115200n8 coreos.inst.persistent-kargs="console=tty1 console=ttyS1,115200n8</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							If you install with iPXE on RHEL KVM, in some circumstances, the VMs on the VM host are not rebooted on first boot and need to be started manually.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Optional: When installing on IBM Power® you must download intramfs, kernel, and root as follows:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
								Copy initrd.img and kernel.img to PXE directory `/var/lib/tftpboot/rhcos `
							</li><li class="listitem">
								Copy rootfs.img to HTTPD directory `/var/www/html/install `
							</li><li class="listitem"><p class="simpara">
								Add following entry to `/var/lib/tftpboot/boot/grub2/grub.cfg `:
							</p><pre class="programlisting language-terminal">if [ ${net_default_mac} == fa:1d:67:35:13:20 ]; then
default=0
fallback=1
timeout=1
menuentry "CoreOS (BIOS)" {
echo "Loading kernel"
linux "/rhcos/kernel.img" ip=dhcp rd.neednet=1 ignition.platform.id=metal ignition.firstboot coreos.live.rootfs_url=http://9.114.98.8:8000/install/rootfs.img
echo "Loading initrd"
initrd "/rhcos/initrd.img"
}
fi</pre></li></ol></div></li></ol></div></section></section><section class="chapter" id="assembly_role-assignment"><div class="titlepage"><div><div><h1 class="title">Chapter 9. Assigning roles to hosts</h1></div></div></div><p>
			You can assign roles to your discovered hosts. These roles define the function of the host within the cluster. The roles can be one of the standard Kubernetes types: <span class="strong strong"><strong>control plane (master)</strong></span> or <span class="strong strong"><strong>worker</strong></span>.
		</p><p>
			The host must meet the minimum requirements for the role you selected. You can find the hardware requirements by referring to the Prerequisites section of this document or using the preflight requirement API.
		</p><p>
			If you do not select a role, the system selects one for you. You can change the role at any time before installation starts.
		</p><section class="section" id="selecting-role-web_role-assignment"><div class="titlepage"><div><div><h2 class="title">9.1. Selecting a role by using the web console</h2></div></div></div><p>
				You can select a role after the host finishes its discovery.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Go to the <span class="strong strong"><strong>Host Discovery</strong></span> tab and scroll down to the <span class="strong strong"><strong>Host Inventory</strong></span> table.
					</li><li class="listitem">
						Select the <span class="strong strong"><strong>Auto-assign</strong></span> drop-down for the required host.
					</li><li class="listitem">
						Select <span class="strong strong"><strong>Control plane node</strong></span> to assign this host a control plane role.
					</li><li class="listitem">
						Select <span class="strong strong"><strong>Worker</strong></span> to assign this host a worker role.
					</li><li class="listitem">
						Check the validation status.
					</li></ol></div></section><section class="section" id="selecting-a-role-using-the-api_role-assignment"><div class="titlepage"><div><div><h2 class="title">9.2. Selecting a role by using the API</h2></div></div></div><p>
				You can select a role for the host by using the <a class="link mimir-link-warn" href="https://api.openshift.com/?urls.primaryName=assisted-service%20service#/installer/v2UpdateHost" title="Mimir does not include content from: api.openshift.com">/v2/infra-envs/{infra_env_id}/hosts/{host_id}</a> endpoint. A host can have one of the following roles:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<code class="literal">master</code>: A host with the <code class="literal">master</code> role operates as a control plane node.
					</li><li class="listitem">
						<code class="literal">worker</code>: A host with the <code class="literal">worker</code> role operates as a worker node.
					</li></ul></div><p>
				By default, the Assisted Installer sets a host to <code class="literal">auto-assign</code>, which means the Assisted Installer will determine whether the host is a <code class="literal">master</code> or <code class="literal">worker</code> role automatically. Use this procedure to set the host’s role.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have added hosts to the cluster.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Refresh the API token:
					</p><pre class="programlisting language-terminal">$ source refresh-token</pre></li><li class="listitem"><p class="simpara">
						Get the host IDs:
					</p><pre class="programlisting language-terminal">$ curl -s -X GET "https://api.openshift.com/api/assisted-install/v2/clusters/$CLUSTER_ID" \
--header "Content-Type: application/json" \
  -H "Authorization: Bearer $API_TOKEN" \
| jq '.host_networks[].host_ids'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">[
  "1062663e-7989-8b2d-7fbb-e6f4d5bb28e5"
]</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Modify the <code class="literal">host_role</code> setting:
					</p><pre class="programlisting language-terminal">$ curl https://api.openshift.com/api/assisted-install/v2/infra-envs/${INFRA_ENV_ID}/hosts/&lt;host_id&gt; \
-X PATCH \
-H "Authorization: Bearer ${API_TOKEN}" \
-H "Content-Type: application/json" \
-d '
    {
      "host_role":"worker"
    }
' | jq</pre><p class="simpara">
						Replace <code class="literal">&lt;host_id&gt;</code> with the ID of the host.
					</p></li></ol></div></section><section class="section" id="con_auto-assign-role_role-assignment"><div class="titlepage"><div><div><h2 class="title">9.3. Auto-assigning roles</h2></div></div></div><p>
				Assisted Installer selects a role automatically for hosts if you do not assign a role yourself. The role selection mechanism factors the host’s memory, CPU, and disk space. It aims to assign a control plane role to the 3 weakest hosts that meet the minimum requirements for control plane nodes. All other hosts default to worker nodes. The goal is to provide enough resources to run the control plane and reserve the more capacity-intensive hosts for running the actual workloads.
			</p><p>
				You can override the auto-assign decision at any time before installation.
			</p><p>
				The validations make sure that the auto selection is a valid one.
			</p></section><section class="section _additional-resources" id="additional_resources_3"><div class="titlepage"><div><div><h2 class="title">9.4. Additional resources</h2></div></div></div><p>
				<a class="link" href="#prerequisites" title="Chapter 2. Prerequisites">Prerequisites</a>
			</p></section></section><section class="chapter" id="assembly_preinstallation-validations"><div class="titlepage"><div><div><h1 class="title">Chapter 10. Preinstallation validations</h1></div></div></div><section class="section" id="definition-of-preinstallation-validations_preinstallation-validations"><div class="titlepage"><div><div><h2 class="title">10.1. Definition of preinstallation validations</h2></div></div></div><p>
				The Assisted Installer aims to make cluster installation as simple, efficient, and error-free as possible. The Assisted Installer performs validation checks on the configuration and the gathered telemetry before starting an installation.
			</p><p>
				The Assisted Installer uses the information provided before installation, such as control plane topology, network configuration and hostnames. It will also use real time telemetry from the hosts you are attempting to install.
			</p><p>
				When a host boots the discovery ISO, an agent will start on the host. The agent will send information about the state of the host to the Assisted Installer.
			</p><p>
				The Assisted Installer uses all of this information to compute real time preinstallation validations. All validations are either blocking or non-blocking to the installation.
			</p></section><section class="section" id="blocking-and-non-blocking-validations_preinstallation-validations"><div class="titlepage"><div><div><h2 class="title">10.2. Blocking and non-blocking validations</h2></div></div></div><p>
				A blocking validation will prevent progress of the installation, meaning that you will need to resolve the issue and pass the blocking validation before you can proceed.
			</p><p>
				A non-blocking validation is a warning and will tell you of things that might cause you a problem.
			</p></section><section class="section" id="validation-types_preinstallation-validations"><div class="titlepage"><div><div><h2 class="title">10.3. Validation types</h2></div></div></div><p>
				The Assisted Installer performs two types of validation:
			</p><div class="formalpara"><p class="title"><strong>Host</strong></p><p>
					Host validations ensure that the configuration of a given host is valid for installation.
				</p></div><div class="formalpara"><p class="title"><strong>Cluster</strong></p><p>
					Cluster validations ensure that the configuration of the whole cluster is valid for installation.
				</p></div></section><section class="section" id="host_validations"><div class="titlepage"><div><div><h2 class="title">10.4. Host validations</h2></div></div></div><section class="section" id="getting-host-validations-by-using-rest-api_preinstallation-validations"><div class="titlepage"><div><div><h3 class="title">10.4.1. Getting host validations by using the REST API</h3></div></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						If you use the web console, many of these validations will not show up by name. To get a list of validations consistent with the labels, use the following procedure.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the <code class="literal">jq</code> utility.
						</li><li class="listitem">
							You have created an Infrastructure Environment by using the API or have created a cluster by using the web console.
						</li><li class="listitem">
							You have hosts booted with the discovery ISO
						</li><li class="listitem">
							You have your Cluster ID exported in your shell as <code class="literal">CLUSTER_ID</code>.
						</li><li class="listitem">
							You have credentials to use when accessing the API and have exported a token as <code class="literal">API_TOKEN</code> in your shell.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedures</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Refresh the API token:
						</p><pre class="programlisting language-terminal">$ source refresh-token</pre></li><li class="listitem"><p class="simpara">
							Get all validations for all hosts:
						</p><pre class="programlisting language-terminal">$ curl \
  --silent \
  --header "Authorization: Bearer $API_TOKEN" \
  https://api.openshift.com/api/assisted-install/v2/clusters/$CLUSTER_ID/hosts \
  | jq -r .[].validations_info \
  | jq 'map(.[])'</pre></li><li class="listitem"><p class="simpara">
							Get non-passing validations for all hosts:
						</p><pre class="programlisting language-terminal">$ curl \
  --silent \
  --header "Authorization: Bearer $API_TOKEN" \
  https://api.openshift.com/api/assisted-install/v2/clusters/$CLUSTER_ID/hosts \
  | jq -r .[].validations_info \
  | jq 'map(.[]) | map(select(.status=="failure" or .status=="pending")) | select(length&gt;0)'</pre></li></ol></div></section><section class="section" id="host-validations-in-detail_preinstallation-validations"><div class="titlepage"><div><div><h3 class="title">10.4.2. Host validations in detail</h3></div></div></div><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 17%; " class="col_2"><!--Empty--></col><col style="width: 50%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139800911966624" scope="col">Parameter</th><th align="left" valign="top" id="idm139800911965536" scope="col">Validation type</th><th align="left" valign="top" id="idm139800911964448" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139800911966624"> <p>
									<code class="literal">connected</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800911965536"> <p>
									non-blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800911964448"> <p>
									Checks that the host has recently communicated with the Assisted Installer.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800911966624"> <p>
									<code class="literal">has-inventory</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800911965536"> <p>
									non-blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800911964448"> <p>
									Checks that the Assisted Installer received the inventory from the host.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800911966624"> <p>
									<code class="literal">has-min-cpu-cores</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800911965536"> <p>
									non-blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800911964448"> <p>
									Checks that the number of CPU cores meets the minimum requirements.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800911966624"> <p>
									<code class="literal">has-min-memory</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800911965536"> <p>
									non-blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800911964448"> <p>
									Checks that the amount of memory meets the minimum requirements.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800911966624"> <p>
									<code class="literal">has-min-valid-disks</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800911965536"> <p>
									non-blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800911964448"> <p>
									Checks that at least one available disk meets the eligibility criteria.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800911966624"> <p>
									<code class="literal">has-cpu-cores-for-role</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800911965536"> <p>
									blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800911964448"> <p>
									Checks that the number of cores meets the minimum requirements for the host role.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800911966624"> <p>
									<code class="literal">has-memory-for-role</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800911965536"> <p>
									blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800911964448"> <p>
									Checks that the amount of memory meets the minimum requirements for the host role.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800911966624"> <p>
									<code class="literal">ignition-downloadable</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800911965536"> <p>
									blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800911964448"> <p>
									For Day 2 hosts, checks that the host can download ignition configuration from the Day 1 cluster.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800911966624"> <p>
									<code class="literal">belongs-to-majority-group</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800911965536"> <p>
									blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800911964448"> <p>
									The majority group is the largest full-mesh connectivity group on the cluster, where all members can communicate with all other members. This validation checks that hosts in a multi-node, Day 1 cluster are in the majority group.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800911966624"> <p>
									<code class="literal">valid-platform-network-settings</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800911965536"> <p>
									blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800911964448"> <p>
									Checks that the platform is valid for the network settings.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800911966624"> <p>
									<code class="literal">ntp-synced</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800911965536"> <p>
									non-blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800911964448"> <p>
									Checks if an NTP server has been successfully used to synchronize time on the host.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800911966624"> <p>
									<code class="literal">container-images-available</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800911965536"> <p>
									non-blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800911964448"> <p>
									Checks if container images have been successfully pulled from the image registry.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800911966624"> <p>
									<code class="literal">sufficient-installation-disk-speed</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800911965536"> <p>
									blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800911964448"> <p>
									Checks that disk speed metrics from an earlier installation meet requirements, if they exist.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800911966624"> <p>
									<code class="literal">sufficient-network-latency-requirement-for-role</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800911965536"> <p>
									blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800911964448"> <p>
									Checks that the average network latency between hosts in the cluster meets the requirements.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800911966624"> <p>
									<code class="literal">sufficient-packet-loss-requirement-for-role</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800911965536"> <p>
									blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800911964448"> <p>
									Checks that the network packet loss between hosts in the cluster meets the requirements.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800911966624"> <p>
									<code class="literal">has-default-route</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800911965536"> <p>
									blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800911964448"> <p>
									Checks that the host has a default route configured.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800911966624"> <p>
									<code class="literal">api-domain-name-resolved-correctly</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800911965536"> <p>
									blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800911964448"> <p>
									For a multi node cluster with user managed networking. Checks that the host is able to resolve the API domain name for the cluster.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800911966624"> <p>
									<code class="literal">api-int-domain-name-resolved-correctly</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800911965536"> <p>
									blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800911964448"> <p>
									For a multi node cluster with user managed networking. Checks that the host is able to resolve the internal API domain name for the cluster.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800911966624"> <p>
									<code class="literal">apps-domain-name-resolved-correctly</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800911965536"> <p>
									blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800911964448"> <p>
									For a multi node cluster with user managed networking. Checks that the host is able to resolve the internal apps domain name for the cluster.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800911966624"> <p>
									<code class="literal">compatible-with-cluster-platform</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800911965536"> <p>
									non-blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800911964448"> <p>
									Checks that the host is compatible with the cluster platform
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800911966624"> <p>
									<code class="literal">dns-wildcard-not-configured</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800911965536"> <p>
									blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800911964448"> <p>
									Checks that the wildcard DNS *.&lt;cluster_name&gt;.&lt;base_domain&gt; is not configured, because this causes known problems for OpenShift
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800911966624"> <p>
									<code class="literal">disk-encryption-requirements-satisfied</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800911965536"> <p>
									non-blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800911964448"> <p>
									Checks that the type of host and disk encryption configured meet the requirements.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800911966624"> <p>
									<code class="literal">non-overlapping-subnets</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800911965536"> <p>
									blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800911964448"> <p>
									Checks that this host does not have any overlapping subnets.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800911966624"> <p>
									<code class="literal">hostname-unique</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800911965536"> <p>
									blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800911964448"> <p>
									Checks that the hostname is unique in the cluster.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800911966624"> <p>
									<code class="literal">hostname-valid</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800911965536"> <p>
									blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800911964448"> <p>
									Checks the validity of the hostname, meaning that it matches the general form of hostnames and is not forbidden.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800911966624"> <p>
									<code class="literal">belongs-to-machine-cidr</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800911965536"> <p>
									blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800911964448"> <p>
									Checks that the host IP is in the address range of the machine CIDR.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800911966624"> <p>
									<code class="literal">lso-requirements-satisfied</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800911965536"> <p>
									blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800911964448"> <p>
									Validates that the cluster meets the requirements of the Local Storage Operator.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800911966624"> <p>
									<code class="literal">odf-requirements-satisfied</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800911965536"> <p>
									blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800911964448"> <p>
									Validates that the cluster meets the requirements of the OpenShift Data Foundation Operator.
								</p>
								 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											The cluster has a minimum of 3 hosts.
										</li><li class="listitem">
											The cluster has only 3 masters or a minimum of 3 workers.
										</li><li class="listitem">
											The cluster has 3 eligible disks and each host must have an eligible disk.
										</li><li class="listitem">
											The host role must not be "Auto Assign" for clusters with more than three hosts.
										</li></ul></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800911966624"> <p>
									<code class="literal">cnv-requirements-satisfied</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800911965536"> <p>
									blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800911964448"> <p>
									Validates that the cluster meets the requirements of Container Native Virtualization.
								</p>
								 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											The BIOS of the host must have CPU virtualization enabled.
										</li><li class="listitem">
											Host must have enough CPU cores and RAM available for Container Native Virtualization.
										</li><li class="listitem">
											Will validate the Host Path Provisioner if necessary.
										</li></ul></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800911966624"> <p>
									<code class="literal">lvm-requirements-satisfied</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800911965536"> <p>
									blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800911964448"> <p>
									Validates that the cluster meets the requirements of the Logical Volume Manager Operator.
								</p>
								 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											Host has at least one additional empty disk, not partitioned and not formatted.
										</li></ul></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800911966624"> <p>
									<code class="literal">vsphere-disk-uuid-enabled</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800911965536"> <p>
									non-blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800911964448"> <p>
									Verifies that each valid disk sets <code class="literal">disk.EnableUUID</code> to <span class="emphasis"><em>true</em></span>. In vSphere this will result in each disk having a UUID.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800911966624"> <p>
									<code class="literal">compatible-agent</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800911965536"> <p>
									blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800911964448"> <p>
									Checks that the discovery agent version is compatible with the agent docker image version.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800911966624"> <p>
									<code class="literal">no-skip-installation-disk</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800911965536"> <p>
									blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800911964448"> <p>
									Checks that installation disk is not skipping disk formatting.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800911966624"> <p>
									<code class="literal">no-skip-missing-disk</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800911965536"> <p>
									blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800911964448"> <p>
									Checks that all disks marked to skip formatting are in the inventory. A disk ID can change on reboot, and this validation prevents issues caused by that.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800911966624"> <p>
									<code class="literal">media-connected</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800911965536"> <p>
									blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800911964448"> <p>
									Checks the connection of the installation media to the host.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800911966624"> <p>
									<code class="literal">machine-cidr-defined</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800911965536"> <p>
									non-blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800911964448"> <p>
									Checks that the machine network definition exists for the cluster.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800911966624"> <p>
									<code class="literal">id-platform-network-settings</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800911965536"> <p>
									blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800911964448"> <p>
									Checks that the platform is compatible with the network settings. Some platforms are only permitted when installing Single Node Openshift or when using User Managed Networking.
								</p>
								 </td></tr></tbody></table></div></section></section><section class="section" id="cluster_validations"><div class="titlepage"><div><div><h2 class="title">10.5. Cluster validations</h2></div></div></div><section class="section" id="getting-cluster-validations-by-using-rest-api_preinstallation-validations"><div class="titlepage"><div><div><h3 class="title">10.5.1. Getting cluster validations by using the REST API</h3></div></div></div><p>
					If you use the web console, many of these validations will not show up by name. To obtain a list of validations consistent with the labels, use the following procedure.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the <code class="literal">jq</code> utility.
						</li><li class="listitem">
							You have created an Infrastructure Environment by using the API or have created a cluster by using the web console.
						</li><li class="listitem">
							You have your Cluster ID exported in your shell as <code class="literal">CLUSTER_ID</code>.
						</li><li class="listitem">
							You have credentials to use when accessing the API and have exported a token as <code class="literal">API_TOKEN</code> in your shell.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedures</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Refresh the API token:
						</p><pre class="programlisting language-terminal">$ source refresh-token</pre></li><li class="listitem"><p class="simpara">
							Get all cluster validations:
						</p><pre class="programlisting language-terminal">$ curl \
  --silent \
  --header "Authorization: Bearer $API_TOKEN" \
  https://api.openshift.com/api/assisted-install/v2/clusters/$CLUSTER_ID \
  | jq -r .validations_info \
  | jq 'map(.[])'</pre></li><li class="listitem"><p class="simpara">
							Get non-passing cluster validations:
						</p><pre class="programlisting language-terminal">$ curl \
  --silent \
  --header "Authorization: Bearer $API_TOKEN" \
  https://api.openshift.com/api/assisted-install/v2/clusters/$CLUSTER_ID \
  | jq -r .validations_info \
  | jq '. | map(.[] | select(.status=="failure" or .status=="pending")) | select(length&gt;0)'</pre></li></ol></div></section><section class="section" id="cluster-validations-in-detail_preinstallation-validations"><div class="titlepage"><div><div><h3 class="title">10.5.2. Cluster validations in detail</h3></div></div></div><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 17%; " class="col_2"><!--Empty--></col><col style="width: 50%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139800912775072" scope="col">Parameter</th><th align="left" valign="top" id="idm139800912773984" scope="col">Validation type</th><th align="left" valign="top" id="idm139800912772896" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139800912775072"> <p>
									<code class="literal">machine-cidr-defined</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800912773984"> <p>
									non-blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800912772896"> <p>
									Checks that the machine network definition exists for the cluster.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800912775072"> <p>
									<code class="literal">cluster-cidr-defined</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800912773984"> <p>
									non-blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800912772896"> <p>
									Checks that the cluster network definition exists for the cluster.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800912775072"> <p>
									<code class="literal">service-cidr-defined</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800912773984"> <p>
									non-blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800912772896"> <p>
									Checks that the service network definition exists for the cluster.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800912775072"> <p>
									<code class="literal">no-cidrs-overlapping</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800912773984"> <p>
									blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800912772896"> <p>
									Checks that the defined networks do not overlap.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800912775072"> <p>
									<code class="literal">networks-same-address-families</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800912773984"> <p>
									blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800912772896"> <p>
									Checks that the defined networks share the same address families (valid address families are IPv4, IPv6)
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800912775072"> <p>
									<code class="literal">network-prefix-valid</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800912773984"> <p>
									blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800912772896"> <p>
									Checks the cluster network prefix to ensure that it is valid and allows enough address space for all hosts.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800912775072"> <p>
									<code class="literal">machine-cidr-equals-to-calculated-cidr</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800912773984"> <p>
									blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800912772896"> <p>
									For a non user managed networking cluster. Checks that <code class="literal">apiVIPs</code> or <code class="literal">ingressVIPs</code> are members of the machine CIDR if they exist.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800912775072"> <p>
									<code class="literal">api-vips-defined</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800912773984"> <p>
									non-blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800912772896"> <p>
									For a non user managed networking cluster. Checks that <code class="literal">apiVIPs</code> exist.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800912775072"> <p>
									<code class="literal">api-vips-valid</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800912773984"> <p>
									blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800912772896"> <p>
									For a non user managed networking cluster. Checks if the <code class="literal">apiVIPs</code> belong to the machine CIDR and are not in use.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800912775072"> <p>
									<code class="literal">ingress-vips-defined</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800912773984"> <p>
									blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800912772896"> <p>
									For a non user managed networking cluster. Checks that <code class="literal">ingressVIPs</code> exist.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800912775072"> <p>
									<code class="literal">ingress-vips-valid</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800912773984"> <p>
									non-blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800912772896"> <p>
									For a non user managed networking cluster. Checks if the <code class="literal">ingressVIPs</code> belong to the machine CIDR and are not in use.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800912775072"> <p>
									<code class="literal">all-hosts-are-ready-to-install</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800912773984"> <p>
									blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800912772896"> <p>
									Checks that all hosts in the cluster are in the "ready to install" status.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800912775072"> <p>
									<code class="literal">sufficient-masters-count</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800912773984"> <p>
									blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800912772896"> <p>
									This validation only applies to multi-node clusters.
								</p>
								 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											The cluster must have exactly three masters.
										</li><li class="listitem">
											If the cluster has worker nodes, a minimum of 2 worker nodes must exist.
										</li></ul></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800912775072"> <p>
									<code class="literal">dns-domain-defined</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800912773984"> <p>
									non-blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800912772896"> <p>
									Checks that the base DNS domain exists for the cluster.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800912775072"> <p>
									<code class="literal">pull-secret-set</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800912773984"> <p>
									non-blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800912772896"> <p>
									Checks that the pull secret exists. Does not check that the pull secret is valid or authorized.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800912775072"> <p>
									<code class="literal">ntp-server-configured</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800912773984"> <p>
									blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800912772896"> <p>
									Checks that each of the host clocks are no more than 4 minutes out of sync with each other.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800912775072"> <p>
									<code class="literal">lso-requirements-satisfied</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800912773984"> <p>
									blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800912772896"> <p>
									Validates that the cluster meets the requirements of the Local Storage Operator.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800912775072"> <p>
									<code class="literal">odf-requirements-satisfied</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800912773984"> <p>
									blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800912772896"> <p>
									Validates that the cluster meets the requirements of the Openshift Data Foundation Operator.
								</p>
								 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											The cluster has a minimum of 3 hosts.
										</li><li class="listitem">
											The cluster has only 3 masters or a minimum of 3 workers.
										</li><li class="listitem">
											The cluster has 3 eligible disks and each host must have an eligible disk.
										</li></ul></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800912775072"> <p>
									<code class="literal">cnv-requirements-satisfied</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800912773984"> <p>
									blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800912772896"> <p>
									Validates that the cluster meets the requirements of Container Native Virtualization.
								</p>
								 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											The CPU architecture for the cluster is x86
										</li></ul></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800912775072"> <p>
									<code class="literal">lvm-requirements-satisfied</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800912773984"> <p>
									blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800912772896"> <p>
									Validates that the cluster meets the requirements of the Logical Volume Manager Operator.
								</p>
								 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											The cluster must be single node.
										</li><li class="listitem">
											The cluster must be running Openshift &gt;= 4.11.0.
										</li></ul></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800912775072"> <p>
									<code class="literal">network-type-valid</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800912773984"> <p>
									blocking
								</p>
								 </td><td align="left" valign="top" headers="idm139800912772896"> <p>
									Checks the validity of the network type if it exists.
								</p>
								 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											The network type must be OpenshiftSDN or OVNKubernetes.
										</li><li class="listitem">
											OpenshiftSDN does not support IPv6 or Single Node Openshift. OpenshiftSDN is not supported for OpenShift Container Platform 4.15 and later releases.
										</li><li class="listitem">
											OVNKubernetes does not support VIP DHCP allocation.
										</li></ul></div>
								 </td></tr></tbody></table></div></section></section></section><section class="chapter" id="assembly_network-configuration"><div class="titlepage"><div><div><h1 class="title">Chapter 11. Network configuration</h1></div></div></div><p>
			This section describes the basics of network configuration using the Assisted Installer.
		</p><section class="section" id="cluster-networking_network-configuration"><div class="titlepage"><div><div><h2 class="title">11.1. Cluster networking</h2></div></div></div><p>
				There are various network types and addresses used by OpenShift and listed in the table below.
			</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 40%; " class="col_2"><!--Empty--></col><col style="width: 40%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139800915041296" scope="col">Type</th><th align="left" valign="top" id="idm139800911914480" scope="col">DNS</th><th align="left" valign="top" id="idm139800911913392" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139800915041296"> <p>
								<code class="literal">clusterNetwork</code>
							</p>
							 </td><td align="left" valign="top" headers="idm139800911914480"> </td><td align="left" valign="top" headers="idm139800911913392"> <p>
								The IP address pools from which Pod IP addresses are allocated.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139800915041296"> <p>
								<code class="literal">serviceNetwork</code>
							</p>
							 </td><td align="left" valign="top" headers="idm139800911914480"> </td><td align="left" valign="top" headers="idm139800911913392"> <p>
								The IP address pool for services.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139800915041296"> <p>
								<code class="literal">machineNetwork</code>
							</p>
							 </td><td align="left" valign="top" headers="idm139800911914480"> </td><td align="left" valign="top" headers="idm139800911913392"> <p>
								The IP address blocks for machines forming the cluster.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139800915041296"> <p>
								<code class="literal">apiVIP</code>
							</p>
							 </td><td align="left" valign="top" headers="idm139800911914480"> <p>
								<code class="literal">api.&lt;clustername.clusterdomain&gt;</code>
							</p>
							 </td><td align="left" valign="top" headers="idm139800911913392"> <p>
								The VIP to use for API communication. This setting must either be provided or preconfigured in the DNS so that the default name resolves correctly. If you are deploying with dual-stack networking, this must be the IPv4 address.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139800915041296"> <p>
								<code class="literal">apiVIPs</code>
							</p>
							 </td><td align="left" valign="top" headers="idm139800911914480"> <p>
								<code class="literal">api.&lt;clustername.clusterdomain&gt;</code>
							</p>
							 </td><td align="left" valign="top" headers="idm139800911913392"> <p>
								The VIPs to use for API communication. This setting must either be provided or preconfigured in the DNS so that the default name resolves correctly. If using dual stack networking, the first address must be the IPv4 address and the second address must be the IPv6 address. You must also set the <code class="literal">apiVIP</code> setting.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139800915041296"> <p>
								<code class="literal">ingressVIP</code>
							</p>
							 </td><td align="left" valign="top" headers="idm139800911914480"> <p>
								<code class="literal">*.apps.&lt;clustername.clusterdomain&gt;</code>
							</p>
							 </td><td align="left" valign="top" headers="idm139800911913392"> <p>
								The VIP to use for ingress traffic. If you are deploying with dual-stack networking, this must be the IPv4 address.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139800915041296"> <p>
								<code class="literal">ingressVIPs</code>
							</p>
							 </td><td align="left" valign="top" headers="idm139800911914480"> <p>
								<code class="literal">*.apps.&lt;clustername.clusterdomain&gt;</code>
							</p>
							 </td><td align="left" valign="top" headers="idm139800911913392"> <p>
								The VIPs to use for ingress traffic. If you are deploying with dual-stack networking, the first address must be the IPv4 address and the second address must be the IPv6 address. You must also set the <code class="literal">ingressVIP</code> setting.
							</p>
							 </td></tr></tbody></table></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					OpenShift Container Platform 4.12 introduces the new <code class="literal">apiVIPs</code> and <code class="literal">ingressVIPs</code> settings to accept multiple IP addresses for dual-stack networking. When using dual-stack networking, the first IP address must be the IPv4 address and the second IP address must be the IPv6 address. The new settings will replace <code class="literal">apiVIP</code> and <code class="literal">IngressVIP</code>, but you must set both the new and old settings when modifying the configuration using the API.
				</p></div></div><p>
				Depending on the required network stack, you can choose different network controllers. Currently, the Assisted Service can deploy OpenShift Container Platform clusters by using one of the following configurations:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						IPv4
					</li><li class="listitem">
						Dual-stack (IPv4 + IPv6)
					</li></ul></div><p>
				Supported network controllers depend on the selected stack and are summarized in the table below. For a detailed Container Network Interface (CNI) network provider feature comparison, refer to the <a class="link mimir-link-warn-approx" href="/products/#letter-O" title="Mimir does not include this page, but the link has been rewritten to point to the nearest parent document.">OCP Networking documentation</a>.
			</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139800914667040" scope="col">Stack</th><th align="left" valign="top" id="idm139800914665952" scope="col">SDN</th><th align="left" valign="top" id="idm139800914664864" scope="col">OVN</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139800914667040"> <p>
								IPv4
							</p>
							 </td><td align="left" valign="top" headers="idm139800914665952"> <p>
								Yes
							</p>
							 </td><td align="left" valign="top" headers="idm139800914664864"> <p>
								Yes
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139800914667040"> <p>
								Dual-stack
							</p>
							 </td><td align="left" valign="top" headers="idm139800914665952"> <p>
								No
							</p>
							 </td><td align="left" valign="top" headers="idm139800914664864"> <p>
								Yes
							</p>
							 </td></tr></tbody></table></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					OVN is the default Container Network Interface (CNI) in OpenShift Container Platform 4.12 and later releases. SDN is supported up to OpenShift Container Platform 4.14, but not for OpenShift Container Platform 4.15 and later releases.
				</p></div></div><section class="section" id="limitations"><div class="titlepage"><div><div><h3 class="title">11.1.1. Limitations</h3></div></div></div><section class="section" id="sdn"><div class="titlepage"><div><div><h4 class="title">11.1.1.1. SDN</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								The SDN controller is not supported with single-node OpenShift.
							</li><li class="listitem">
								The SDN controller does not support IPv6.
							</li><li class="listitem">
								The SDN controller is not supported for OpenShift Container Platform 4.15 and later releases. For more information, see <a class="link mimir-link-warn" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.15/html/release_notes/ocp-4-15-release-notes#ocp-4-15-deprecation-sdn" title="This content is not included in Mimir.">Deprecation of the OpenShift SDN network plugin</a> in the OpenShift Container Platform release notes.
							</li></ul></div></section><section class="section" id="ovn_kubernetes"><div class="titlepage"><div><div><h4 class="title">11.1.1.2. OVN-Kubernetes</h4></div></div></div><p>
						Please see the <a class="link mimir-link-warn-approx" href="/products/#letter-O" title="Mimir does not include this page, but the link has been rewritten to point to the nearest parent document.">OVN-Kubernetes limitations section in the OCP documentation</a>.
					</p></section></section><section class="section" id="cluster_network"><div class="titlepage"><div><div><h3 class="title">11.1.2. Cluster network</h3></div></div></div><p>
					The cluster network is a network from which every Pod deployed in the cluster gets its IP address. Given that the workload may live across many nodes forming the cluster, it’s important for the network provider to be able to easily find an individual node based on the Pod’s IP address. To do this, <code class="literal">clusterNetwork.cidr</code> is further split into subnets of the size defined in <code class="literal">clusterNetwork.hostPrefix</code>.
				</p><p>
					The host prefix specifies a length of the subnet assigned to each individual node in the cluster. An example of how a cluster may assign addresses for the multi-node cluster:
				</p><pre class="programlisting language-yaml">---
  clusterNetwork:
  - cidr: 10.128.0.0/14
    hostPrefix: 23
---</pre><p>
					Creating a 3-node cluster using the snippet above may create the following network topology:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Pods scheduled in node #1 get IPs from <code class="literal">10.128.0.0/23</code>
						</li><li class="listitem">
							Pods scheduled in node #2 get IPs from <code class="literal">10.128.2.0/23</code>
						</li><li class="listitem">
							Pods scheduled in node #3 get IPs from <code class="literal">10.128.4.0/23</code>
						</li></ul></div><p>
					Explaining OVN-K8s internals is out of scope for this document, but the pattern described above provides a way to route Pod-to-Pod traffic between different nodes without keeping a big list of mapping between Pods and their corresponding nodes.
				</p></section><section class="section" id="machine_network"><div class="titlepage"><div><div><h3 class="title">11.1.3. Machine network</h3></div></div></div><p>
					The machine network is a network used by all the hosts forming the cluster to communicate with each other. This is also the subnet that must include the API and Ingress VIPs.
				</p></section><section class="section" id="single_node_openshift_compared_to_multi_node_cluster"><div class="titlepage"><div><div><h3 class="title">11.1.4. Single-node OpenShift compared to multi-node cluster</h3></div></div></div><p>
					Depending on whether you are deploying single-node OpenShift or a multi-node cluster, different values are mandatory. The table below explains this in more detail.
				</p><div class="informaltable"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 25%; " class="col_1"><!--Empty--></col><col style="width: 25%; " class="col_2"><!--Empty--></col><col style="width: 25%; " class="col_3"><!--Empty--></col><col style="width: 25%; " class="col_4"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139800916200464" scope="col">Parameter</th><th align="left" valign="top" id="idm139800916199376" scope="col">single-node OpenShift</th><th align="left" valign="top" id="idm139800916198288" scope="col">Multi-node cluster with DHCP mode</th><th align="left" valign="top" id="idm139800916197232" scope="col">Multi-node cluster without DHCP mode</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139800916200464"> <p>
									<code class="literal">clusterNetwork</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800916199376"> <p>
									Required
								</p>
								 </td><td align="left" valign="top" headers="idm139800916198288"> <p>
									Required
								</p>
								 </td><td align="left" valign="top" headers="idm139800916197232"> <p>
									Required
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800916200464"> <p>
									<code class="literal">serviceNetwork</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800916199376"> <p>
									Required
								</p>
								 </td><td align="left" valign="top" headers="idm139800916198288"> <p>
									Required
								</p>
								 </td><td align="left" valign="top" headers="idm139800916197232"> <p>
									Required
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800916200464"> <p>
									<code class="literal">machineNetwork</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800916199376"> <p>
									Auto-assign possible (*)
								</p>
								 </td><td align="left" valign="top" headers="idm139800916198288"> <p>
									Auto-assign possible (*)
								</p>
								 </td><td align="left" valign="top" headers="idm139800916197232"> <p>
									Auto-assign possible (*)
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800916200464"> <p>
									<code class="literal">apiVIP</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800916199376"> <p>
									Forbidden
								</p>
								 </td><td align="left" valign="top" headers="idm139800916198288"> <p>
									Forbidden
								</p>
								 </td><td align="left" valign="top" headers="idm139800916197232"> <p>
									Required
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800916200464"> <p>
									<code class="literal">apiVIPs</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800916199376"> <p>
									Forbidden
								</p>
								 </td><td align="left" valign="top" headers="idm139800916198288"> <p>
									Forbidden
								</p>
								 </td><td align="left" valign="top" headers="idm139800916197232"> <p>
									Required in 4.12 and later releases
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800916200464"> <p>
									<code class="literal">ingressVIP</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800916199376"> <p>
									Forbidden
								</p>
								 </td><td align="left" valign="top" headers="idm139800916198288"> <p>
									Forbidden
								</p>
								 </td><td align="left" valign="top" headers="idm139800916197232"> <p>
									Required
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139800916200464"> <p>
									<code class="literal">ingressVIPs</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139800916199376"> <p>
									Forbidden
								</p>
								 </td><td align="left" valign="top" headers="idm139800916198288"> <p>
									Forbidden
								</p>
								 </td><td align="left" valign="top" headers="idm139800916197232"> <p>
									Required in 4.12 and later releases
								</p>
								 </td></tr></tbody></table></div><p>
					(*) Auto assignment of the machine network CIDR happens if there is only a single host network. Otherwise you need to specify it explicitly.
				</p></section><section class="section" id="air_gapped_environments"><div class="titlepage"><div><div><h3 class="title">11.1.5. Air-gapped environments</h3></div></div></div><p>
					The workflow for deploying a cluster without Internet access has some prerequisites which are out of scope of this document. You can consult the <a class="link mimir-link-warn" href="https://github.com/jparrill/ztp-the-hard-way/blob/main/docs/disconnected-ZTP-flow-hub-deployment.md" title="Mimir does not include content from: github.com">Zero Touch Provisioning the hard way Git repository</a> for some insights.
				</p></section></section><section class="section" id="vip-dhcp-allocation_network-configuration"><div class="titlepage"><div><div><h2 class="title">11.2. VIP DHCP allocation</h2></div></div></div><p>
				The VIP DHCP allocation is a feature allowing users to skip the requirement of manually providing virtual IPs for API and Ingress by leveraging the ability of a service to automatically assign those IP addresses from the DHCP server.
			</p><p>
				If you enable the feature, instead of using <code class="literal">api_vips</code> and <code class="literal">ingress_vips</code> from the cluster configuration, the service will send a lease allocation request and based on the reply it will use VIPs accordingly. The service will allocate the IP addresses from the Machine Network.
			</p><p>
				Please note this is not an OpenShift Container Platform feature and it has been implemented in the Assisted Service to make the configuration easier.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					VIP DHCP allocation is currently limited to the OpenShift Container Platform SDN network type. SDN is not supported from OpenShift Container Platform version 4.15 and later. Therefore, support for VIP DHCP allocation is also ending from OpenShift Container Platform 4.15 and later.
				</p></div></div><section class="section" id="example_payload_to_enable_autoallocation"><div class="titlepage"><div><div><h3 class="title">11.2.1. Example payload to enable autoallocation</h3></div></div></div><pre class="programlisting language-json">---
{
  "vip_dhcp_allocation": true,
  "network_type": "OVNKubernetes",
  "user_managed_networking": false,
  "cluster_networks": [
    {
      "cidr": "10.128.0.0/14",
      "host_prefix": 23
    }
  ],
  "service_networks": [
    {
      "cidr": "172.30.0.0/16"
    }
  ],
  "machine_networks": [
    {
      "cidr": "192.168.127.0/24"
    }
  ]
}
---</pre></section><section class="section" id="example_payload_to_disable_autoallocation"><div class="titlepage"><div><div><h3 class="title">11.2.2. Example payload to disable autoallocation</h3></div></div></div><pre class="programlisting language-json">---
{
  "api_vips": [
    {
        "ip": "192.168.127.100"
    }
  ],
  "ingress_vips": [
    {
        "ip": "192.168.127.101"
    }
  ],
  "vip_dhcp_allocation": false,
  "network_type": "OVNKubernetes",
  "user_managed_networking": false,
  "cluster_networks": [
    {
      "cidr": "10.128.0.0/14",
      "host_prefix": 23
    }
  ],
  "service_networks": [
    {
      "cidr": "172.30.0.0/16"
    }
  ]
}
---</pre></section></section><section class="section _additional-resources" id="additional_resources_4"><div class="titlepage"><div><div><h2 class="title">11.3. Additional resources</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link mimir-link-warn-approx" href="/products/#letter-O" title="Mimir does not include this page, but the link has been rewritten to point to the nearest parent document.">Bare metal IPI documentation</a> provides additional explanation of the syntax for the VIP addresses.
					</li></ul></div></section><section class="section" id="understanding-differences-between-user-and-cluster-managed-networking_network-configuration"><div class="titlepage"><div><div><h2 class="title">11.4. Understanding differences between user- and cluster-managed networking</h2></div></div></div><p>
				User managed networking is a feature in the Assisted Installer that allows customers with non-standard network topologies to deploy OpenShift Container Platform clusters. Examples include:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Customers with an external load balancer who do not want to use <code class="literal">keepalived</code> and VRRP for handling VIP addressses.
					</li><li class="listitem">
						Deployments with cluster nodes distributed across many distinct L2 network segments.
					</li></ul></div><section class="section" id="validations"><div class="titlepage"><div><div><h3 class="title">11.4.1. Validations</h3></div></div></div><p>
					There are various network validations happening in the Assisted Installer before it allows the installation to start. When you enable User Managed Networking, the following validations change:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							L3 connectivity check (ICMP) is performed instead of L2 check (ARP)
						</li></ul></div></section></section><section class="section" id="static-network-configuration_network-configuration"><div class="titlepage"><div><div><h2 class="title">11.5. Static network configuration</h2></div></div></div><p>
				You may use static network configurations when generating or updating the discovery ISO.
			</p><section class="section" id="prerequisites-1"><div class="titlepage"><div><div><h3 class="title">11.5.1. Prerequisites</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							You are familiar with <a class="link mimir-link-warn" href="https://nmstate.io/examples.html" title="Mimir does not include content from: nmstate.io">NMState</a>.
						</li></ul></div></section><section class="section" id="nmstate_configuration"><div class="titlepage"><div><div><h3 class="title">11.5.2. NMState configuration</h3></div></div></div><p>
					The NMState file in YAML format specifies the desired network configuration for the host. It has the logical names of the interfaces that will be replaced with the actual name of the interface at discovery time.
				</p><section class="section" id="example_of_nmstate_configuration"><div class="titlepage"><div><div><h4 class="title">11.5.2.1. Example of NMState configuration</h4></div></div></div><pre class="programlisting language-yaml">---
dns-resolver:
  config:
    server:
    - 192.168.126.1
interfaces:
- ipv4:
    address:
    - ip: 192.168.126.30
      prefix-length: 24
    dhcp: false
    enabled: true
  name: eth0
  state: up
  type: ethernet
- ipv4:
    address:
    - ip: 192.168.141.30
      prefix-length: 24
    dhcp: false
    enabled: true
  name: eth1
  state: up
  type: ethernet
routes:
  config:
  - destination: 0.0.0.0/0
    next-hop-address: 192.168.126.1
    next-hop-interface: eth0
    table-id: 254
---</pre></section></section><section class="section" id="mac_interface_mapping"><div class="titlepage"><div><div><h3 class="title">11.5.3. MAC interface mapping</h3></div></div></div><p>
					MAC interface map is an attribute that maps logical interfaces defined in the NMState configuration with the actual interfaces present on the host.
				</p><p>
					The mapping should always use physical interfaces present on the host. For example, when the NMState configuration defines a bond or VLAN, the mapping should only contain an entry for parent interfaces.
				</p><section class="section" id="example_of_mac_interface_mapping"><div class="titlepage"><div><div><h4 class="title">11.5.3.1. Example of MAC interface mapping</h4></div></div></div><pre class="programlisting language-yaml">---
mac_interface_map: [
    {
      mac_address: 02:00:00:2c:23:a5,
      logical_nic_name: eth0
    },
    {
      mac_address: 02:00:00:68:73:dc,
      logical_nic_name: eth1
    }
]
---</pre></section></section><section class="section" id="additional_nmstate_configuration_examples"><div class="titlepage"><div><div><h3 class="title">11.5.4. Additional NMState configuration examples</h3></div></div></div><p>
					The examples below are only meant to show a partial configuration. They are not meant to be used as-is, and you should always adjust to the environment where they will be used. If used incorrectly, they may leave your machines with no network connectivity.
				</p><section class="section" id="tagged_vlan"><div class="titlepage"><div><div><h4 class="title">11.5.4.1. Tagged VLAN</h4></div></div></div><pre class="programlisting language-yaml">---
    interfaces:
    - ipv4:
        address:
        - ip: 192.168.143.15
          prefix-length: 24
        dhcp: false
        enabled: true
      ipv6:
        enabled: false
      name: eth0.404
      state: up
      type: vlan
      vlan:
        base-iface: eth0
        id: 404
        reorder-headers: true
---</pre></section><section class="section" id="network_bond"><div class="titlepage"><div><div><h4 class="title">11.5.4.2. Network bond</h4></div></div></div><pre class="programlisting language-yaml">---
    interfaces:
    - ipv4:
        address:
        - ip: 192.168.138.15
          prefix-length: 24
        dhcp: false
        enabled: true
      ipv6:
        enabled: false
      link-aggregation:
        mode: active-backup
        options:
          all_slaves_active: delivered
          miimon: "140"
        slaves:
        - eth0
        - eth1
      name: bond0
      state: up
      type: bond
---</pre></section></section></section><section class="section" id="applying-static-network-configuration_network-configuration"><div class="titlepage"><div><div><h2 class="title">11.6. Applying a static network configuration with the API</h2></div></div></div><p>
				You can apply a static network configuration using the Assisted Installer API.
			</p><div class="orderedlist"><p class="title"><strong>Prerequisites</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						You have created an infrastructure environment using the API or have created a cluster using the web console.
					</li><li class="listitem">
						You have your infrastructure environment ID exported in your shell as <code class="literal">$INFRA_ENV_ID</code>.
					</li><li class="listitem">
						You have credentials to use when accessing the API and have exported a token as <code class="literal">$API_TOKEN</code> in your shell.
					</li><li class="listitem">
						You have YAML files with a static network configuration available as <code class="literal">server-a.yaml</code> and <code class="literal">server-b.yaml</code>.
					</li></ol></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create a temporary file <code class="literal">/tmp/request-body.txt</code> with the API request:
					</p><pre class="programlisting language-terminal">---
jq -n --arg NMSTATE_YAML1 "$(cat server-a.yaml)" --arg NMSTATE_YAML2 "$(cat server-b.yaml)" \
'{
  "static_network_config": [
    {
      "network_yaml": $NMSTATE_YAML1,
      "mac_interface_map": [{"mac_address": "02:00:00:2c:23:a5", "logical_nic_name": "eth0"}, {"mac_address": "02:00:00:68:73:dc", "logical_nic_name": "eth1"}]
    },
    {
      "network_yaml": $NMSTATE_YAML2,
      "mac_interface_map": [{"mac_address": "02:00:00:9f:85:eb", "logical_nic_name": "eth1"}, {"mac_address": "02:00:00:c8:be:9b", "logical_nic_name": "eth0"}]
     }
  ]
}' &gt;&gt; /tmp/request-body.txt
---</pre></li><li class="listitem"><p class="simpara">
						Refresh the API token:
					</p><pre class="programlisting language-terminal">$ source refresh-token</pre></li><li class="listitem"><p class="simpara">
						Send the request to the Assisted Service API endpoint:
					</p><pre class="programlisting language-terminal">---
$ curl -H "Content-Type: application/json" \
-X PATCH -d @/tmp/request-body.txt \
-H "Authorization: Bearer ${API_TOKEN}" \
https://api.openshift.com/api/assisted-install/v2/infra-envs/$INFRA_ENV_ID
---</pre></li></ol></div></section><section class="section _additional-resources" id="additional_resources_5"><div class="titlepage"><div><div><h2 class="title">11.7. Additional resources</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link" href="#configuring-static-networks_installing-with-ui" title="3.3. Optional: Configuring static networks">Applying a static network configuration with the web console</a>
					</li></ul></div></section><section class="section" id="converting-to-dual-stack-networking_network-configuration"><div class="titlepage"><div><div><h2 class="title">11.8. Converting to dual-stack networking</h2></div></div></div><p>
				Dual-stack IPv4/IPv6 configuration allows deployment of a cluster with pods residing in both IPv4 and IPv6 subnets.
			</p><section class="section" id="prerequisites_2"><div class="titlepage"><div><div><h3 class="title">11.8.1. Prerequisites</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							You are familiar with <a class="link mimir-link-warn-approx" href="/products/#letter-O" title="Mimir does not include this page, but the link has been rewritten to point to the nearest parent document.">OVN-K8s documentation</a>
						</li></ul></div></section><section class="section" id="example_payload_for_single_node_openshift"><div class="titlepage"><div><div><h3 class="title">11.8.2. Example payload for single-node OpenShift</h3></div></div></div><pre class="programlisting language-json">---
{
  "network_type": "OVNKubernetes",
  "user_managed_networking": false,
  "cluster_networks": [
    {
      "cidr": "10.128.0.0/14",
      "host_prefix": 23
    },
    {
      "cidr": "fd01::/48",
      "host_prefix": 64
    }
  ],
  "service_networks": [
    {"cidr": "172.30.0.0/16"}, {"cidr": "fd02::/112"}
  ],
  "machine_networks": [
    {"cidr": "192.168.127.0/24"},{"cidr": "1001:db8::/120"}
  ]
}
---</pre></section><section class="section" id="example_payload_for_an_openshift_container_platform_cluster_consisting_of_many_nodes"><div class="titlepage"><div><div><h3 class="title">11.8.3. Example payload for an OpenShift Container Platform cluster consisting of many nodes</h3></div></div></div><pre class="programlisting language-json">---
{
  "vip_dhcp_allocation": false,
  "network_type": "OVNKubernetes",
  "user_managed_networking": false,
  "api_vips": [
     {
        "ip": "192.168.127.100"
     },
     {
        "ip": "2001:0db8:85a3:0000:0000:8a2e:0370:7334"
     }
  ],
  "ingress_vips": [
     {
        "ip": "192.168.127.101"
     },
     {
        "ip": "2001:0db8:85a3:0000:0000:8a2e:0370:7335"
     }
  ],
  "cluster_networks": [
    {
      "cidr": "10.128.0.0/14",
      "host_prefix": 23
    },
    {
      "cidr": "fd01::/48",
      "host_prefix": 64
    }
  ],
  "service_networks": [
    {"cidr": "172.30.0.0/16"}, {"cidr": "fd02::/112"}
  ],
  "machine_networks": [
    {"cidr": "192.168.127.0/24"},{"cidr": "1001:db8::/120"}
  ]
}
---</pre></section><section class="section" id="limitations_2"><div class="titlepage"><div><div><h3 class="title">11.8.4. Limitations</h3></div></div></div><p>
					The <code class="literal">api_vips</code> IP address and <code class="literal">ingress_vips</code> IP address settings must be of the primary IP address family when using dual-stack networking, which must be IPv4 addresses. Currently, Red Hat does not support dual-stack VIPs or dual-stack networking with IPv6 as the primary IP address family. Red Hat supports dual-stack networking with IPv4 as the primary IP address family and IPv6 as the secondary IP address family. Therefore, you must place the IPv4 entries before the IPv6 entries when entering the IP address values.
				</p></section></section><section class="section _additional-resources" id="additional_resources_6"><div class="titlepage"><div><div><h2 class="title">11.9. Additional resources</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link mimir-link-warn-approx" href="/products/#letter-O" title="Mimir does not include this page, but the link has been rewritten to point to the nearest parent document.">Understanding OpenShift networking</a>
					</li><li class="listitem">
						<a class="link mimir-link-warn-approx" href="/products/#letter-O" title="Mimir does not include this page, but the link has been rewritten to point to the nearest parent document.">OpenShift SDN - CNI network provider</a>
					</li><li class="listitem">
						<a class="link mimir-link-warn-approx" href="/products/#letter-O" title="Mimir does not include this page, but the link has been rewritten to point to the nearest parent document.">OVN-Kubernetes - CNI network provider</a>
					</li><li class="listitem">
						<a class="link mimir-link-warn" href="https://kubernetes.io/docs/concepts/services-networking/dual-stack/#dual-stack-service-configuration-scenarios" title="Mimir does not include content from: kubernetes.io">Dual-stack Service configuration scenarios</a>
					</li><li class="listitem">
						<a class="link mimir-link-warn-approx" href="/products/#letter-O" title="Mimir does not include this page, but the link has been rewritten to point to the nearest parent document.">Installing on bare metal OCP</a>.
					</li><li class="listitem">
						<a class="link mimir-link-warn-approx" href="/products/#letter-O" title="Mimir does not include this page, but the link has been rewritten to point to the nearest parent document.">Cluster Network Operator configuration</a>.
					</li></ul></div></section></section><section class="chapter" id="expanding-the-cluster"><div class="titlepage"><div><div><h1 class="title">Chapter 12. Expanding the cluster</h1></div></div></div><p>
			You can expand a cluster installed with the Assisted Installer by adding hosts using the user interface or the API.
		</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
					<a class="link" href="#api-connectivity-failure_troubleshooting" title="16.5. API connectivity failure when adding nodes to a cluster">API connectivity failure when adding nodes to a cluster</a>
				</li><li class="listitem">
					<a class="link mimir-link-warn" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html-single/postinstallation_configuration/index#configuring-multi-architecture-compute-machines-on-an-openshift-cluster" title="This content is not included in Mimir.">Configuring multi-architecture compute machines on an OpenShift cluster</a>
				</li></ul></div><section class="section" id="checking-multi-architecture-support"><div class="titlepage"><div><div><h2 class="title">12.1. Checking for multi-architecture support</h2></div></div></div><p>
				You must check that your cluster can support multiple architectures before you add a node with a different architecture.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Log in to the cluster using the CLI.
					</li><li class="listitem"><p class="simpara">
						Check that your cluster uses the architecture payload by running the following command:
					</p><pre class="programlisting language-terminal">$ oc adm release info -o json | jq .metadata.metadata</pre></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						If you see the following output, your cluster supports multiple architectures:
					</p><pre class="programlisting language-terminal">{
  "release.openshift.io/architecture": "multi"
}</pre></li></ul></div></section><section class="section" id="installing-mixed-architecture-clusters_expanding-the-cluster"><div class="titlepage"><div><div><h2 class="title">12.2. Installing a multi-architecture cluster</h2></div></div></div><p>
				A cluster with an x86_64 control plane can support worker nodes that have two different CPU architectures. Mixed-architecture clusters combine the strengths of each architecture and support a variety of workloads.
			</p><p>
				For example, you can add arm64, IBM Power®, or IBM Z® worker nodes to an existing OpenShift Container Platform cluster with an x86_64.
			</p><p>
				The main steps of the installation are as follows:
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						Create and register a multi-architecture cluster.
					</li><li class="listitem">
						Create an x86_64 infrastructure environment, download the ISO discovery image for x86_64, and add the control plane. The control plane must have the x86_64 architecture.
					</li><li class="listitem">
						Create an arm64, IBM Power®, or IBM Z® infrastructure environment, download the ISO discovery images for arm64, IBM Power®, or IBM Z®, and add the worker nodes.
					</li></ol></div><div class="formalpara"><p class="title"><strong>Supported platforms</strong></p><p>
					The following table lists the platforms that support a mixed-architecture cluster for each OpenShift Container Platform version. Use the appropriate platforms for the version you are installing.
				</p></div><div class="informaltable"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 14%; " class="col_1"><!--Empty--></col><col style="width: 29%; " class="col_2"><!--Empty--></col><col style="width: 29%; " class="col_3"><!--Empty--></col><col style="width: 29%; " class="col_4"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139800917372816" scope="col">OpenShift Container Platform version</th><th align="left" valign="top" id="idm139800917371712" scope="col">Supported platforms</th><th align="left" valign="top" id="idm139800917370624" scope="col">Day 1 control plane architecture</th><th align="left" valign="top" id="idm139800917369520" scope="col">Day 2 node architecture</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139800917372816"> <p>
								4.12.0
							</p>
							 </td><td align="left" valign="top" headers="idm139800917371712"> <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										Microsoft Azure (TP)
									</li></ul></div>
							 </td><td align="left" valign="top" headers="idm139800917370624"> <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										x86_64
									</li></ul></div>
							 </td><td align="left" valign="top" headers="idm139800917369520"> <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										arm64
									</li></ul></div>
							 </td></tr><tr><td align="left" valign="top" headers="idm139800917372816"> <p>
								4.13.0
							</p>
							 </td><td align="left" valign="top" headers="idm139800917371712"> <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										Microsoft Azure
									</li><li class="listitem">
										Amazon Web Services
									</li><li class="listitem">
										Bare metal (TP)
									</li></ul></div>
							 </td><td align="left" valign="top" headers="idm139800917370624"> <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										x86_64
									</li><li class="listitem">
										x86_64
									</li><li class="listitem">
										x86_64
									</li></ul></div>
							 </td><td align="left" valign="top" headers="idm139800917369520"> <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										arm64
									</li><li class="listitem">
										arm64
									</li><li class="listitem">
										arm64
									</li></ul></div>
							 </td></tr><tr><td align="left" valign="top" headers="idm139800917372816"> <p>
								4.14.0
							</p>
							 </td><td align="left" valign="top" headers="idm139800917371712"> <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										Microsoft Azure
									</li><li class="listitem">
										Amazon Web Services
									</li><li class="listitem">
										Bare metal
									</li><li class="listitem">
										Google Cloud Platform
									</li><li class="listitem">
										IBM Power®
									</li><li class="listitem">
										IBM Z®
									</li></ul></div>
							 </td><td align="left" valign="top" headers="idm139800917370624"> <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										x86_64
									</li><li class="listitem">
										x86_64
									</li><li class="listitem">
										x86_64
									</li><li class="listitem">
										x86_64
									</li><li class="listitem">
										x86_64
									</li><li class="listitem">
										x86_64
									</li></ul></div>
							 </td><td align="left" valign="top" headers="idm139800917369520"> <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										arm64
									</li><li class="listitem">
										arm64
									</li><li class="listitem">
										arm64
									</li><li class="listitem">
										arm64
									</li><li class="listitem">
										ppc64le
									</li><li class="listitem">
										s390x
									</li></ul></div>
							 </td></tr></tbody></table></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					Technology Preview (TP) features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
				</p><p>
					For more information about the support scope of Red Hat Technology Preview features, see <a class="link mimir-link-warn" href="https://access.redhat.com/support/offerings/techpreview" title="This content is not included in Mimir.">Technology Preview Features Support Scope</a>.
				</p></div></div><div class="orderedlist"><p class="title"><strong>Main steps</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Start the procedure for installing OpenShift Container Platform using the API. For details, see <span class="emphasis"><em>Installing with the Assisted Installer API</em></span> in the <span class="emphasis"><em>Additional Resources</em></span> section.
					</li><li class="listitem"><p class="simpara">
						When you reach the "Registering a new cluster" step of the installation, register the cluster as a multi-architecture cluster:
					</p><pre class="programlisting language-terminal">$ curl -s -X POST https://api.openshift.com/api/assisted-install/v2/clusters \
-H "Authorization: Bearer ${API_TOKEN}" \
-H "Content-Type: application/json" \
-d "$(jq --null-input \
   --slurpfile pull_secret ~/Downloads/pull-secret.txt '
{
   "name": "testcluster",
   "openshift_version": "&lt;version-number&gt;-multi", <span id="CO17-1"><!--Empty--></span><span class="callout">1</span>
   "cpu_architecture" : "multi" <span id="CO17-2"><!--Empty--></span><span class="callout">2</span>
   "high_availability_mode": "full" <span id="CO17-3"><!--Empty--></span><span class="callout">3</span>
   "base_dns_domain": "example.com",
   "pull_secret": $pull_secret[0] | tojson
}
')" | jq '.id'</pre><div class="admonition note"><div class="admonition_header">Note</div><div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO17-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Use the <code class="literal">multi-</code> option for the OpenShift Container Platform version number; for example, <code class="literal">"4.12-multi"</code>.
								</div></dd><dt><a href="#CO17-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Set the CPU architecture to <code class="literal">"multi"</code>.
								</div></dd><dt><a href="#CO17-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Use the <code class="literal">full</code> value to indicate Multi-Node OpenShift Container Platform.
								</div></dd></dl></div></div></div></li><li class="listitem"><p class="simpara">
						When you reach the "Registering a new infrastructure environment" step of the installation, set <code class="literal">cpu_architecture</code> to x86_64:
					</p><pre class="programlisting language-terminal">$ curl https://api.openshift.com/api/assisted-install/v2/infra-envs \
-H "Authorization: Bearer ${API_TOKEN}" \
-H "Content-Type: application/json" \
-d "$(jq --null-input \
 --slurpfile pull_secret ~/Downloads/pull-secret.txt \
 --arg cluster_id ${CLUSTER_ID} '
   {
     "name": "testcluster-infra-env",
     "image_type":"full-iso",
     "cluster_id": $cluster_id,
     "cpu_architecture" : "x86_64"
     "pull_secret": $pull_secret[0] | tojson
   }
')" | jq '.id'</pre></li><li class="listitem"><p class="simpara">
						When you reach the "Adding hosts" step of the installation, set <code class="literal">host_role</code> to <code class="literal">master</code>:
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							For more information, see <span class="emphasis"><em>Assigning Roles to Hosts</em></span> in <span class="emphasis"><em>Additional Resources</em></span>.
						</p></div></div><pre class="programlisting language-terminal">$ curl https://api.openshift.com/api/assisted-install/v2/infra-envs/${INFRA_ENV_ID}/hosts/&lt;host_id&gt; \
-X PATCH \
-H "Authorization: Bearer ${API_TOKEN}" \
-H "Content-Type: application/json" \
-d '
   {
     "host_role":"master"
   }
' | jq</pre></li><li class="listitem">
						Download the discovery image for the x86_64 architecture.
					</li><li class="listitem">
						Boot the x86_64 architecture hosts using the generated discovery image.
					</li><li class="listitem">
						Start the installation and wait for the cluster to be fully installed.
					</li><li class="listitem"><p class="simpara">
						Repeat the "Registering a new infrastructure environment" step of the installation. This time, set <code class="literal">cpu_architecture</code> to one of the following: <code class="literal">ppc64le</code> (for IBM Power®), <code class="literal">s390x</code> (for IBM Z®), or <code class="literal">arm64</code>. For example:
					</p><pre class="programlisting language-terminal">$ curl -s -X POST https://api.openshift.com/api/assisted-install/v2/clusters \
-H "Authorization: Bearer ${API_TOKEN}" \
-H "Content-Type: application/json" \
-d "$(jq --null-input \
   --slurpfile pull_secret ~/Downloads/pull-secret.txt '
{
   "name": "testcluster",
   "openshift_version": "4.12",
   "cpu_architecture" : "arm64"
   "high_availability_mode": "full"
   "base_dns_domain": "example.com",
   "pull_secret": $pull_secret[0] | tojson
}
')" | jq '.id'</pre></li><li class="listitem"><p class="simpara">
						Repeat the "Adding hosts" step of the installation. This time, set <code class="literal">host_role</code> to <code class="literal">worker</code>:
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							For more details, see <span class="emphasis"><em>Assigning Roles to Hosts</em></span> in <span class="emphasis"><em>Additional Resources</em></span>.
						</p></div></div><pre class="programlisting language-terminal">$ curl https://api.openshift.com/api/assisted-install/v2/infra-envs/${INFRA_ENV_ID}/hosts/&lt;host_id&gt; \
-X PATCH \
-H "Authorization: Bearer ${API_TOKEN}" \
-H "Content-Type: application/json" \
-d '
   {
     "host_role":"worker"
   }
' | jq</pre></li><li class="listitem">
						Download the discovery image for the arm64, ppc64le or s390x architecture.
					</li><li class="listitem">
						Boot the architecture hosts using the generated discovery image.
					</li><li class="listitem">
						Start the installation and wait for the cluster to be fully installed.
					</li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						View the arm64, ppc64le, or s390x worker nodes in the cluster by running the following command:
					</p><pre class="programlisting language-terminal white-space-pre white-space-pre">$ oc get nodes -o wide</pre></li></ul></div></section><section class="section" id="adding-hosts-with-the-ui_expanding-the-cluster"><div class="titlepage"><div><div><h2 class="title">12.3. Adding hosts with the web console</h2></div></div></div><p>
				You can add hosts to clusters that were created using the <a class="link mimir-link-warn" href="https://console.redhat.com/openshift/assisted-installer/clusters/~new" title="Mimir does not include content from: console.redhat.com">Assisted Installer</a>.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Adding hosts to Assisted Installer clusters is only supported for clusters running OpenShift Container Platform version 4.11 and later.
						</li><li class="listitem">
							When adding a control plane node during Day 2 operations, ensure that the new node shares the same subnet as the Day 1 network. The subnet is specified in the <code class="literal">machineNetwork</code> field of the <code class="literal">install-config.yaml</code> file. This requirement applies to cluster-managed networks such as bare metal or vSphere, and not to user-managed networks.
						</li></ul></div></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Log in to <a class="link mimir-link-warn" href="https://console.redhat.com/openshift" title="Mimir does not include content from: console.redhat.com">OpenShift Cluster Manager</a> and click the cluster that you want to expand.
					</li><li class="listitem">
						Click <span class="strong strong"><strong>Add hosts</strong></span> and download the discovery ISO for the new host, adding an SSH public key and configuring cluster-wide proxy settings as needed.
					</li><li class="listitem">
						Optional: Modify ignition files as needed.
					</li><li class="listitem">
						Boot the target host using the discovery ISO, and wait for the host to be discovered in the console.
					</li><li class="listitem">
						Select the host role. It can be either a <code class="literal">worker</code> or a <code class="literal">control plane</code> host.
					</li><li class="listitem">
						Start the installation.
					</li><li class="listitem"><p class="simpara">
						As the installation proceeds, the installation generates pending certificate signing requests (CSRs) for the host. When prompted, approve the pending CSRs to complete the installation.
					</p><p class="simpara">
						When the host is successfully installed, it is listed as a host in the cluster web console.
					</p></li></ol></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					New hosts will be encrypted using the same method as the original cluster.
				</p></div></div></section><section class="section" id="adding-hosts-with-the-api_expanding-the-cluster"><div class="titlepage"><div><div><h2 class="title">12.4. Adding hosts with the API</h2></div></div></div><p>
				You can add hosts to clusters using the Assisted Installer REST API.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Install the Red Hat OpenShift Cluster Manager CLI (<code class="literal">ocm</code>).
					</li><li class="listitem">
						Log in to <a class="link mimir-link-warn" href="https://console.redhat.com/openshift/assisted-installer/clusters" title="Mimir does not include content from: console.redhat.com">OpenShift Cluster Manager</a> as a user with cluster creation privileges.
					</li><li class="listitem">
						Install <code class="literal">jq</code>.
					</li><li class="listitem">
						Ensure that all the required DNS records exist for the cluster that you want to expand.
					</li></ul></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					When adding a control plane node during Day 2 operations, ensure that the new node shares the same subnet as the Day 1 network. The subnet is specified in the <code class="literal">machineNetwork</code> field of the <code class="literal">install-config.yaml</code> file. This requirement applies to cluster-managed networks such as bare metal or vSphere, and not to user-managed networks.
				</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Authenticate against the Assisted Installer REST API and generate an API token for your session. The generated token is valid for 15 minutes only.
					</li><li class="listitem"><p class="simpara">
						Set the <code class="literal">$API_URL</code> variable by running the following command:
					</p><pre class="programlisting language-terminal">$ export API_URL=&lt;api_url&gt; <span id="CO18-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO18-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Replace <code class="literal">&lt;api_url&gt;</code> with the Assisted Installer API URL, for example, <code class="literal"><a class="link mimir-link-warn" href="https://api.openshift.com" title="Mimir does not include content from: api.openshift.com">https://api.openshift.com</a></code>
							</div></dd></dl></div></li><li class="listitem"><p class="simpara">
						Import the cluster by running the following commands:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Set the <code class="literal">$CLUSTER_ID</code> variable. Log in to the cluster and run the following command:
							</p><pre class="programlisting language-terminal">$ export CLUSTER_ID=$(oc get clusterversion -o jsonpath='{.items[].spec.clusterID}')</pre></li><li class="listitem"><p class="simpara">
								Set the <code class="literal">$CLUSTER_REQUEST</code> variable that is used to import the cluster:
							</p><pre class="programlisting language-terminal">$ export CLUSTER_REQUEST=$(jq --null-input --arg openshift_cluster_id "$CLUSTER_ID" '{
  "api_vip_dnsname": "&lt;api_vip&gt;", <span id="CO19-1"><!--Empty--></span><span class="callout">1</span>
  "openshift_cluster_id": $CLUSTER_ID,
  "name": "&lt;openshift_cluster_name&gt;" <span id="CO19-2"><!--Empty--></span><span class="callout">2</span>
}')</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO19-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Replace <code class="literal">&lt;api_vip&gt;</code> with the hostname for the cluster’s API server. This can be the DNS domain for the API server or the IP address of the single node which the host can reach. For example, <code class="literal">api.compute-1.example.com</code>.
									</div></dd><dt><a href="#CO19-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Replace <code class="literal">&lt;openshift_cluster_name&gt;</code> with the plain text name for the cluster. The cluster name should match the cluster name that was set during the Day 1 cluster installation.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Import the cluster and set the <code class="literal">$CLUSTER_ID</code> variable. Run the following command:
							</p><pre class="programlisting language-terminal">$ CLUSTER_ID=$(curl "$API_URL/api/assisted-install/v2/clusters/import" -H "Authorization: Bearer ${API_TOKEN}" -H 'accept: application/json' -H 'Content-Type: application/json' \
  -d "$CLUSTER_REQUEST" | tee /dev/stderr | jq -r '.id')</pre></li></ol></div></li><li class="listitem"><p class="simpara">
						Generate the <code class="literal">InfraEnv</code> resource for the cluster and set the <code class="literal">$INFRA_ENV_ID</code> variable by running the following commands:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
								Download the pull secret file from Red Hat OpenShift Cluster Manager at <a class="link mimir-link-warn" href="https://console.redhat.com/openshift/downloads#tool-pull-secret" title="Mimir does not include content from: console.redhat.com">console.redhat.com</a>.
							</li><li class="listitem"><p class="simpara">
								Set the <code class="literal">$INFRA_ENV_REQUEST</code> variable:
							</p><pre class="programlisting language-terminal">export INFRA_ENV_REQUEST=$(jq --null-input \
    --slurpfile pull_secret &lt;path_to_pull_secret_file&gt; \<span id="CO20-1"><!--Empty--></span><span class="callout">1</span>
    --arg ssh_pub_key "$(cat &lt;path_to_ssh_pub_key&gt;)" \<span id="CO20-2"><!--Empty--></span><span class="callout">2</span>
    --arg cluster_id "$CLUSTER_ID" '{
  "name": "&lt;infraenv_name&gt;", <span id="CO20-3"><!--Empty--></span><span class="callout">3</span>
  "pull_secret": $pull_secret[0] | tojson,
  "cluster_id": $cluster_id,
  "ssh_authorized_key": $ssh_pub_key,
  "image_type": "&lt;iso_image_type&gt;" <span id="CO20-4"><!--Empty--></span><span class="callout">4</span>
}')</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO20-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Replace <code class="literal">&lt;path_to_pull_secret_file&gt;</code> with the path to the local file containing the downloaded pull secret from Red Hat OpenShift Cluster Manager at <a class="link mimir-link-warn" href="https://console.redhat.com/openshift/downloads#tool-pull-secret" title="Mimir does not include content from: console.redhat.com">console.redhat.com</a>.
									</div></dd><dt><a href="#CO20-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Replace <code class="literal">&lt;path_to_ssh_pub_key&gt;</code> with the path to the public SSH key required to access the host. If you do not set this value, you cannot access the host while in discovery mode.
									</div></dd><dt><a href="#CO20-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										Replace <code class="literal">&lt;infraenv_name&gt;</code> with the plain text name for the <code class="literal">InfraEnv</code> resource.
									</div></dd><dt><a href="#CO20-4"><span class="callout">4</span></a> </dt><dd><div class="para">
										Replace <code class="literal">&lt;iso_image_type&gt;</code> with the ISO image type, either <code class="literal">full-iso</code> or <code class="literal">minimal-iso</code>.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Post the <code class="literal">$INFRA_ENV_REQUEST</code> to the <a class="link mimir-link-warn" href="https://api.openshift.com/?urls.primaryName=assisted-service%20service#/installer/RegisterInfraEnv" title="Mimir does not include content from: api.openshift.com">/v2/infra-envs</a> API and set the <code class="literal">$INFRA_ENV_ID</code> variable:
							</p><pre class="programlisting language-terminal">$ INFRA_ENV_ID=$(curl "$API_URL/api/assisted-install/v2/infra-envs" -H "Authorization: Bearer ${API_TOKEN}" -H 'accept: application/json' -H 'Content-Type: application/json' -d "$INFRA_ENV_REQUEST" | tee /dev/stderr | jq -r '.id')</pre></li></ol></div></li><li class="listitem"><p class="simpara">
						Get the URL of the discovery ISO for the cluster host by running the following command:
					</p><pre class="programlisting language-terminal">$ curl -s "$API_URL/api/assisted-install/v2/infra-envs/$INFRA_ENV_ID" -H "Authorization: Bearer ${API_TOKEN}" | jq -r '.download_url'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">https://api.openshift.com/api/assisted-images/images/41b91e72-c33e-42ee-b80f-b5c5bbf6431a?arch=x86_64&amp;image_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE2NTYwMjYzNzEsInN1YiI6IjQxYjkxZTcyLWMzM2UtNDJlZS1iODBmLWI1YzViYmY2NDMxYSJ9.1EX_VGaMNejMhrAvVRBS7PDPIQtbOOc8LtG8OukE1a4&amp;type=minimal-iso&amp;version=4.12</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Download the ISO:
					</p><pre class="programlisting language-terminal">$ curl -L -s '&lt;iso_url&gt;' --output rhcos-live-minimal.iso <span id="CO21-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO21-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Replace <code class="literal">&lt;iso_url&gt;</code> with the URL for the ISO from the previous step.
							</div></dd></dl></div></li><li class="listitem">
						Boot the new worker host from the downloaded <code class="literal">rhcos-live-minimal.iso</code>.
					</li><li class="listitem"><p class="simpara">
						Get the list of hosts in the cluster that are <span class="emphasis"><em>not</em></span> installed. Keep running the following command until the new host shows up:
					</p><pre class="programlisting language-terminal">$ curl -s "$API_URL/api/assisted-install/v2/clusters/$CLUSTER_ID" -H "Authorization: Bearer ${API_TOKEN}" | jq -r '.hosts[] | select(.status != "installed").id'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">2294ba03-c264-4f11-ac08-2f1bb2f8c296</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Set the <code class="literal">$HOST_ID</code> variable for the new host, for example:
					</p><pre class="programlisting language-terminal">$ HOST_ID=&lt;host_id&gt; <span id="CO22-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO22-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Replace <code class="literal">&lt;host_id&gt;</code> with the host ID from the previous step.
							</div></dd></dl></div></li><li class="listitem"><p class="simpara">
						Check that the host is ready to install by running the following command:
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							Ensure that you copy the entire command including the complete <code class="literal">jq</code> expression.
						</p></div></div><pre class="programlisting language-terminal">$ curl -s $API_URL/api/assisted-install/v2/clusters/$CLUSTER_ID -H "Authorization: Bearer ${API_TOKEN}" | jq '
def host_name($host):
    if (.suggested_hostname // "") == "" then
        if (.inventory // "") == "" then
            "Unknown hostname, please wait"
        else
            .inventory | fromjson | .hostname
        end
    else
        .suggested_hostname
    end;

def is_notable($validation):
    ["failure", "pending", "error"] | any(. == $validation.status);

def notable_validations($validations_info):
    [
        $validations_info // "{}"
        | fromjson
        | to_entries[].value[]
        | select(is_notable(.))
    ];

{
    "Hosts validations": {
        "Hosts": [
            .hosts[]
            | select(.status != "installed")
            | {
                "id": .id,
                "name": host_name(.),
                "status": .status,
                "notable_validations": notable_validations(.validations_info)
            }
        ]
    },
    "Cluster validations info": {
        "notable_validations": notable_validations(.validations_info)
    }
}
' -r</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">{
  "Hosts validations": {
    "Hosts": [
      {
        "id": "97ec378c-3568-460c-bc22-df54534ff08f",
        "name": "localhost.localdomain",
        "status": "insufficient",
        "notable_validations": [
          {
            "id": "ntp-synced",
            "status": "failure",
            "message": "Host couldn't synchronize with any NTP server"
          },
          {
            "id": "api-domain-name-resolved-correctly",
            "status": "error",
            "message": "Parse error for domain name resolutions result"
          },
          {
            "id": "api-int-domain-name-resolved-correctly",
            "status": "error",
            "message": "Parse error for domain name resolutions result"
          },
          {
            "id": "apps-domain-name-resolved-correctly",
            "status": "error",
            "message": "Parse error for domain name resolutions result"
          }
        ]
      }
    ]
  },
  "Cluster validations info": {
    "notable_validations": []
  }
}</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						When the previous command shows that the host is ready, start the installation using the <a class="link mimir-link-warn" href="https://api.openshift.com/?urls.primaryName=assisted-service%20service#/installer/v2InstallHost" title="Mimir does not include content from: api.openshift.com">/v2/infra-envs/{infra_env_id}/hosts/{host_id}/actions/install</a> API by running the following command:
					</p><pre class="programlisting language-terminal">$ curl -X POST -s "$API_URL/api/assisted-install/v2/infra-envs/$INFRA_ENV_ID/hosts/$HOST_ID/actions/install"  -H "Authorization: Bearer ${API_TOKEN}"</pre></li><li class="listitem"><p class="simpara">
						As the installation proceeds, the installation generates pending certificate signing requests (CSRs) for the host.
					</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							You must approve the CSRs to complete the installation.
						</p></div></div><p class="simpara">
						Keep running the following API call to monitor the cluster installation:
					</p><pre class="programlisting language-terminal">$ curl -s "$API_URL/api/assisted-install/v2/clusters/$CLUSTER_ID" -H "Authorization: Bearer ${API_TOKEN}" | jq '{
    "Cluster day-2 hosts":
        [
            .hosts[]
            | select(.status != "installed")
            | {id, requested_hostname, status, status_info, progress, status_updated_at, updated_at, infra_env_id, cluster_id, created_at}
        ]
}'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">{
  "Cluster day-2 hosts": [
    {
      "id": "a1c52dde-3432-4f59-b2ae-0a530c851480",
      "requested_hostname": "control-plane-1",
      "status": "added-to-existing-cluster",
      "status_info": "Host has rebooted and no further updates will be posted. Please check console for progress and to possibly approve pending CSRs",
      "progress": {
        "current_stage": "Done",
        "installation_percentage": 100,
        "stage_started_at": "2022-07-08T10:56:20.476Z",
        "stage_updated_at": "2022-07-08T10:56:20.476Z"
      },
      "status_updated_at": "2022-07-08T10:56:20.476Z",
      "updated_at": "2022-07-08T10:57:15.306369Z",
      "infra_env_id": "b74ec0c3-d5b5-4717-a866-5b6854791bd3",
      "cluster_id": "8f721322-419d-4eed-aa5b-61b50ea586ae",
      "created_at": "2022-07-06T22:54:57.161614Z"
    }
  ]
}</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Optional: Run the following command to see all the events for the cluster:
					</p><pre class="programlisting language-terminal">$ curl -s "$API_URL/api/assisted-install/v2/events?cluster_id=$CLUSTER_ID" -H "Authorization: Bearer ${API_TOKEN}" | jq -c '.[] | {severity, message, event_time, host_id}'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">{"severity":"info","message":"Host compute-0: updated status from insufficient to known (Host is ready to be installed)","event_time":"2022-07-08T11:21:46.346Z","host_id":"9d7b3b44-1125-4ad0-9b14-76550087b445"}
{"severity":"info","message":"Host compute-0: updated status from known to installing (Installation is in progress)","event_time":"2022-07-08T11:28:28.647Z","host_id":"9d7b3b44-1125-4ad0-9b14-76550087b445"}
{"severity":"info","message":"Host compute-0: updated status from installing to installing-in-progress (Starting installation)","event_time":"2022-07-08T11:28:52.068Z","host_id":"9d7b3b44-1125-4ad0-9b14-76550087b445"}
{"severity":"info","message":"Uploaded logs for host compute-0 cluster 8f721322-419d-4eed-aa5b-61b50ea586ae","event_time":"2022-07-08T11:29:47.802Z","host_id":"9d7b3b44-1125-4ad0-9b14-76550087b445"}
{"severity":"info","message":"Host compute-0: updated status from installing-in-progress to added-to-existing-cluster (Host has rebooted and no further updates will be posted. Please check console for progress and to possibly approve pending CSRs)","event_time":"2022-07-08T11:29:48.259Z","host_id":"9d7b3b44-1125-4ad0-9b14-76550087b445"}
{"severity":"info","message":"Host: compute-0, reached installation stage Rebooting","event_time":"2022-07-08T11:29:48.261Z","host_id":"9d7b3b44-1125-4ad0-9b14-76550087b445"}</pre>

						</p></div></li><li class="listitem">
						Log in to the cluster and approve the pending CSRs to complete the installation.
					</li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						Check that the new host was successfully added to the cluster with a status of <code class="literal">Ready</code>:
					</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME                           STATUS   ROLES           AGE   VERSION
control-plane-1.example.com    Ready    master,worker   56m   v1.25.0
compute-1.example.com          Ready    worker          11m   v1.25.0</pre>

						</p></div></li></ul></div></section><section class="section" id="installing-primary-control-plane-node-healthy-cluster_expanding-the-cluster"><div class="titlepage"><div><div><h2 class="title">12.5. Installing a primary control plane node on a healthy cluster</h2></div></div></div><p>
				This procedure describes how to install a primary control plane node on a healthy OpenShift Container Platform cluster.
			</p><p>
				If the cluster is unhealthy, additional operations are required before they can be managed. See <span class="emphasis"><em>Additional Resources</em></span> for more information.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You are using OpenShift Container Platform 4.11 or later with the correct <code class="literal">etcd-operator</code> version.
					</li><li class="listitem">
						You have installed a healthy cluster with a minimum of three nodes.
					</li><li class="listitem">
						You have created a single control plane node.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Retrieve pending <code class="literal">CertificateSigningRequests</code> (CSRs):
					</p><pre class="programlisting language-terminal">$ oc get csr | grep Pending</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">csr-5sd59   8m19s   kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   &lt;none&gt;              Pending
csr-xzqts   10s     kubernetes.io/kubelet-serving                 system:node:worker-6                                                   &lt;none&gt;              Pending</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Approve pending CSRs:
					</p><pre class="programlisting language-terminal">$ oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{"\n"}}{{end}}{{end}}' | xargs --no-run-if-empty oc adm certificate approve</pre><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							You must approve the CSRs to complete the installation.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Confirm the primary node is in <code class="literal">Ready</code> status:
					</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME       STATUS   ROLES    AGE     VERSION
master-0   Ready    master   4h42m   v1.24.0+3882f8f
worker-1   Ready    worker   4h29m   v1.24.0+3882f8f
master-2   Ready    master   4h43m   v1.24.0+3882f8f
master-3   Ready    master   4h27m   v1.24.0+3882f8f
worker-4   Ready    worker   4h30m   v1.24.0+3882f8f
master-5   Ready    master   105s    v1.24.0+3882f8f</pre>

						</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The <code class="literal">etcd-operator</code> requires a <code class="literal">Machine</code> Custom Resource (CR) referencing the new node when the cluster runs with a functional Machine API.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Link the <code class="literal">Machine</code> CR with <code class="literal">BareMetalHost</code> and <code class="literal">Node</code>:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Create the <code class="literal">BareMetalHost</code> CR with a unique <code class="literal">.metadata.name</code> value:
							</p><pre class="programlisting language-terminal">apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: custom-master3
  namespace: openshift-machine-api
  annotations:
spec:
  automatedCleaningMode: metadata
  bootMACAddress: 00:00:00:00:00:02
  bootMode: UEFI
  customDeploy:
    method: install_coreos
  externallyProvisioned: true
  online: true
  userData:
    name: master-user-data-managed
    namespace: openshift-machine-api</pre><pre class="programlisting language-terminal">$ oc create -f &lt;filename&gt;</pre></li><li class="listitem"><p class="simpara">
								Apply the <code class="literal">BareMetalHost</code> CR:
							</p><pre class="programlisting language-terminal">$ oc apply -f &lt;filename&gt;</pre></li><li class="listitem"><p class="simpara">
								Create the <code class="literal">Machine</code> CR using the unique <code class="literal">.machine.name</code> value:
							</p><pre class="programlisting language-terminal">apiVersion: machine.openshift.io/v1beta1
kind: Machine
metadata:
  annotations:
    machine.openshift.io/instance-state: externally provisioned
    metal3.io/BareMetalHost: openshift-machine-api/custom-master3
  finalizers:
  - machine.machine.openshift.io
  generation: 3
  labels:
    machine.openshift.io/cluster-api-cluster: test-day2-1-6qv96
    machine.openshift.io/cluster-api-machine-role: master
    machine.openshift.io/cluster-api-machine-type: master
  name: custom-master3
  namespace: openshift-machine-api
spec:
  metadata: {}
  providerSpec:
    value:
      apiVersion: baremetal.cluster.k8s.io/v1alpha1
      customDeploy:
        method: install_coreos
      hostSelector: {}
      image:
        checksum: ""
        url: ""
      kind: BareMetalMachineProviderSpec
      metadata:
        creationTimestamp: null
      userData:
        name: master-user-data-managed</pre><pre class="programlisting language-terminal">$ oc create -f &lt;filename&gt;</pre></li><li class="listitem"><p class="simpara">
								Apply the <code class="literal">Machine</code> CR:
							</p><pre class="programlisting language-terminal">$ oc apply -f &lt;filename&gt;</pre></li><li class="listitem"><p class="simpara">
								Link <code class="literal">BareMetalHost</code>, <code class="literal">Machine</code>, and <code class="literal">Node</code> using the <code class="literal">link-machine-and-node.sh</code> script:
							</p><pre class="programlisting language-terminal">#!/bin/bash

# Credit goes to https://bugzilla.redhat.com/show_bug.cgi?id=1801238.
# This script will link Machine object and Node object. This is needed
# in order to have IP address of the Node present in the status of the Machine.

# set -x
set -e

machine="$1"
node="$2"

if [ -z "$machine" ] || [ -z "$node" ]; then
    echo "Usage: $0 MACHINE NODE"
    exit 1
fi

# uid=$(echo "${node}" | cut -f1 -d':')
node_name=$(echo "${node}" | cut -f2 -d':')

oc proxy &amp;
proxy_pid=$!
function kill_proxy {
    kill $proxy_pid
}
trap kill_proxy EXIT SIGINT

HOST_PROXY_API_PATH="http://localhost:8001/apis/metal3.io/v1alpha1/namespaces/openshift-machine-api/baremetalhosts"

function print_nics() {
    local ips
    local eob
    declare -a ips

    readarray -t ips &lt; &lt;(echo "${1}" \
                         | jq '.[] | select(. | .type == "InternalIP") | .address' \
                         | sed 's/"//g')

    eob=','
    for (( i=0; i&lt;${#ips[@]}; i++ )); do
        if [ $((i+1)) -eq ${#ips[@]} ]; then
            eob=""
        fi
        cat &lt;&lt;- EOF
          {
            "ip": "${ips[$i]}",
            "mac": "00:00:00:00:00:00",
            "model": "unknown",
            "speedGbps": 10,
            "vlanId": 0,
            "pxe": true,
            "name": "eth1"
          }${eob}
EOF
    done
}

function wait_for_json() {
    local name
    local url
    local curl_opts
    local timeout

    local start_time
    local curr_time
    local time_diff

    name="$1"
    url="$2"
    timeout="$3"
    shift 3
    curl_opts="$@"
    echo -n "Waiting for $name to respond"
    start_time=$(date +%s)
    until curl -g -X GET "$url" "${curl_opts[@]}" 2&gt; /dev/null | jq '.' 2&gt; /dev/null &gt; /dev/null; do
        echo -n "."
        curr_time=$(date +%s)
        time_diff=$((curr_time - start_time))
        if [[ $time_diff -gt $timeout ]]; then
            printf '\nTimed out waiting for %s' "${name}"
            return 1
        fi
        sleep 5
    done
    echo " Success!"
    return 0
}
wait_for_json oc_proxy "${HOST_PROXY_API_PATH}" 10 -H "Accept: application/json" -H "Content-Type: application/json"

addresses=$(oc get node -n openshift-machine-api "${node_name}" -o json | jq -c '.status.addresses')

machine_data=$(oc get machines.machine.openshift.io -n openshift-machine-api -o json "${machine}")
host=$(echo "$machine_data" | jq '.metadata.annotations["metal3.io/BareMetalHost"]' | cut -f2 -d/ | sed 's/"//g')

if [ -z "$host" ]; then
    echo "Machine $machine is not linked to a host yet." 1&gt;&amp;2
    exit 1
fi

# The address structure on the host doesn't match the node, so extract
# the values we want into separate variables so we can build the patch
# we need.
hostname=$(echo "${addresses}" | jq '.[] | select(. | .type == "Hostname") | .address' | sed 's/"//g')

set +e
read -r -d '' host_patch &lt;&lt; EOF
{
  "status": {
    "hardware": {
      "hostname": "${hostname}",
      "nics": [
$(print_nics "${addresses}")
      ],
      "systemVendor": {
        "manufacturer": "Red Hat",
        "productName": "product name",
        "serialNumber": ""
      },
      "firmware": {
        "bios": {
          "date": "04/01/2014",
          "vendor": "SeaBIOS",
          "version": "1.11.0-2.el7"
        }
      },
      "ramMebibytes": 0,
      "storage": [],
      "cpu": {
        "arch": "x86_64",
        "model": "Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz",
        "clockMegahertz": 2199.998,
        "count": 4,
        "flags": []
      }
    }
  }
}
EOF
set -e

echo "PATCHING HOST"
echo "${host_patch}" | jq .

curl -s \
     -X PATCH \
     "${HOST_PROXY_API_PATH}/${host}/status" \
     -H "Content-type: application/merge-patch+json" \
     -d "${host_patch}"

oc get baremetalhost -n openshift-machine-api -o yaml "${host}"</pre><pre class="programlisting language-terminal">$ bash link-machine-and-node.sh custom-master3 worker-5</pre></li></ol></div></li><li class="listitem"><p class="simpara">
						Confirm <code class="literal">etcd</code> members:
					</p><pre class="programlisting language-terminal">$ oc rsh -n openshift-etcd etcd-worker-2
etcdctl member list -w table</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">+--------+---------+--------+--------------+--------------+---------+
|   ID   |  STATUS |  NAME  |  PEER ADDRS  | CLIENT ADDRS | LEARNER |
+--------+---------+--------+--------------+--------------+---------+
|2c18942f| started |worker-3|192.168.111.26|192.168.111.26|  false  |
|61e2a860| started |worker-2|192.168.111.25|192.168.111.25|  false  |
|ead4f280| started |worker-5|192.168.111.28|192.168.111.28|  false  |
+--------+---------+--------+--------------+--------------+---------+</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Confirm the <code class="literal">etcd-operator</code> configuration applies to all nodes:
					</p><pre class="programlisting language-terminal">$ oc get clusteroperator etcd</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME   VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE
etcd   4.11.5    True        False         False      5h54m</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Confirm <code class="literal">etcd-operator</code> health:
					</p><pre class="programlisting language-terminal">$ oc rsh -n openshift-etcd etcd-worker-0
etcdctl endpoint health</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">192.168.111.26 is healthy: committed proposal: took = 11.297561ms
192.168.111.25 is healthy: committed proposal: took = 13.892416ms
192.168.111.28 is healthy: committed proposal: took = 11.870755ms</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Confirm node health:
					</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME       STATUS   ROLES    AGE     VERSION
master-0   Ready    master   6h20m   v1.24.0+3882f8f
worker-1   Ready    worker   6h7m    v1.24.0+3882f8f
master-2   Ready    master   6h20m   v1.24.0+3882f8f
master-3   Ready    master   6h4m    v1.24.0+3882f8f
worker-4   Ready    worker   6h7m    v1.24.0+3882f8f
master-5   Ready    master   99m     v1.24.0+3882f8f</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Confirm the <code class="literal">ClusterOperators</code> health:
					</p><pre class="programlisting language-terminal">$ oc get ClusterOperators</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME                                      VERSION AVAILABLE PROGRESSING DEGRADED SINCE MSG
authentication                            4.11.5  True      False       False    5h57m
baremetal                                 4.11.5  True      False       False    6h19m
cloud-controller-manager                  4.11.5  True      False       False    6h20m
cloud-credential                          4.11.5  True      False       False    6h23m
cluster-autoscaler                        4.11.5  True      False       False    6h18m
config-operator                           4.11.5  True      False       False    6h19m
console                                   4.11.5  True      False       False    6h4m
csi-snapshot-controller                   4.11.5  True      False       False    6h19m
dns                                       4.11.5  True      False       False    6h18m
etcd                                      4.11.5  True      False       False    6h17m
image-registry                            4.11.5  True      False       False    6h7m
ingress                                   4.11.5  True      False       False    6h6m
insights                                  4.11.5  True      False       False    6h12m
kube-apiserver                            4.11.5  True      False       False    6h16m
kube-controller-manager                   4.11.5  True      False       False    6h16m
kube-scheduler                            4.11.5  True      False       False    6h16m
kube-storage-version-migrator             4.11.5  True      False       False    6h19m
machine-api                               4.11.5  True      False       False    6h15m
machine-approver                          4.11.5  True      False       False    6h19m
machine-config                            4.11.5  True      False       False    6h18m
marketplace                               4.11.5  True      False       False    6h18m
monitoring                                4.11.5  True      False       False    6h4m
network                                   4.11.5  True      False       False    6h20m
node-tuning                               4.11.5  True      False       False    6h18m
openshift-apiserver                       4.11.5  True      False       False    6h8m
openshift-controller-manager              4.11.5  True      False       False    6h7m
openshift-samples                         4.11.5  True      False       False    6h12m
operator-lifecycle-manager                4.11.5  True      False       False    6h18m
operator-lifecycle-manager-catalog        4.11.5  True      False       False    6h19m
operator-lifecycle-manager-pkgsvr         4.11.5  True      False       False    6h12m
service-ca                                4.11.5  True      False       False    6h19m
storage                                   4.11.5  True      False       False    6h19m</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Confirm the <code class="literal">ClusterVersion</code>:
					</p><pre class="programlisting language-terminal">$ oc get ClusterVersion</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME      VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS
version   4.11.5    True        False         5h57m   Cluster version is 4.11.5</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Remove the old control plane node:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Delete the <code class="literal">BareMetalHost</code> CR:
							</p><pre class="programlisting language-terminal">$ oc delete bmh -n openshift-machine-api custom-master3</pre></li><li class="listitem"><p class="simpara">
								Confirm the <code class="literal">Machine</code> is unhealthy:
							</p><pre class="programlisting language-terminal">$ oc get machine -A</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">NAMESPACE              NAME                              PHASE    AGE
openshift-machine-api  custom-master3                    Running  14h
openshift-machine-api  test-day2-1-6qv96-master-0        Failed   20h
openshift-machine-api  test-day2-1-6qv96-master-1        Running  20h
openshift-machine-api  test-day2-1-6qv96-master-2        Running  20h
openshift-machine-api  test-day2-1-6qv96-worker-0-8w7vr  Running  19h
openshift-machine-api  test-day2-1-6qv96-worker-0-rxddj  Running  19h</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								Delete the <code class="literal">Machine</code> CR:
							</p><pre class="programlisting language-terminal">$ oc delete machine -n openshift-machine-api   test-day2-1-6qv96-master-0
machine.machine.openshift.io "test-day2-1-6qv96-master-0" deleted</pre></li><li class="listitem"><p class="simpara">
								Confirm removal of the <code class="literal">Node</code> CR:
							</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">NAME       STATUS   ROLES    AGE   VERSION
worker-1   Ready    worker   19h   v1.24.0+3882f8f
master-2   Ready    master   20h   v1.24.0+3882f8f
master-3   Ready    master   19h   v1.24.0+3882f8f
worker-4   Ready    worker   19h   v1.24.0+3882f8f
master-5   Ready    master   15h   v1.24.0+3882f8f</pre>

								</p></div></li></ol></div></li><li class="listitem"><p class="simpara">
						Check <code class="literal">etcd-operator</code> logs to confirm status of the <code class="literal">etcd</code> cluster:
					</p><pre class="programlisting language-terminal">$ oc logs -n openshift-etcd-operator etcd-operator-8668df65d-lvpjf</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">E0927 07:53:10.597523       1 base_controller.go:272] ClusterMemberRemovalController reconciliation failed: cannot remove member: 192.168.111.23 because it is reported as healthy but it doesn't have a machine nor a node resource</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Remove the physical machine to allow <code class="literal">etcd-operator</code> to reconcile the cluster members:
					</p><pre class="programlisting language-terminal">$ oc rsh -n openshift-etcd etcd-worker-2
etcdctl member list -w table; etcdctl endpoint health</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">+--------+---------+--------+--------------+--------------+---------+
|   ID   |  STATUS |  NAME  |  PEER ADDRS  | CLIENT ADDRS | LEARNER |
+--------+---------+--------+--------------+--------------+---------+
|2c18942f| started |worker-3|192.168.111.26|192.168.111.26|  false  |
|61e2a860| started |worker-2|192.168.111.25|192.168.111.25|  false  |
|ead4f280| started |worker-5|192.168.111.28|192.168.111.28|  false  |
+--------+---------+--------+--------------+--------------+---------+
192.168.111.26 is healthy: committed proposal: took = 10.458132ms
192.168.111.25 is healthy: committed proposal: took = 11.047349ms
192.168.111.28 is healthy: committed proposal: took = 11.414402ms</pre>

						</p></div></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						<a class="link" href="#installing-primary-control-plane-node-unhealthy-cluster_expanding-the-cluster" title="12.6. Installing a primary control plane node on an unhealthy cluster">Installing a primary control plane node on an unhealthy cluster</a>
					</li></ul></div></section><section class="section" id="installing-primary-control-plane-node-unhealthy-cluster_expanding-the-cluster"><div class="titlepage"><div><div><h2 class="title">12.6. Installing a primary control plane node on an unhealthy cluster</h2></div></div></div><p>
				This procedure describes how to install a primary control plane node on an unhealthy OpenShift Container Platform cluster.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have installed a healthy cluster with a minimum of three nodes.
					</li><li class="listitem">
						You have created the Day 2 control plane.
					</li><li class="listitem">
						You have created a single control plane node.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Confirm initial state of the cluster:
					</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME       STATUS     ROLES    AGE   VERSION
worker-1   Ready      worker   20h   v1.24.0+3882f8f
master-2   NotReady   master   20h   v1.24.0+3882f8f
master-3   Ready      master   20h   v1.24.0+3882f8f
worker-4   Ready      worker   20h   v1.24.0+3882f8f
master-5   Ready      master   15h   v1.24.0+3882f8f</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Confirm the <code class="literal">etcd-operator</code> detects the cluster as unhealthy:
					</p><pre class="programlisting language-terminal">$ oc logs -n openshift-etcd-operator etcd-operator-8668df65d-lvpjf</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">E0927 08:24:23.983733       1 base_controller.go:272] DefragController reconciliation failed: cluster is unhealthy: 2 of 3 members are available, worker-2 is unhealthy</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Confirm the <code class="literal">etcdctl</code> members:
					</p><pre class="programlisting language-terminal">$ oc rsh -n openshift-etcd etcd-worker-3
etcdctl member list -w table</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">+--------+---------+--------+--------------+--------------+---------+
|   ID   | STATUS  |  NAME  |  PEER ADDRS  | CLIENT ADDRS | LEARNER |
+--------+---------+--------+--------------+--------------+---------+
|2c18942f| started |worker-3|192.168.111.26|192.168.111.26|  false  |
|61e2a860| started |worker-2|192.168.111.25|192.168.111.25|  false  |
|ead4f280| started |worker-5|192.168.111.28|192.168.111.28|  false  |
+--------+---------+--------+--------------+--------------+---------+</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Confirm that <code class="literal">etcdctl</code> reports an unhealthy member of the cluster:
					</p><pre class="programlisting language-terminal">$ etcdctl endpoint health</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">{"level":"warn","ts":"2022-09-27T08:25:35.953Z","logger":"client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc000680380/192.168.111.25","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = latest balancer error: last connection error: connection error: desc = \"transport: Error while dialing dial tcp 192.168.111.25: connect: no route to host\""}
192.168.111.28 is healthy: committed proposal: took = 12.465641ms
192.168.111.26 is healthy: committed proposal: took = 12.297059ms
192.168.111.25 is unhealthy: failed to commit proposal: context deadline exceeded
Error: unhealthy cluster</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Remove the unhealthy control plane by deleting the <code class="literal">Machine</code> Custom Resource:
					</p><pre class="programlisting language-terminal">$ oc delete machine -n openshift-machine-api test-day2-1-6qv96-master-2</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The <code class="literal">Machine</code> and <code class="literal">Node</code> Custom Resources (CRs) will not be deleted if the unhealthy cluster cannot run successfully.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Confirm that <code class="literal">etcd-operator</code> has not removed the unhealthy machine:
					</p><pre class="programlisting language-terminal">$ oc logs -n openshift-etcd-operator etcd-operator-8668df65d-lvpjf -f</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">I0927 08:58:41.249222       1 machinedeletionhooks.go:135] skip removing the deletion hook from machine test-day2-1-6qv96-master-2 since its member is still present with any of: [{InternalIP } {InternalIP 192.168.111.26}]</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Remove the unhealthy <code class="literal">etcdctl</code> member manually:
					</p><pre class="programlisting language-terminal">$ oc rsh -n openshift-etcd etcd-worker-3\
etcdctl member list -w table</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">+--------+---------+--------+--------------+--------------+---------+
|   ID   |  STATUS |  NAME  |  PEER ADDRS  | CLIENT ADDRS | LEARNER |
+--------+---------+--------+--------------+--------------+---------+
|2c18942f| started |worker-3|192.168.111.26|192.168.111.26|  false  |
|61e2a860| started |worker-2|192.168.111.25|192.168.111.25|  false  |
|ead4f280| started |worker-5|192.168.111.28|192.168.111.28|  false  |
+--------+---------+--------+--------------+--------------+---------+</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Confirm that <code class="literal">etcdctl</code> reports an unhealthy member of the cluster:
					</p><pre class="programlisting language-terminal">$ etcdctl endpoint health</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">{"level":"warn","ts":"2022-09-27T10:31:07.227Z","logger":"client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0000d6e00/192.168.111.25","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = latest balancer error: last connection error: connection error: desc = \"transport: Error while dialing dial tcp 192.168.111.25: connect: no route to host\""}
192.168.111.28 is healthy: committed proposal: took = 13.038278ms
192.168.111.26 is healthy: committed proposal: took = 12.950355ms
192.168.111.25 is unhealthy: failed to commit proposal: context deadline exceeded
Error: unhealthy cluster</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Remove the unhealthy cluster by deleting the <code class="literal">etcdctl</code> member Custom Resource:
					</p><pre class="programlisting language-terminal">$ etcdctl member remove 61e2a86084aafa62</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">Member 61e2a86084aafa62 removed from cluster 6881c977b97990d7</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Confirm members of <code class="literal">etcdctl</code> by running the following command:
					</p><pre class="programlisting language-terminal">$ etcdctl member list -w table</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">+----------+---------+--------+--------------+--------------+-------+
|    ID    | STATUS  |  NAME  |  PEER ADDRS  | CLIENT ADDRS |LEARNER|
+----------+---------+--------+--------------+--------------+-------+
| 2c18942f | started |worker-3|192.168.111.26|192.168.111.26| false |
| ead4f280 | started |worker-5|192.168.111.28|192.168.111.28| false |
+----------+---------+--------+--------------+--------------+-------+</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Review and approve Certificate Signing Requests
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Review the Certificate Signing Requests (CSRs):
							</p><pre class="programlisting language-terminal">$ oc get csr | grep Pending</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">csr-5sd59   8m19s   kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   &lt;none&gt;              Pending
csr-xzqts   10s     kubernetes.io/kubelet-serving                 system:node:worker-6                                                   &lt;none&gt;              Pending</pre>

								</p></div></li><li class="listitem"><p class="simpara">
								Approve all pending CSRs:
							</p><pre class="programlisting language-terminal">$ oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{"\n"}}{{end}}{{end}}' | xargs --no-run-if-empty oc adm certificate approve</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									You must approve the CSRs to complete the installation.
								</p></div></div></li></ol></div></li><li class="listitem"><p class="simpara">
						Confirm ready status of the control plane node:
					</p><pre class="programlisting language-terminal">$ oc get nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME       STATUS   ROLES    AGE     VERSION
worker-1   Ready    worker   22h     v1.24.0+3882f8f
master-3   Ready    master   22h     v1.24.0+3882f8f
worker-4   Ready    worker   22h     v1.24.0+3882f8f
master-5   Ready    master   17h     v1.24.0+3882f8f
master-6   Ready    master   2m52s   v1.24.0+3882f8f</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Validate the <code class="literal">Machine</code>, <code class="literal">Node</code> and <code class="literal">BareMetalHost</code> Custom Resources.
					</p><p class="simpara">
						The <code class="literal">etcd-operator</code> requires <code class="literal">Machine</code> CRs to be present if the cluster is running with the functional Machine API. <code class="literal">Machine</code> CRs are displayed during the <code class="literal">Running</code> phase when present.
					</p></li><li class="listitem"><p class="simpara">
						Create <code class="literal">Machine</code> Custom Resource linked with <code class="literal">BareMetalHost</code> and <code class="literal">Node</code>.
					</p><p class="simpara">
						Make sure there is a <code class="literal">Machine</code> CR referencing the newly added node.
					</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							Boot-it-yourself will not create <code class="literal">BareMetalHost</code> and <code class="literal">Machine</code> CRs, so you must create them. Failure to create the <code class="literal">BareMetalHost</code> and <code class="literal">Machine</code> CRs will generate errors when running <code class="literal">etcd-operator</code>.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Add <code class="literal">BareMetalHost</code> Custom Resource:
					</p><pre class="programlisting language-terminal">$ oc create bmh -n openshift-machine-api custom-master3</pre></li><li class="listitem"><p class="simpara">
						Add <code class="literal">Machine</code> Custom Resource:
					</p><pre class="programlisting language-terminal">$ oc create machine -n openshift-machine-api custom-master3</pre></li><li class="listitem"><p class="simpara">
						Link <code class="literal">BareMetalHost</code>, <code class="literal">Machine</code>, and <code class="literal">Node</code> by running the <code class="literal">link-machine-and-node.sh</code> script:
					</p><pre class="programlisting language-terminal">#!/bin/bash

# Credit goes to https://bugzilla.redhat.com/show_bug.cgi?id=1801238.
# This script will link Machine object and Node object. This is needed
# in order to have IP address of the Node present in the status of the Machine.

# set -x
set -e

machine="$1"
node="$2"

if [ -z "$machine" ] || [ -z "$node" ]; then
    echo "Usage: $0 MACHINE NODE"
    exit 1
fi

# uid=$(echo "${node}" | cut -f1 -d':')
node_name=$(echo "${node}" | cut -f2 -d':')

oc proxy &amp;
proxy_pid=$!
function kill_proxy {
    kill $proxy_pid
}
trap kill_proxy EXIT SIGINT

HOST_PROXY_API_PATH="http://localhost:8001/apis/metal3.io/v1alpha1/namespaces/openshift-machine-api/baremetalhosts"

function print_nics() {
    local ips
    local eob
    declare -a ips

    readarray -t ips &lt; &lt;(echo "${1}" \
                         | jq '.[] | select(. | .type == "InternalIP") | .address' \
                         | sed 's/"//g')

    eob=','
    for (( i=0; i&lt;${#ips[@]}; i++ )); do
        if [ $((i+1)) -eq ${#ips[@]} ]; then
            eob=""
        fi
        cat &lt;&lt;- EOF
          {
            "ip": "${ips[$i]}",
            "mac": "00:00:00:00:00:00",
            "model": "unknown",
            "speedGbps": 10,
            "vlanId": 0,
            "pxe": true,
            "name": "eth1"
          }${eob}
EOF
    done
}

function wait_for_json() {
    local name
    local url
    local curl_opts
    local timeout

    local start_time
    local curr_time
    local time_diff

    name="$1"
    url="$2"
    timeout="$3"
    shift 3
    curl_opts="$@"
    echo -n "Waiting for $name to respond"
    start_time=$(date +%s)
    until curl -g -X GET "$url" "${curl_opts[@]}" 2&gt; /dev/null | jq '.' 2&gt; /dev/null &gt; /dev/null; do
        echo -n "."
        curr_time=$(date +%s)
        time_diff=$((curr_time - start_time))
        if [[ $time_diff -gt $timeout ]]; then
            printf '\nTimed out waiting for %s' "${name}"
            return 1
        fi
        sleep 5
    done
    echo " Success!"
    return 0
}
wait_for_json oc_proxy "${HOST_PROXY_API_PATH}" 10 -H "Accept: application/json" -H "Content-Type: application/json"

addresses=$(oc get node -n openshift-machine-api "${node_name}" -o json | jq -c '.status.addresses')

machine_data=$(oc get machines.machine.openshift.io -n openshift-machine-api -o json "${machine}")
host=$(echo "$machine_data" | jq '.metadata.annotations["metal3.io/BareMetalHost"]' | cut -f2 -d/ | sed 's/"//g')

if [ -z "$host" ]; then
    echo "Machine $machine is not linked to a host yet." 1&gt;&amp;2
    exit 1
fi

# The address structure on the host doesn't match the node, so extract
# the values we want into separate variables so we can build the patch
# we need.
hostname=$(echo "${addresses}" | jq '.[] | select(. | .type == "Hostname") | .address' | sed 's/"//g')

set +e
read -r -d '' host_patch &lt;&lt; EOF
{
  "status": {
    "hardware": {
      "hostname": "${hostname}",
      "nics": [
$(print_nics "${addresses}")
      ],
      "systemVendor": {
        "manufacturer": "Red Hat",
        "productName": "product name",
        "serialNumber": ""
      },
      "firmware": {
        "bios": {
          "date": "04/01/2014",
          "vendor": "SeaBIOS",
          "version": "1.11.0-2.el7"
        }
      },
      "ramMebibytes": 0,
      "storage": [],
      "cpu": {
        "arch": "x86_64",
        "model": "Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz",
        "clockMegahertz": 2199.998,
        "count": 4,
        "flags": []
      }
    }
  }
}
EOF
set -e

echo "PATCHING HOST"
echo "${host_patch}" | jq .

curl -s \
     -X PATCH \
     "${HOST_PROXY_API_PATH}/${host}/status" \
     -H "Content-type: application/merge-patch+json" \
     -d "${host_patch}"

oc get baremetalhost -n openshift-machine-api -o yaml "${host}"</pre><pre class="programlisting language-terminal">$ bash link-machine-and-node.sh custom-master3 worker-3</pre></li><li class="listitem"><p class="simpara">
						Confirm members of <code class="literal">etcdctl</code> by running the following command:
					</p><pre class="programlisting language-terminal">$ oc rsh -n openshift-etcd etcd-worker-3
etcdctl member list -w table</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">+---------+-------+--------+--------------+--------------+-------+
|   ID    | STATUS|  NAME  |   PEER ADDRS | CLIENT ADDRS |LEARNER|
+---------+-------+--------+--------------+--------------+-------+
| 2c18942f|started|worker-3|192.168.111.26|192.168.111.26| false |
| ead4f280|started|worker-5|192.168.111.28|192.168.111.28| false |
| 79153c5a|started|worker-6|192.168.111.29|192.168.111.29| false |
+---------+-------+--------+--------------+--------------+-------+</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Confirm the <code class="literal">etcd</code> operator has configured all nodes:
					</p><pre class="programlisting language-terminal">$ oc get clusteroperator etcd</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME   VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE
etcd   4.11.5    True        False         False      22h</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Confirm health of <code class="literal">etcdctl</code>:
					</p><pre class="programlisting language-terminal">$ oc rsh -n openshift-etcd etcd-worker-3
etcdctl endpoint health</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">192.168.111.26 is healthy: committed proposal: took = 9.105375ms
192.168.111.28 is healthy: committed proposal: took = 9.15205ms
192.168.111.29 is healthy: committed proposal: took = 10.277577ms</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Confirm the health of the nodes:
					</p><pre class="programlisting language-terminal">$ oc get Nodes</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME       STATUS   ROLES    AGE   VERSION
worker-1   Ready    worker   22h   v1.24.0+3882f8f
master-3   Ready    master   22h   v1.24.0+3882f8f
worker-4   Ready    worker   22h   v1.24.0+3882f8f
master-5   Ready    master   18h   v1.24.0+3882f8f
master-6   Ready    master   40m   v1.24.0+3882f8f</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Confirm the health of the <code class="literal">ClusterOperators</code>:
					</p><pre class="programlisting language-terminal">$ oc get ClusterOperators</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME                               VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE
authentication                     4.11.5    True        False         False      150m
baremetal                          4.11.5    True        False         False      22h
cloud-controller-manager           4.11.5    True        False         False      22h
cloud-credential                   4.11.5    True        False         False      22h
cluster-autoscaler                 4.11.5    True        False         False      22h
config-operator                    4.11.5    True        False         False      22h
console                            4.11.5    True        False         False      145m
csi-snapshot-controller            4.11.5    True        False         False      22h
dns                                4.11.5    True        False         False      22h
etcd                               4.11.5    True        False         False      22h
image-registry                     4.11.5    True        False         False      22h
ingress                            4.11.5    True        False         False      22h
insights                           4.11.5    True        False         False      22h
kube-apiserver                     4.11.5    True        False         False      22h
kube-controller-manager            4.11.5    True        False         False      22h
kube-scheduler                     4.11.5    True        False         False      22h
kube-storage-version-migrator      4.11.5    True        False         False      148m
machine-api                        4.11.5    True        False         False      22h
machine-approver                   4.11.5    True        False         False      22h
machine-config                     4.11.5    True        False         False      110m
marketplace                        4.11.5    True        False         False      22h
monitoring                         4.11.5    True        False         False      22h
network                            4.11.5    True        False         False      22h
node-tuning                        4.11.5    True        False         False      22h
openshift-apiserver                4.11.5    True        False         False      163m
openshift-controller-manager       4.11.5    True        False         False      22h
openshift-samples                  4.11.5    True        False         False      22h
operator-lifecycle-manager         4.11.5    True        False         False      22h
operator-lifecycle-manager-catalog 4.11.5    True        False         False      22h
operator-lifecycle-manager-pkgsvr  4.11.5    True        False         False      22h
service-ca                         4.11.5    True        False         False      22h
storage                            4.11.5    True        False         False      22h</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Confirm the <code class="literal">ClusterVersion</code>:
					</p><pre class="programlisting language-terminal">$ oc get ClusterVersion</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME      VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS
version   4.11.5    True        False         22h     Cluster version is 4.11.5</pre>

						</p></div></li></ol></div></section><section class="section _additional-resources" id="additional_resources_7"><div class="titlepage"><div><div><h2 class="title">12.7. Additional resources</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link" href="#installing-primary-control-plane-node-healthy-cluster_expanding-the-cluster" title="12.5. Installing a primary control plane node on a healthy cluster">Installing a primary control plane node on a healthy cluster</a>
					</li><li class="listitem">
						<a class="link" href="#authenticating-with-the-rest-api_installing-with-api" title="4.2. Authenticating with the REST API">Authenticating with the REST API</a>
					</li></ul></div></section></section><section class="chapter" id="assembly_installing-on-nutanix"><div class="titlepage"><div><div><h1 class="title">Chapter 13. Optional: Installing on Nutanix</h1></div></div></div><p>
			If you install OpenShift Container Platform on Nutanix, the Assisted Installer can integrate the OpenShift Container Platform cluster with the Nutanix platform, which exposes the Machine API to Nutanix and enables autoscaling and dynamically provisioning storage containers with the Nutanix Container Storage Interface (CSI).
		</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
				To deploy an OpenShift Container Platform cluster and maintain its daily operation, you need access to a Nutanix account with the necessary environment requirements. For details, see <a class="link mimir-link-warn" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.16/html/installing/installing-on-nutanix#preparing-to-install-on-nutanix" title="This content is not included in Mimir.">Environment requirements</a>.
			</p></div></div><section class="section" id="adding-hosts-on-nutanix-with-the-ui_installing-on-nutanix"><div class="titlepage"><div><div><h2 class="title">13.1. Adding hosts on Nutanix with the UI</h2></div></div></div><p>
				To add hosts on Nutanix with the user interface (UI), generate the discovery image ISO from the Assisted Installer. Use the minimal discovery image ISO. This is the default setting. The image includes only what is required to boot a host with networking. The majority of the content is downloaded upon boot. The ISO image is about 100MB in size.
			</p><p>
				After this is complete, you must create an image for the Nutanix platform and create the Nutanix virtual machines.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have created a cluster profile in the Assisted Installer UI.
					</li><li class="listitem">
						You have a Nutanix cluster environment set up, and made a note of the cluster name and subnet name.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						In <span class="strong strong"><strong>Cluster details</strong></span>, select Nutanix from the <span class="strong strong"><strong>Integrate with external partner platforms</strong></span> dropdown list. The <span class="strong strong"><strong>Include custom manifest</strong></span> checkbox is optional.
					</li><li class="listitem">
						In Host discovery, click the Add hosts button.
					</li><li class="listitem"><p class="simpara">
						Optional: Add an SSH public key so that you can connect to the Nutanix VMs as the <code class="literal">core</code> user. Having a login to the cluster hosts can provide you with debugging information during the installation.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
								If you do not have an existing SSH key pair on your local machine, follow the steps in <a class="link mimir-link-warn" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html-single/installing/index#ssh-agent-using_installing-platform-agnostic" title="This content is not included in Mimir.">Generating a key pair for cluster node SSH access</a>.
							</li><li class="listitem">
								In the <span class="strong strong"><strong>SSH public key</strong></span> field, click <span class="strong strong"><strong>Browse</strong></span> to upload the <code class="literal">id_rsa.pub</code> file containing the SSH public key. Alternatively, drag and drop the file into the field from the file manager. To see the file in the file manager, select <span class="strong strong"><strong>Show hidden files</strong></span> in the menu.
							</li></ol></div></li><li class="listitem"><p class="simpara">
						Select the required provisioning type.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							<span class="strong strong"><strong>Minimal image file: Provision with virtual media</strong></span> downloads a smaller image that will fetch the data needed to boot.
						</p></div></div></li><li class="listitem"><p class="simpara">
						In <span class="strong strong"><strong>Networking</strong></span>, select <span class="strong strong"><strong>Cluster-managed networking</strong></span>. Nutanix does not support <span class="strong strong"><strong>User-managed networking</strong></span>.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Optional: If the cluster hosts are behind a firewall that requires the use of a proxy, select <span class="strong strong"><strong>Configure cluster-wide proxy settings</strong></span>. Enter the username, password, IP address and port for the HTTP and HTTPS URLs of the proxy server.
							</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									The proxy username and password must be URL-encoded.
								</p></div></div></li><li class="listitem">
								Optional: Configure the discovery image if you want to boot it with an ignition file. See <a class="link" href="#assembly_configuring-the-discovery-image" title="Chapter 7. Configuring the discovery image">Configuring the discovery image</a> for additional details.
							</li></ol></div></li><li class="listitem">
						Click <span class="strong strong"><strong>Generate Discovery ISO</strong></span>.
					</li><li class="listitem">
						Copy the <span class="strong strong"><strong>Discovery ISO URL</strong></span>.
					</li><li class="listitem">
						In the Nutanix Prism UI, follow the directions to <a class="link mimir-link-warn" href="https://portal.nutanix.com/page/documents/details?targetId=Prism-Central-Guide-vpc_2022_6:mul-images-upload-from-remote-server-pc-t.html" title="Mimir does not include content from: portal.nutanix.com">upload the discovery image from the Assisted Installer</a>.
					</li><li class="listitem"><p class="simpara">
						In the Nutanix Prism UI, create the control plane (master) VMs <a class="link mimir-link-warn" href="https://portal.nutanix.com/page/documents/details?targetId=Prism-Central-Guide-Prism-vpc:mul-vm-create-acropolis-pc-t.html" title="Mimir does not include content from: portal.nutanix.com">through Prism Central</a>.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
								Enter the <span class="strong strong"><strong>Name</strong></span>. For example, <code class="literal">control-plane</code> or <code class="literal">master</code>.
							</li><li class="listitem">
								Enter the <span class="strong strong"><strong>Number of VMs</strong></span>. This should be 3 for the control plane.
							</li><li class="listitem">
								Ensure the remaining settings meet the minimum requirements for control plane hosts.
							</li></ol></div></li><li class="listitem"><p class="simpara">
						In the Nutanix Prism UI, create the worker VMs <a class="link mimir-link-warn" href="https://portal.nutanix.com/page/documents/details?targetId=Prism-Central-Guide-Prism-vpc:mul-vm-create-acropolis-pc-t.html" title="Mimir does not include content from: portal.nutanix.com">through Prism Central</a>.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
								Enter the <span class="strong strong"><strong>Name</strong></span>. For example, <code class="literal">worker</code>.
							</li><li class="listitem">
								Enter the <span class="strong strong"><strong>Number of VMs</strong></span>. You should create at least 2 worker nodes.
							</li><li class="listitem">
								Ensure the remaining settings meet the minimum requirements for worker hosts.
							</li></ol></div></li><li class="listitem">
						Return to the Assisted Installer user interface and wait until the Assisted Installer discovers the hosts and each of them have a <code class="literal">Ready</code> status.
					</li><li class="listitem">
						Continue with the installation procedure.
					</li></ol></div></section><section class="section" id="adding-hosts-on-nutanix-with-the-api_installing-on-nutanix"><div class="titlepage"><div><div><h2 class="title">13.2. Adding hosts on Nutanix with the API</h2></div></div></div><p>
				To add hosts on Nutanix with the API, generate the discovery image ISO from the Assisted Installer. Use the minimal discovery image ISO. This is the default setting. The image includes only what is required to boot a host with networking. The majority of the content is downloaded upon boot. The ISO image is about 100MB in size.
			</p><p>
				Once this is complete, you must create an image for the Nutanix platform and create the Nutanix virtual machines.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have set up the Assisted Installer API authentication.
					</li><li class="listitem">
						You have created an Assisted Installer cluster profile.
					</li><li class="listitem">
						You have created an Assisted Installer infrastructure environment.
					</li><li class="listitem">
						You have your infrastructure environment ID exported in your shell as <code class="literal">$INFRA_ENV_ID</code>.
					</li><li class="listitem">
						You have completed the Assisted Installer cluster configuration.
					</li><li class="listitem">
						You have a Nutanix cluster environment set up, and made a note of the cluster name and subnet name.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Configure the discovery image if you want it to boot with an ignition file.
					</li><li class="listitem"><p class="simpara">
						Create a Nutanix cluster configuration file to hold the environment variables:
					</p><pre class="programlisting language-terminal">$ touch ~/nutanix-cluster-env.sh</pre><pre class="programlisting language-terminal">$ chmod +x ~/nutanix-cluster-env.sh</pre><p class="simpara">
						If you have to start a new terminal session, you can reload the environment variables easily. For example:
					</p><pre class="programlisting language-terminal">$ source ~/nutanix-cluster-env.sh</pre></li><li class="listitem"><p class="simpara">
						Assign the Nutanix cluster’s name to the <code class="literal">NTX_CLUSTER_NAME</code> environment variable in the configuration file:
					</p><pre class="programlisting language-terminal">$ cat &lt;&lt; EOF &gt;&gt; ~/nutanix-cluster-env.sh
export NTX_CLUSTER_NAME=&lt;cluster_name&gt;
EOF</pre><p class="simpara">
						Replace <code class="literal">&lt;cluster_name&gt;</code> with the name of the Nutanix cluster.
					</p></li><li class="listitem"><p class="simpara">
						Assign the Nutanix cluster’s subnet name to the <code class="literal">NTX_SUBNET_NAME</code> environment variable in the configuration file:
					</p><pre class="programlisting language-terminal">$ cat &lt;&lt; EOF &gt;&gt; ~/nutanix-cluster-env.sh
export NTX_SUBNET_NAME=&lt;subnet_name&gt;
EOF</pre><p class="simpara">
						Replace <code class="literal">&lt;subnet_name&gt;</code> with the name of the Nutanix cluster’s subnet.
					</p></li><li class="listitem"><p class="simpara">
						Refresh the API token:
					</p><pre class="programlisting language-terminal">$ source refresh-token</pre></li><li class="listitem"><p class="simpara">
						Get the download URL:
					</p><pre class="programlisting language-terminal">$ curl -H "Authorization: Bearer ${API_TOKEN}" \
https://api.openshift.com/api/assisted-install/v2/infra-envs/${INFRA_ENV_ID}/downloads/image-url</pre></li><li class="listitem"><p class="simpara">
						Create the Nutanix image configuration file:
					</p><pre class="programlisting language-terminal">$ cat &lt;&lt; EOF &gt; create-image.json
{
  "spec": {
    "name": "ocp_ai_discovery_image.iso",
    "description": "ocp_ai_discovery_image.iso",
    "resources": {
      "architecture": "X86_64",
      "image_type": "ISO_IMAGE",
      "source_uri": "&lt;image_url&gt;",
      "source_options": {
        "allow_insecure_connection": true
      }
    }
  },
  "metadata": {
    "spec_version": 3,
    "kind": "image"
  }
}
EOF</pre><p class="simpara">
						Replace <code class="literal">&lt;image_url&gt;</code> with the image URL downloaded from the previous step.
					</p></li><li class="listitem"><p class="simpara">
						Create the Nutanix image:
					</p><pre class="programlisting language-terminal">$ curl  -k -u &lt;user&gt;:'&lt;password&gt;' -X 'POST' \
'https://&lt;domain-or-ip&gt;:&lt;port&gt;/api/nutanix/v3/images \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d @./create-image.json | jq '.metadata.uuid'</pre><p class="simpara">
						Replace <code class="literal">&lt;user&gt;</code> with the Nutanix user name. Replace <code class="literal">'&lt;password&gt;'</code> with the Nutanix password. Replace <code class="literal">&lt;domain-or-ip&gt;</code> with the domain name or IP address of the Nutanix plaform. Replace <code class="literal">&lt;port&gt;</code> with the port for the Nutanix server. The port defaults to <code class="literal">9440</code>.
					</p></li><li class="listitem"><p class="simpara">
						Assign the returned UUID to the <code class="literal">NTX_IMAGE_UUID</code> environment variable in the configuration file:
					</p><pre class="programlisting language-terminal">$ cat &lt;&lt; EOF &gt;&gt; ~/nutanix-cluster-env.sh
export NTX_IMAGE_UUID=&lt;uuid&gt;
EOF</pre></li><li class="listitem"><p class="simpara">
						Get the Nutanix cluster UUID:
					</p><pre class="screen white-space-pre white-space-pre">$ curl -k -u &lt;user&gt;:'&lt;password&gt;' -X 'POST' \
  'https://&lt;domain-or-ip&gt;:&lt;port&gt;/api/nutanix/v3/clusters/list' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
  "kind": "cluster"
}'  | jq '.entities[] | select(.spec.name=="&lt;nutanix_cluster_name&gt;") | .metadata.uuid'</pre><p class="simpara">
						Replace <code class="literal">&lt;user&gt;</code> with the Nutanix user name. Replace <code class="literal">'&lt;password&gt;'</code> with the Nutanix password. Replace <code class="literal">&lt;domain-or-ip&gt;</code> with the domain name or IP address of the Nutanix plaform. Replace <code class="literal">&lt;port&gt;</code> with the port for the Nutanix server. The port defaults to <code class="literal">9440</code>. Replace <code class="literal">&lt;nutanix_cluster_name&gt;</code> with the name of the Nutanix cluster.
					</p></li><li class="listitem"><p class="simpara">
						Assign the returned Nutanix cluster UUID to the <code class="literal">NTX_CLUSTER_UUID</code> environment variable in the configuration file:
					</p><pre class="programlisting language-terminal">$ cat &lt;&lt; EOF &gt;&gt; ~/nutanix-cluster-env.sh
export NTX_CLUSTER_UUID=&lt;uuid&gt;
EOF</pre><p class="simpara">
						Replace <code class="literal">&lt;uuid&gt;</code> with the returned UUID of the Nutanix cluster.
					</p></li><li class="listitem"><p class="simpara">
						Get the Nutanix cluster’s subnet UUID:
					</p><pre class="screen white-space-pre white-space-pre">$ curl -k -u &lt;user&gt;:'&lt;password&gt;' -X 'POST' \
  'https://&lt;domain-or-ip&gt;:&lt;port&gt;/api/nutanix/v3/subnets/list' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
  "kind": "subnet",
  "filter": "name==&lt;subnet_name&gt;"
}' | jq '.entities[].metadata.uuid'</pre><p class="simpara">
						Replace <code class="literal">&lt;user&gt;</code> with the Nutanix user name. Replace <code class="literal">'&lt;password&gt;'</code> with the Nutanix password. Replace <code class="literal">&lt;domain-or-ip&gt;</code> with the domain name or IP address of the Nutanix plaform. Replace <code class="literal">&lt;port&gt;</code> with the port for the Nutanix server. The port defaults to <code class="literal">9440</code>. Replace <code class="literal">&lt;subnet_name&gt;</code> with the name of the cluster’s subnet.
					</p></li><li class="listitem"><p class="simpara">
						Assign the returned Nutanix subnet UUID to the <code class="literal">NTX_CLUSTER_UUID</code> environment variable in the configuration file:
					</p><pre class="programlisting language-terminal">$ cat &lt;&lt; EOF &gt;&gt; ~/nutanix-cluster-env.sh
export NTX_SUBNET_UUID=&lt;uuid&gt;
EOF</pre><p class="simpara">
						Replace <code class="literal">&lt;uuid&gt;</code> with the returned UUID of the cluster subnet.
					</p></li><li class="listitem"><p class="simpara">
						Ensure the Nutanix environment variables are set:
					</p><pre class="programlisting language-terminal">$ source ~/nutanix-cluster-env.sh</pre></li><li class="listitem"><p class="simpara">
						Create a VM configuration file for each Nutanix host. Create three control plane (master) VMs and at least two worker VMs. For example:
					</p><pre class="programlisting language-terminal">$ touch create-master-0.json</pre><pre class="programlisting language-terminal">$ cat &lt;&lt; EOF &gt; create-master-0.json
{
   "spec": {
      "name": "&lt;host_name&gt;",
      "resources": {
         "power_state": "ON",
         "num_vcpus_per_socket": 1,
         "num_sockets": 16,
         "memory_size_mib": 32768,
         "disk_list": [
            {
               "disk_size_mib": 122880,
               "device_properties": {
                  "device_type": "DISK"
               }
            },
            {
               "device_properties": {
                  "device_type": "CDROM"
               },
               "data_source_reference": {
                 "kind": "image",
                 "uuid": "$NTX_IMAGE_UUID"
              }
            }
         ],
         "nic_list": [
            {
               "nic_type": "NORMAL_NIC",
               "is_connected": true,
               "ip_endpoint_list": [
                  {
                     "ip_type": "DHCP"
                  }
               ],
               "subnet_reference": {
                  "kind": "subnet",
                  "name": "$NTX_SUBNET_NAME",
                  "uuid": "$NTX_SUBNET_UUID"
               }
            }
         ],
         "guest_tools": {
            "nutanix_guest_tools": {
               "state": "ENABLED",
               "iso_mount_state": "MOUNTED"
            }
         }
      },
      "cluster_reference": {
         "kind": "cluster",
         "name": "$NTX_CLUSTER_NAME",
         "uuid": "$NTX_CLUSTER_UUID"
      }
   },
   "api_version": "3.1.0",
   "metadata": {
      "kind": "vm"
   }
}
EOF</pre><p class="simpara">
						Replace <code class="literal">&lt;host_name&gt;</code> with the name of the host.
					</p></li><li class="listitem"><p class="simpara">
						Boot each Nutanix virtual machine:
					</p><pre class="programlisting language-terminal">$ curl -k -u &lt;user&gt;:'&lt;password&gt;' -X 'POST' \
  'https://&lt;domain-or-ip&gt;:&lt;port&gt;/api/nutanix/v3/vms' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d @./&lt;vm_config_file_name&gt; | jq '.metadata.uuid'</pre><p class="simpara">
						Replace <code class="literal">&lt;user&gt;</code> with the Nutanix user name. Replace <code class="literal">'&lt;password&gt;'</code> with the Nutanix password. Replace <code class="literal">&lt;domain-or-ip&gt;</code> with the domain name or IP address of the Nutanix plaform. Replace <code class="literal">&lt;port&gt;</code> with the port for the Nutanix server. The port defaults to <code class="literal">9440</code>. Replace <code class="literal">&lt;vm_config_file_name&gt;</code> with the name of the VM configuration file.
					</p></li><li class="listitem"><p class="simpara">
						Assign the returned VM UUID to a unique environment variable in the configuration file:
					</p><pre class="programlisting language-terminal">$ cat &lt;&lt; EOF &gt;&gt; ~/nutanix-cluster-env.sh
export NTX_MASTER_0_UUID=&lt;uuid&gt;
EOF</pre><p class="simpara">
						Replace <code class="literal">&lt;uuid&gt;</code> with the returned UUID of the VM.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The environment variable must have a unique name for each VM.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Wait until the Assisted Installer has discovered each VM and they have passed validation.
					</p><pre class="programlisting language-terminal">$ curl -s -X GET "https://api.openshift.com/api/assisted-install/v2/clusters/$CLUSTER_ID"
--header "Content-Type: application/json"
-H "Authorization: Bearer $API_TOKEN"
| jq '.enabled_host_count'</pre></li><li class="listitem"><p class="simpara">
						Modify the cluster definition to enable integration with Nutanix:
					</p><pre class="programlisting language-terminal">$ curl https://api.openshift.com/api/assisted-install/v2/clusters/${CLUSTER_ID} \
-X PATCH \
-H "Authorization: Bearer ${API_TOKEN}" \
-H "Content-Type: application/json" \
-d '
{
    "platform_type":"nutanix"
}
' | jq</pre></li><li class="listitem">
						Continue with the installation procedure.
					</li></ol></div></section><section class="section" id="nutanix-post-installation-configuration_installing-on-nutanix"><div class="titlepage"><div><div><h2 class="title">13.3. Nutanix postinstallation configuration</h2></div></div></div><p>
				Follow the steps below to complete and validate the OpenShift Container Platform integration with the Nutanix cloud provider.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						The Assisted Installer has finished installing the cluster successfully.
					</li><li class="listitem">
						The cluster is connected to <a class="link mimir-link-warn" href="https://console.redhat.com/" title="Mimir does not include content from: console.redhat.com">console.redhat.com</a>.
					</li><li class="listitem">
						You have access to the Red Hat OpenShift Container Platform command line interface.
					</li></ul></div><section class="section" id="updating-the-nutanix-configuration-settings_installing-on-nutanix"><div class="titlepage"><div><div><h3 class="title">13.3.1. Updating the Nutanix configuration settings</h3></div></div></div><p>
					After installing OpenShift Container Platform on the Nutanix platform using the Assisted Installer, you must update the following Nutanix configuration settings manually:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<code class="literal">&lt;prismcentral_username&gt;</code>: The Nutanix Prism Central username.
						</li><li class="listitem">
							<code class="literal">&lt;prismcentral_password&gt;</code>: The Nutanix Prism Central password.
						</li><li class="listitem">
							<code class="literal">&lt;prismcentral_address&gt;</code>: The Nutanix Prism Central address.
						</li><li class="listitem">
							<code class="literal">&lt;prismcentral_port&gt;</code>: The Nutanix Prism Central port.
						</li><li class="listitem">
							<code class="literal">&lt;prismelement_username&gt;</code>: The Nutanix Prism Element username.
						</li><li class="listitem">
							<code class="literal">&lt;prismelement_password&gt;</code>: The Nutanix Prism Element password.
						</li><li class="listitem">
							<code class="literal">&lt;prismelement_address&gt;</code>: The Nutanix Prism Element address.
						</li><li class="listitem">
							<code class="literal">&lt;prismelement_port&gt;</code>: The Nutanix Prism Element port.
						</li><li class="listitem">
							<code class="literal">&lt;prismelement_clustername&gt;</code>: The Nutanix Prism Element cluster name.
						</li><li class="listitem">
							<code class="literal">&lt;nutanix_storage_container&gt;</code>: The Nutanix Prism storage container.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							In the OpenShift Container Platform command line interface, update the Nutanix cluster configuration settings:
						</p><pre class="programlisting language-terminal white-space-pre white-space-pre">$ oc patch infrastructure/cluster --type=merge --patch-file=/dev/stdin &lt;&lt;-EOF
{
  "spec": {
    "platformSpec": {
      "nutanix": {
        "prismCentral": {
          "address": "&lt;prismcentral_address&gt;",
          "port": &lt;prismcentral_port&gt;
        },
        "prismElements": [
          {
            "endpoint": {
              "address": "&lt;prismelement_address&gt;",
              "port": &lt;prismelement_port&gt;
            },
            "name": "&lt;prismelement_clustername&gt;"
          }
        ]
      },
      "type": "Nutanix"
    }
  }
}
EOF</pre><div class="formalpara"><p class="title"><strong>Sample output</strong></p><p>
								
<pre class="programlisting language-terminal">infrastructure.config.openshift.io/cluster patched</pre>

							</p></div><p class="simpara">
							For additional details, see <a class="link mimir-link-warn-approx" href="/products/#letter-O" title="Mimir does not include this page, but the link has been rewritten to point to the nearest parent document.">Creating a machine set on Nutanix</a>.
						</p></li><li class="listitem"><p class="simpara">
							Create the Nutanix secret:
						</p><pre class="programlisting language-terminal white-space-pre white-space-pre">$ cat &lt;&lt;EOF | oc create -f -
apiVersion: v1
kind: Secret
metadata:
   name: nutanix-credentials
   namespace: openshift-machine-api
type: Opaque
stringData:
  credentials: |
[{"type":"basic_auth","data":{"prismCentral":{"username":"${&lt;prismcentral_username&gt;}","password":"${&lt;prismcentral_password&gt;}"},"prismElements":null}}]
EOF</pre><div class="formalpara"><p class="title"><strong>Sample output</strong></p><p>
								
<pre class="programlisting language-terminal">secret/nutanix-credentials created</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							When installing OpenShift Container Platform version 4.13 or later, update the Nutanix cloud provider configuration:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Get the Nutanix cloud provider configuration YAML file:
								</p><pre class="programlisting language-terminal white-space-pre white-space-pre">$ oc get cm cloud-provider-config -o yaml -n openshift-config &gt; cloud-provider-config-backup.yaml</pre></li><li class="listitem"><p class="simpara">
									Create a backup of the configuration file:
								</p><pre class="programlisting language-terminal white-space-pre white-space-pre">$ cp cloud-provider-config_backup.yaml cloud-provider-config.yaml</pre></li><li class="listitem"><p class="simpara">
									Open the configuration YAML file:
								</p><pre class="programlisting language-yaml">$ vi cloud-provider-config.yaml</pre></li><li class="listitem"><p class="simpara">
									Edit the configuration YAML file as follows:
								</p><pre class="programlisting language-yaml">kind: ConfigMap
apiVersion: v1
metadata:
  name: cloud-provider-config
  namespace: openshift-config
data:
  config: |
    {
    	"prismCentral": {
    		"address": "&lt;prismcentral_address&gt;",
    		"port":&lt;prismcentral_port&gt;,
    		"credentialRef": {
    		   "kind": "Secret",
    		   "name": "nutanix-credentials",
    		   "namespace": "openshift-cloud-controller-manager"
    		}
    	},
    	"topologyDiscovery": {
    		"type": "Prism",
    		"topologyCategories": null
    	},
    	"enableCustomLabeling": true
    }</pre></li><li class="listitem"><p class="simpara">
									Apply the configuration updates:
								</p><pre class="programlisting language-terminal white-space-pre white-space-pre">$ oc apply -f cloud-provider-config.yaml</pre><div class="formalpara"><p class="title"><strong>Sample output</strong></p><p>
										
<pre class="programlisting language-terminal">Warning: resource configmaps/cloud-provider-config is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by oc apply. oc apply should only be used on resources created declaratively by either oc create --save-config or oc apply. The missing annotation will be patched automatically.

configmap/cloud-provider-config configured</pre>

									</p></div></li></ol></div></li></ol></div></section><section class="section" id="creating-the-nutanix-csi-operator-group_installing-on-nutanix"><div class="titlepage"><div><div><h3 class="title">13.3.2. Creating the Nutanix CSI Operator group</h3></div></div></div><p>
					Create an Operator group for the Nutanix CSI Operator.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						For a description of operator groups and related concepts, see <span class="emphasis"><em>Common Operator Framework Terms</em></span> in <span class="emphasis"><em>Additional Resources</em></span>.
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Open the Nutanix CSI Operator Group YAML file:
						</p><pre class="programlisting language-yaml">$ vi openshift-cluster-csi-drivers-operator-group.yaml</pre></li><li class="listitem"><p class="simpara">
							Edit the YAML file as follows:
						</p><pre class="programlisting language-yaml">apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  generateName: openshift-cluster-csi-drivers
  namespace: openshift-cluster-csi-drivers
spec:
  targetNamespaces:
  - openshift-cluster-csi-drivers
  upgradeStrategy: Default</pre></li><li class="listitem"><p class="simpara">
							Create the Operator Group:
						</p><pre class="programlisting language-terminal white-space-pre white-space-pre">$ oc create -f openshift-cluster-csi-drivers-operator-group.yaml</pre><div class="formalpara"><p class="title"><strong>Sample output</strong></p><p>
								
<pre class="screen">operatorgroup.operators.coreos.com/openshift-cluster-csi-driversjw9cd created</pre>

							</p></div></li></ol></div></section><section class="section" id="installing-the-nutanix-csi-operator_installing-on-nutanix"><div class="titlepage"><div><div><h3 class="title">13.3.3. Installing the Nutanix CSI Operator</h3></div></div></div><p>
					The Nutanix Container Storage Interface (CSI) Operator for Kubernetes deploys and manages the Nutanix CSI Driver.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						For instructions on performing this step through the OpenShift Container Platform web console, see the <span class="emphasis"><em>Installing the Operator</em></span> section of the <span class="emphasis"><em>Nutanix CSI Operator</em></span> document in <span class="emphasis"><em>Additional Resources</em></span>.
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Get the parameter values for the Nutanix CSI Operator YAML file:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Check that the Nutanix CSI Operator exists:
								</p><pre class="programlisting language-terminal white-space-pre white-space-pre">$ oc get packagemanifests | grep nutanix</pre><div class="formalpara"><p class="title"><strong>Sample output</strong></p><p>
										
<pre class="screen">nutanixcsioperator   Certified Operators   129m</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									Assign the default channel for the Operator to a BASH variable:
								</p><pre class="screen white-space-pre white-space-pre">$ DEFAULT_CHANNEL=$(oc get packagemanifests nutanixcsioperator -o jsonpath={.status.defaultChannel})</pre></li><li class="listitem"><p class="simpara">
									Assign the starting cluster service version (CSV) for the Operator to a BASH variable:
								</p><pre class="screen white-space-pre white-space-pre">$ STARTING_CSV=$(oc get packagemanifests nutanixcsioperator -o jsonpath=\{.status.channels[*].currentCSV\})</pre></li><li class="listitem"><p class="simpara">
									Assign the catalog source for the subscription to a BASH variable:
								</p><pre class="screen white-space-pre white-space-pre">$ CATALOG_SOURCE=$(oc get packagemanifests nutanixcsioperator -o jsonpath=\{.status.catalogSource\})</pre></li><li class="listitem"><p class="simpara">
									Assign the Nutanix CSI Operator source namespace to a BASH variable:
								</p><pre class="screen white-space-pre white-space-pre">$ SOURCE_NAMESPACE=$(oc get packagemanifests nutanixcsioperator -o jsonpath=\{.status.catalogSourceNamespace\})</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Create the Nutanix CSI Operator YAML file using the BASH variables:
						</p><pre class="programlisting language-yaml">$ cat &lt;&lt; EOF &gt; nutanixcsioperator.yaml
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: nutanixcsioperator
  namespace: openshift-cluster-csi-drivers
spec:
  channel: $DEFAULT_CHANNEL
  installPlanApproval: Automatic
  name: nutanixcsioperator
  source: $CATALOG_SOURCE
  sourceNamespace: $SOURCE_NAMESPACE
  startingCSV: $STARTING_CSV
EOF</pre></li><li class="listitem"><p class="simpara">
							Create the CSI Nutanix Operator:
						</p><pre class="screen white-space-pre white-space-pre">$ oc apply -f nutanixcsioperator.yaml</pre><div class="formalpara"><p class="title"><strong>Sample output</strong></p><p>
								
<pre class="programlisting language-terminal">subscription.operators.coreos.com/nutanixcsioperator created</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Run the following command until the Operator subscription state changes to <code class="literal">AtLatestKnown</code>. This indicates that the Operator subscription has been created, and may take some time.
						</p><pre class="screen white-space-pre white-space-pre">$ oc get subscription nutanixcsioperator -n openshift-cluster-csi-drivers -o 'jsonpath={..status.state}'</pre></li></ol></div></section><section class="section" id="deploying-the-nutanix-csi-storage-driver_installing-on-nutanix"><div class="titlepage"><div><div><h3 class="title">13.3.4. Deploying the Nutanix CSI storage driver</h3></div></div></div><p>
					The Nutanix Container Storage Interface (CSI) Driver for Kubernetes provides scalable and persistent storage for stateful applications.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						For instructions on performing this step through the OpenShift Container Platform web console, see the <span class="emphasis"><em>Installing the CSI Driver using the Operator</em></span> section of the <span class="emphasis"><em>Nutanix CSI Operator</em></span> document in <span class="emphasis"><em>Additional Resources</em></span>.
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a <code class="literal">NutanixCsiStorage</code> resource to deploy the driver:
						</p><pre class="screen white-space-pre white-space-pre">$ cat &lt;&lt;EOF | oc create -f -
apiVersion: crd.nutanix.com/v1alpha1
kind: NutanixCsiStorage
metadata:
  name: nutanixcsistorage
  namespace: openshift-cluster-csi-drivers
spec: {}
EOF</pre><div class="formalpara"><p class="title"><strong>Sample output</strong></p><p>
								
<pre class="programlisting language-terminal">snutanixcsistorage.crd.nutanix.com/nutanixcsistorage created</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Create a Nutanix secret YAML file for the CSI storage driver:
						</p><pre class="screen white-space-pre white-space-pre">$ cat &lt;&lt;EOF | oc create -f -
apiVersion: v1
kind: Secret
metadata:
  name: ntnx-secret
  namespace: openshift-cluster-csi-drivers
stringData:
  # prism-element-ip:prism-port:admin:password
  key: &lt;prismelement_address:prismelement_port:prismcentral_username:prismcentral_password&gt; <span id="CO23-1"><!--Empty--></span><span class="callout">1</span>
EOF</pre><div class="admonition note"><div class="admonition_header">Note</div><div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO23-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Replace these parameters with actual values while keeping the same format.
									</div></dd></dl></div></div></div><div class="formalpara"><p class="title"><strong>Sample output</strong></p><p>
								
<pre class="programlisting language-terminal">secret/nutanix-secret created</pre>

							</p></div></li></ol></div></section><section class="section" id="validating-the-post-installation-configurations_installing-on-nutanix"><div class="titlepage"><div><div><h3 class="title">13.3.5. Validating the postinstallation configurations</h3></div></div></div><p>
					Run the following steps to validate the configuration.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Verify that you can create a storage class:
						</p><pre class="screen white-space-pre white-space-pre">$ cat &lt;&lt;EOF | oc create -f -
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: nutanix-volume
  annotations:
    storageclass.kubernetes.io/is-default-class: 'true'
provisioner: csi.nutanix.com
parameters:
  csi.storage.k8s.io/fstype: ext4
  csi.storage.k8s.io/provisioner-secret-namespace: openshift-cluster-csi-drivers
  csi.storage.k8s.io/provisioner-secret-name: ntnx-secret
  storageContainer: &lt;nutanix_storage_container&gt; <span id="CO24-1"><!--Empty--></span><span class="callout">1</span>
  csi.storage.k8s.io/controller-expand-secret-name: ntnx-secret
  csi.storage.k8s.io/node-publish-secret-namespace: openshift-cluster-csi-drivers
  storageType: NutanixVolumes
  csi.storage.k8s.io/node-publish-secret-name: ntnx-secret
  csi.storage.k8s.io/controller-expand-secret-namespace: openshift-cluster-csi-drivers
reclaimPolicy: Delete
allowVolumeExpansion: true
volumeBindingMode: Immediate
EOF</pre><div class="admonition note"><div class="admonition_header">Note</div><div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO24-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Take &lt;nutanix_storage_container&gt; from the Nutanix configuration; for example, SelfServiceContainer.
									</div></dd></dl></div></div></div><div class="formalpara"><p class="title"><strong>Sample output</strong></p><p>
								
<pre class="programlisting language-terminal">storageclass.storage.k8s.io/nutanix-volume created</pre>

							</p></div></li><li class="listitem"><p class="simpara">
							Verify that you can create the Nutanix persistent volume claim (PVC):
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Create the persistent volume claim (PVC):
								</p><pre class="programlisting language-terminal white-space-pre white-space-pre">$ cat &lt;&lt;EOF | oc create -f -
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: nutanix-volume-pvc
  namespace: openshift-cluster-csi-drivers
  annotations:
    volume.beta.kubernetes.io/storage-provisioner: csi.nutanix.com
    volume.kubernetes.io/storage-provisioner: csi.nutanix.com
  finalizers:
    - kubernetes.io/pvc-protection
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: nutanix-volume
  volumeMode: Filesystem
EOF</pre><div class="white-space-pre white-space-pre"><p class="title"><strong>Sample output</strong></p><p>
										
<pre class="programlisting language-terminal">persistentvolumeclaim/nutanix-volume-pvc created</pre>

									</p></div></li><li class="listitem"><p class="simpara">
									Validate that the persistent volume claim (PVC) status is Bound:
								</p><pre class="programlisting language-terminal white-space-pre white-space-pre">$ oc get pvc -n openshift-cluster-csi-drivers</pre><div class="formalpara"><p class="title"><strong>Sample output</strong></p><p>
										
<pre class="programlisting language-terminal">NAME                 STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS     AGE
nutanix-volume-pvc   Bound                                        nutanix-volume   52s</pre>

									</p></div></li></ol></div></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link mimir-link-warn-approx" href="/products/#letter-O" title="Mimir does not include this page, but the link has been rewritten to point to the nearest parent document.">Creating a machine set on Nutanix</a>.
						</li><li class="listitem">
							<a class="link mimir-link-warn" href="https://opendocs.nutanix.com/openshift/operators/csi" title="Mimir does not include content from: opendocs.nutanix.com">Nutanix CSI Operator</a>
						</li><li class="listitem">
							<a class="link mimir-link-warn" href="https://portal.nutanix.com/page/documents/details?targetId=CSI-Volume-Driver-v2_5:csi-csi-plugin-storage-c.html" title="Mimir does not include content from: portal.nutanix.com">Storage Management</a>
						</li><li class="listitem">
							<a class="link mimir-link-warn-approx" href="/products/#letter-O" title="Mimir does not include this page, but the link has been rewritten to point to the nearest parent document.">Common Operator Framework Terms</a>
						</li></ul></div></section></section></section><section class="chapter" id="installing-on-vsphere"><div class="titlepage"><div><div><h1 class="title">Chapter 14. Optional: Installing on vSphere</h1></div></div></div><p>
			The Assisted Installer integrates the OpenShift Container Platform cluster with the vSphere platform, which exposes the Machine API to vSphere and enables autoscaling.
		</p><section class="section" id="adding-hosts-on-vsphere_installing-on-vsphere"><div class="titlepage"><div><div><h2 class="title">14.1. Adding hosts on vSphere</h2></div></div></div><p>
				You can add hosts to the Assisted Installer cluster using the online vSphere client or the <code class="literal">govc</code> vSphere CLI tool. The following procedure demonstrates adding hosts with the <code class="literal">govc</code> CLI tool. To use the online vSphere Client, refer to the documentation for vSphere.
			</p><p>
				To add hosts on vSphere with the vSphere <code class="literal">govc</code> CLI, generate the discovery image ISO from the Assisted Installer. The minimal discovery image ISO is the default setting. This image includes only what is required to boot a host with networking. The majority of the content is downloaded upon boot. The ISO image is about 100MB in size.
			</p><p>
				After this is complete, you must create an image for the vSphere platform and create the vSphere virtual machines.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You are using vSphere 7.0.2 or higher.
					</li><li class="listitem">
						You have the vSphere <code class="literal">govc</code> CLI tool installed and configured.
					</li><li class="listitem">
						You have set <code class="literal">clusterSet disk.enableUUID</code> to <span class="strong strong"><strong>true</strong></span> in vSphere.
					</li><li class="listitem">
						You have created a cluster in the Assisted Installer web console, or
					</li><li class="listitem">
						You have created an Assisted Installer cluster profile and infrastructure environment with the API.
					</li><li class="listitem">
						You have exported your infrastructure environment ID in your shell as <code class="literal">$INFRA_ENV_ID</code>.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Configure the discovery image if you want it to boot with an ignition file.
					</li><li class="listitem">
						In <span class="strong strong"><strong>Cluster details</strong></span>, select vSphere from the <span class="strong strong"><strong>Integrate with external partner platforms</strong></span> dropdown list. The <span class="strong strong"><strong>Include custom manifest</strong></span> checkbox is optional.
					</li><li class="listitem">
						In <span class="strong strong"><strong>Host discovery</strong></span>, click the <span class="strong strong"><strong>Add hosts</strong></span> button and select the provisioning type.
					</li><li class="listitem"><p class="simpara">
						Add an SSH public key so that you can connect to the vSphere VMs as the <code class="literal">core</code> user. Having a login to the cluster hosts can provide you with debugging information during the installation.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
								If you do not have an existing SSH key pair on your local machine, follow the steps in <a class="link mimir-link-warn" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html-single/installing/index#ssh-agent-using_installing-platform-agnostic" title="This content is not included in Mimir.">Generating a key pair for cluster node SSH access</a>.
							</li><li class="listitem">
								In the <span class="strong strong"><strong>SSH public key</strong></span> field, click <span class="strong strong"><strong>Browse</strong></span> to upload the <code class="literal">id_rsa.pub</code> file containing the SSH public key. Alternatively, drag and drop the file into the field from the file manager. To see the file in the file manager, select <span class="strong strong"><strong>Show hidden files</strong></span> in the menu.
							</li></ol></div></li><li class="listitem"><p class="simpara">
						Select the required discovery image ISO.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							<span class="strong strong"><strong>Minimal image file: Provision with virtual media</strong></span> downloads a smaller image that will fetch the data needed to boot.
						</p></div></div></li><li class="listitem"><p class="simpara">
						In <span class="strong strong"><strong>Networking</strong></span>, select <span class="strong strong"><strong>Cluster-managed networking</strong></span> or <span class="strong strong"><strong>User-managed networking</strong></span>:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Optional: If the cluster hosts are behind a firewall that requires the use of a proxy, select <span class="strong strong"><strong>Configure cluster-wide proxy settings</strong></span>. Enter the username, password, IP address and port for the HTTP and HTTPS URLs of the proxy server.
							</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									The proxy username and password must be URL-encoded.
								</p></div></div></li><li class="listitem">
								Optional: If the cluster hosts are in a network with a re-encrypting man-in-the-middle (MITM) proxy or the cluster needs to trust certificates for other purposes such as container image registries, select <span class="strong strong"><strong>Configure cluster-wide trusted certificates</strong></span> and add the additional certificates.
							</li><li class="listitem">
								Optional: Configure the discovery image if you want to boot it with an ignition file. For more information, see <span class="emphasis"><em>Additional Resources</em></span>.
							</li></ol></div></li><li class="listitem">
						Click <span class="strong strong"><strong>Generate Discovery ISO</strong></span>.
					</li><li class="listitem">
						Copy the <span class="strong strong"><strong>Discovery ISO URL</strong></span>.
					</li><li class="listitem"><p class="simpara">
						Download the discovery ISO:
					</p><pre class="programlisting language-terminal">$ wget - O vsphere-discovery-image.iso &lt;discovery_url&gt;</pre><p class="simpara">
						Replace <code class="literal">&lt;discovery_url&gt;</code> with the <span class="strong strong"><strong>Discovery ISO URL</strong></span> from the preceding step.
					</p></li><li class="listitem"><p class="simpara">
						On the command line, power off and delete any preexisting virtual machines:
					</p><pre class="programlisting language-terminal">$ for VM in $(/usr/local/bin/govc ls /&lt;datacenter&gt;/vm/&lt;folder_name&gt;)
do
 	/usr/local/bin/govc vm.power -off $VM
 	/usr/local/bin/govc vm.destroy $VM
done</pre><p class="simpara">
						Replace <code class="literal">&lt;datacenter&gt;</code> with the name of the data center. Replace <code class="literal">&lt;folder_name&gt;</code> with the name of the VM inventory folder.
					</p></li><li class="listitem"><p class="simpara">
						Remove preexisting ISO images from the data store, if there are any:
					</p><pre class="programlisting language-terminal">$ govc datastore.rm -ds &lt;iso_datastore&gt; &lt;image&gt;</pre><p class="simpara">
						Replace <code class="literal">&lt;iso_datastore&gt;</code> with the name of the data store. Replace <code class="literal">image</code> with the name of the ISO image.
					</p></li><li class="listitem"><p class="simpara">
						Upload the Assisted Installer discovery ISO:
					</p><pre class="programlisting language-terminal">$ govc datastore.upload -ds &lt;iso_datastore&gt;  vsphere-discovery-image.iso</pre><p class="simpara">
						Replace <code class="literal">&lt;iso_datastore&gt;</code> with the name of the data store.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							All nodes in the cluster must boot from the discovery image.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Boot three control plane nodes:
					</p><pre class="programlisting language-terminal">$ govc vm.create -net.adapter &lt;network_adapter_type&gt; \
                 -disk.controller &lt;disk_controller_type&gt; \
                 -pool=&lt;resource_pool&gt; \
                 -c=16 \
                 -m=32768 \
                 -disk=120GB \
                 -disk-datastore=&lt;datastore_file&gt; \
                 -net.address="&lt;nic_mac_address&gt;" \
                 -iso-datastore=&lt;iso_datastore&gt; \
                 -iso="vsphere-discovery-image.iso" \
                 -folder="&lt;inventory_folder&gt;" \
                 &lt;hostname&gt;.&lt;cluster_name&gt;.example.com</pre><p class="simpara">
						See <a class="link mimir-link-warn" href="https://github.com/vmware/govmomi/blob/main/govc/USAGE.md#vmcreate" title="Mimir does not include content from: github.com">vm.create</a> for details.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The foregoing example illustrates the minimum required resources for control plane nodes.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Boot at least two worker nodes:
					</p><pre class="programlisting language-terminal">$ govc vm.create -net.adapter &lt;network_adapter_type&gt; \
                 -disk.controller &lt;disk_controller_type&gt; \
                 -pool=&lt;resource_pool&gt; \
                 -c=4 \
                 -m=8192 \
                 -disk=120GB \
                 -disk-datastore=&lt;datastore_file&gt; \
                 -net.address="&lt;nic_mac_address&gt;" \
                 -iso-datastore=&lt;iso_datastore&gt; \
                 -iso="vsphere-discovery-image.iso" \
                 -folder="&lt;inventory_folder&gt;" \
                 &lt;hostname&gt;.&lt;cluster_name&gt;.example.com</pre><p class="simpara">
						See <a class="link mimir-link-warn" href="https://github.com/vmware/govmomi/blob/main/govc/USAGE.md#vmcreate" title="Mimir does not include content from: github.com">vm.create</a> for details.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The foregoing example illustrates the minimum required resources for worker nodes.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Ensure the VMs are running:
					</p><pre class="programlisting language-terminal">$  govc ls /&lt;datacenter&gt;/vm/&lt;folder_name&gt;</pre><p class="simpara">
						Replace <code class="literal">&lt;datacenter&gt;</code> with the name of the data center. Replace <code class="literal">&lt;folder_name&gt;</code> with the name of the VM inventory folder.
					</p></li><li class="listitem"><p class="simpara">
						After 2 minutes, shut down the VMs:
					</p><pre class="programlisting language-bash">$ for VM in $(govc ls /&lt;datacenter&gt;/vm/&lt;folder_name&gt;)
do
     govc vm.power -s=true $VM
done</pre><p class="simpara">
						Replace <code class="literal">&lt;datacenter&gt;</code> with the name of the data center. Replace <code class="literal">&lt;folder_name&gt;</code> with the name of the VM inventory folder.
					</p></li><li class="listitem"><p class="simpara">
						Set the <code class="literal">disk.enableUUID</code> setting to <code class="literal">TRUE</code>:
					</p><pre class="programlisting language-bash">$ for VM in $(govc ls /&lt;datacenter&gt;/vm/&lt;folder_name&gt;)
do
     govc vm.change -vm $VM -e disk.enableUUID=TRUE
done</pre><p class="simpara">
						Replace <code class="literal">&lt;datacenter&gt;</code> with the name of the data center. Replace <code class="literal">&lt;folder_name&gt;</code> with the name of the VM inventory folder.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							You must set <code class="literal">disk.enableUUID</code> to <code class="literal">TRUE</code> on all of the nodes to enable autoscaling with vSphere.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Restart the VMs:
					</p><pre class="programlisting language-bash">$  for VM in $(govc ls /&lt;datacenter&gt;/vm/&lt;folder_name&gt;)
do
     govc vm.power -on=true $VM
done</pre><p class="simpara">
						Replace <code class="literal">&lt;datacenter&gt;</code> with the name of the data center. Replace <code class="literal">&lt;folder_name&gt;</code> with the name of the VM inventory folder.
					</p></li><li class="listitem">
						Return to the Assisted Installer user interface and wait until the Assisted Installer discovers the hosts and each of them have a <code class="literal">Ready</code> status.
					</li><li class="listitem">
						Select roles if needed.
					</li><li class="listitem">
						In <span class="strong strong"><strong>Networking</strong></span>, clear the <span class="strong strong"><strong>Allocate IPs via DHCP server</strong></span> checkbox.
					</li><li class="listitem">
						Set the API VIP address.
					</li><li class="listitem">
						Set the Ingress VIP address.
					</li><li class="listitem">
						Continue with the installation procedure.
					</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						<a class="link" href="#assembly_configuring-the-discovery-image" title="Chapter 7. Configuring the discovery image">Configuring the discovery image</a>
					</li></ul></div></section><section class="section" id="vsphere-post-installation-configuration_installing-on-vsphere"><div class="titlepage"><div><div><h2 class="title">14.2. vSphere postinstallation configuration using the CLI</h2></div></div></div><p>
				After installing an OpenShift Container Platform cluster using the Assisted Installer on vSphere with the platform integration feature enabled, you must update the following vSphere configuration settings manually:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						vCenter username
					</li><li class="listitem">
						vCenter password
					</li><li class="listitem">
						vCenter address
					</li><li class="listitem">
						vCenter cluster
					</li><li class="listitem">
						Data center
					</li><li class="listitem">
						Data store
					</li><li class="listitem">
						Folder
					</li></ul></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						The Assisted Installer has finished installing the cluster successfully.
					</li><li class="listitem">
						The cluster is connected to <a class="link mimir-link-warn" href="http://console.redhat.com" title="Mimir does not include content from: console.redhat.com">console.redhat.com</a>.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Generate a base64-encoded username and password for vCenter:
					</p><pre class="programlisting language-terminal">$ echo -n "&lt;vcenter_username&gt;" | base64 -w0</pre><p class="simpara">
						Replace <code class="literal">&lt;vcenter_username&gt;</code> with your vCenter username.
					</p><pre class="programlisting language-terminal">$ echo -n "&lt;vcenter_password&gt;" | base64 -w0</pre><p class="simpara">
						Replace <code class="literal">&lt;vcenter_password&gt;</code> with your vCenter password.
					</p></li><li class="listitem"><p class="simpara">
						Backup the vSphere credentials:
					</p><pre class="programlisting language-terminal">$ oc get secret vsphere-creds -o yaml -n kube-system &gt; creds_backup.yaml</pre></li><li class="listitem"><p class="simpara">
						Edit the vSphere credentials:
					</p><pre class="programlisting language-terminal">$ cp creds_backup.yaml vsphere-creds.yaml</pre><pre class="programlisting language-terminal">$ vi vsphere-creds.yaml</pre><pre class="programlisting language-yaml white-space-pre white-space-pre">apiVersion: v1
data:
  &lt;vcenter_address&gt;.username: &lt;vcenter_username_encoded&gt;
  &lt;vcenter_address&gt;.password: &lt;vcenter_password_encoded&gt;
kind: Secret
metadata:
  annotations:
    cloudcredential.openshift.io/mode: passthrough
  creationTimestamp: "2022-01-25T17:39:50Z"
  name: vsphere-creds
  namespace: kube-system
  resourceVersion: "2437"
  uid: 06971978-e3a5-4741-87f9-2ca3602f2658
type: Opaque</pre><p class="simpara">
						Replace <code class="literal">&lt;vcenter_address&gt;</code> with the vCenter address. Replace <code class="literal">&lt;vcenter_username_encoded&gt;</code> with the base64-encoded version of your vSphere username. Replace <code class="literal">&lt;vcenter_password_encoded&gt;</code> with the base64-encoded version of your vSphere password.
					</p></li><li class="listitem"><p class="simpara">
						Replace the vSphere credentials:
					</p><pre class="programlisting language-terminal">$ oc replace -f vsphere-creds.yaml</pre></li><li class="listitem"><p class="simpara">
						Redeploy the kube-controller-manager pods:
					</p><pre class="programlisting language-terminal">$ oc patch kubecontrollermanager cluster -p='{"spec": {"forceRedeploymentReason": "recovery-'"$( date --rfc-3339=ns )"'"}}' --type=merge</pre></li><li class="listitem"><p class="simpara">
						Backup the vSphere cloud provider configuration:
					</p><pre class="programlisting language-terminal">$ oc get cm cloud-provider-config -o yaml -n openshift-config &gt; cloud-provider-config_backup.yaml</pre></li><li class="listitem"><p class="simpara">
						Edit the cloud provider configuration:
					</p><pre class="programlisting language-terminal">$ cloud-provider-config_backup.yaml cloud-provider-config.yaml</pre><pre class="programlisting language-terminal white-space-pre white-space-pre">$ vi cloud-provider-config.yaml</pre><pre class="programlisting language-yaml white-space-pre white-space-pre">apiVersion: v1
data:
  config: |
    [Global]
    secret-name = "vsphere-creds"
    secret-namespace = "kube-system"
    insecure-flag = "1"

    [Workspace]
    server = "&lt;vcenter_address&gt;"
    datacenter = "&lt;datacenter&gt;"
    default-datastore = "&lt;datastore&gt;"
    folder = "/&lt;datacenter&gt;/vm/&lt;folder&gt;"

    [VirtualCenter "&lt;vcenter_address&gt;"]
    datacenters = "&lt;datacenter&gt;"
kind: ConfigMap
metadata:
  creationTimestamp: "2022-01-25T17:40:49Z"
  name: cloud-provider-config
  namespace: openshift-config
  resourceVersion: "2070"
  uid: 80bb8618-bf25-442b-b023-b31311918507</pre><p class="simpara">
						Replace <code class="literal">&lt;vcenter_address&gt;</code> with the vCenter address. Replace <code class="literal">&lt;datacenter&gt;</code> with the name of the data center. Replace <code class="literal">&lt;datastore&gt;</code> with the name of the data store. Replace <code class="literal">&lt;folder&gt;</code> with the folder containing the cluster VMs.
					</p></li><li class="listitem"><p class="simpara">
						Apply the cloud provider configuration:
					</p><pre class="programlisting language-terminal">$ oc apply -f cloud-provider-config.yaml</pre></li><li class="listitem"><p class="simpara">
						Taint the nodes with the <code class="literal">uninitialized</code> taint:
					</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							Follow steps 9 through 12 if you are installing OpenShift Container Platform 4.13 or later.
						</p></div></div><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Identify the nodes to taint:
							</p><pre class="programlisting language-terminal">$ oc get nodes</pre></li><li class="listitem"><p class="simpara">
								Run the following command for each node:
							</p><pre class="programlisting language-terminal">$ oc adm taint node &lt;node_name&gt; node.cloudprovider.kubernetes.io/uninitialized=true:NoSchedule</pre><p class="simpara">
								Replace <code class="literal">&lt;node_name&gt;</code> with the name of the node.
							</p></li></ol></div><div class="formalpara"><p class="title"><strong>Example</strong></p><p>
							
<pre class="programlisting language-terminal">$ oc get nodes
NAME                STATUS   ROLES                  AGE   VERSION
master-0   Ready    control-plane,master   45h   v1.26.3+379cd9f
master-1   Ready    control-plane,master   45h   v1.26.3+379cd9f
worker-0   Ready    worker                 45h   v1.26.3+379cd9f
worker-1   Ready    worker                 45h   v1.26.3+379cd9f
master-2   Ready    control-plane,master   45h   v1.26.3+379cd9f

$ oc adm taint node master-0 node.cloudprovider.kubernetes.io/uninitialized=true:NoSchedule
$ oc adm taint node master-1 node.cloudprovider.kubernetes.io/uninitialized=true:NoSchedule
$ oc adm taint node master-2 node.cloudprovider.kubernetes.io/uninitialized=true:NoSchedule
$ oc adm taint node worker-0 node.cloudprovider.kubernetes.io/uninitialized=true:NoSchedule
$ oc adm taint node worker-1 node.cloudprovider.kubernetes.io/uninitialized=true:NoSchedule</pre>

						</p></div></li><li class="listitem"><p class="simpara">
						Back up the infrastructures configuration:
					</p><pre class="programlisting language-terminal white-space-pre white-space-pre">$ oc get infrastructures.config.openshift.io -o yaml &gt; infrastructures.config.openshift.io.yaml.backup</pre></li><li class="listitem"><p class="simpara">
						Edit the infrastructures configuration:
					</p><pre class="programlisting language-terminal white-space-pre white-space-pre">$ cp infrastructures.config.openshift.io.yaml.backup infrastructures.config.openshift.io.yaml</pre><pre class="programlisting language-terminal white-space-pre white-space-pre">$ vi infrastructures.config.openshift.io.yaml</pre><pre class="programlisting language-yaml white-space-pre white-space-pre">apiVersion: v1
items:
- apiVersion: config.openshift.io/v1
  kind: Infrastructure
  metadata:
    creationTimestamp: "2022-05-07T10:19:55Z"
    generation: 1
    name: cluster
    resourceVersion: "536"
    uid: e8a5742c-6d15-44e6-8a9e-064b26ab347d
  spec:
    cloudConfig:
      key: config
      name: cloud-provider-config
    platformSpec:
      type: VSphere
      vsphere:
        failureDomains:
        - name: assisted-generated-failure-domain
          region: assisted-generated-region
          server: &lt;vcenter_address&gt;
          topology:
            computeCluster: /&lt;data_center&gt;/host/&lt;vcenter_cluster&gt;
            datacenter: &lt;data_center&gt;
            datastore: /&lt;data_center&gt;/datastore/&lt;datastore&gt;
            folder: "/&lt;data_center&gt;/path/to/folder"
            networks:
            - "VM Network"
            resourcePool: /&lt;data_center&gt;/host/&lt;vcenter_cluster&gt;/Resources
          zone: assisted-generated-zone
        nodeNetworking:
          external: {}
          internal: {}
        vcenters:
        - datacenters:
          - &lt;data_center&gt;
          server: &lt;vcenter_address&gt;

kind: List
metadata:
  resourceVersion: ""</pre><p class="simpara">
						Replace <code class="literal">&lt;vcenter_address&gt;</code> with your vCenter address. Replace <code class="literal">&lt;datacenter&gt;</code> with the name of your vCenter data center. Replace <code class="literal">&lt;datastore&gt;</code> with the name of your vCenter data store. Replace <code class="literal">&lt;folder&gt;</code> with the folder containing the cluster VMs. Replace <code class="literal">&lt;vcenter_cluster&gt;</code> with the vSphere vCenter cluster where OpenShift Container Platform is installed.
					</p></li><li class="listitem"><p class="simpara">
						Apply the infrastructures configuration:
					</p><pre class="programlisting language-terminal white-space-pre white-space-pre">$ oc apply -f infrastructures.config.openshift.io.yaml --overwrite=true</pre></li></ol></div></section><section class="section" id="vsphere-post-installation-configuration-console_installing-on-vsphere"><div class="titlepage"><div><div><h2 class="title">14.3. vSphere postinstallation configuration using the web console</h2></div></div></div><p>
				After installing an OpenShift Container Platform cluster by using the Assisted Installer on vSphere with the platform integration feature enabled, you must update the following vSphere configuration settings manually:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						vCenter address
					</li><li class="listitem">
						vCenter cluster
					</li><li class="listitem">
						vCenter username
					</li><li class="listitem">
						vCenter password
					</li><li class="listitem">
						Data center
					</li><li class="listitem">
						Default data store
					</li><li class="listitem">
						Virtual machine folder
					</li></ul></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						The Assisted Installer has finished installing the cluster successfully.
					</li><li class="listitem">
						The cluster is connected to <a class="link mimir-link-warn" href="http://console.redhat.com" title="Mimir does not include content from: console.redhat.com">console.redhat.com</a>.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						In the Administrator perspective, navigate to <span class="strong strong"><strong>Home → Overview</strong></span>.
					</li><li class="listitem">
						Under <span class="strong strong"><strong>Status</strong></span>, click <span class="strong strong"><strong>vSphere connection</strong></span> to open the <span class="strong strong"><strong>vSphere connection configuration</strong></span> wizard.
					</li><li class="listitem">
						In the <span class="strong strong"><strong>vCenter</strong></span> field, enter the network address of the vSphere vCenter server. This can be either a domain name or an IP address. It appears in the vSphere web client URL; for example <code class="literal">https://[your_vCenter_address]/ui</code>.
					</li><li class="listitem"><p class="simpara">
						In the <span class="strong strong"><strong>vCenter cluster</strong></span> field, enter the name of the vSphere vCenter cluster where OpenShift Container Platform is installed.
					</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							This step is mandatory if you installed OpenShift Container Platform 4.13 or later.
						</p></div></div></li><li class="listitem">
						In the <span class="strong strong"><strong>Username</strong></span> field, enter your vSphere vCenter username.
					</li><li class="listitem"><p class="simpara">
						In the <span class="strong strong"><strong>Password</strong></span> field, enter your vSphere vCenter password.
					</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
							The system stores the username and password in the <code class="literal">vsphere-creds</code> secret in the <code class="literal">kube-system</code> namespace of the cluster. An incorrect vCenter username or password makes the cluster nodes unschedulable.
						</p></div></div></li><li class="listitem">
						In the <span class="strong strong"><strong>Datacenter</strong></span> field, enter the name of the vSphere data center that contains the virtual machines used to host the cluster; for example, <code class="literal">SDDC-Datacenter</code>.
					</li><li class="listitem"><p class="simpara">
						In the <span class="strong strong"><strong>Default data store</strong></span> field, enter the vSphere data store that stores the persistent data volumes; for example, <code class="literal">/SDDC-Datacenter/datastore/datastorename</code>.
					</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
							Updating the vSphere data center or default data store after the configuration has been saved detaches any active vSphere <code class="literal">PersistentVolumes</code>.
						</p></div></div></li><li class="listitem">
						In the <span class="strong strong"><strong>Virtual Machine Folder</strong></span> field, enter the data center folder that contains the virtual machine of the cluster; for example, <code class="literal">/SDDC-Datacenter/vm/ci-ln-hjg4vg2-c61657-t2gzr</code>. For the OpenShift Container Platform installation to succeed, all virtual machines comprising the cluster must be located in a single data center folder.
					</li><li class="listitem">
						Click <span class="strong strong"><strong>Save Configuration</strong></span>. This updates the <code class="literal">cloud-provider-config</code> file in the <code class="literal">openshift-config</code> namespace, and starts the configuration process.
					</li><li class="listitem">
						Reopen the <span class="strong strong"><strong>vSphere connection configuration</strong></span> wizard and expand the <span class="strong strong"><strong>Monitored operators</strong></span> panel. Check that the status of the operators is either <span class="strong strong"><strong>Progressing</strong></span> or <span class="strong strong"><strong>Healthy</strong></span>.
					</li></ol></div><div class="formalpara"><p class="title"><strong>Verification</strong></p><p>
					The connection configuration process updates operator statuses and control plane nodes. It takes approximately an hour to complete. During the configuration process, the nodes will reboot. Previously bound <code class="literal">PersistentVolumeClaims</code> objects might become disconnected.
				</p></div><p>
				Follow the steps below to monitor the configuration process.
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Check that the configuration process completed successfully:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
								In the Administrator perspective, navigate to <span class="strong strong"><strong>Home</strong></span> &gt; <span class="strong strong"><strong>Overview</strong></span>.
							</li><li class="listitem">
								Under <span class="strong strong"><strong>Status</strong></span> click <span class="strong strong"><strong>Operators</strong></span>. Wait for all operator statuses to change from <span class="strong strong"><strong>Progressing</strong></span> to <span class="strong strong"><strong>All succeeded</strong></span>. A <span class="strong strong"><strong>Failed</strong></span> status indicates that the configuration failed.
							</li><li class="listitem">
								Under <span class="strong strong"><strong>Status</strong></span>, click <span class="strong strong"><strong>Control Plane</strong></span>. Wait for the response rate of all Control Pane components to return to 100%. A <span class="strong strong"><strong>Failed</strong></span> control plane component indicates that the configuration failed.
							</li></ol></div><p class="simpara">
						A failure indicates that at least one of the connection settings is incorrect. Change the settings in the <span class="strong strong"><strong>vSphere connection configuration</strong></span> wizard and save the configuration again.
					</p></li><li class="listitem"><p class="simpara">
						Check that you are able to bind <code class="literal">PersistentVolumeClaims</code> objects by performing the following steps:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Create a <code class="literal">StorageClass</code> object using the following YAML:
							</p><pre class="programlisting language-yaml">kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
 name: vsphere-sc
provisioner: kubernetes.io/vsphere-volume
parameters:
 datastore: YOURVCENTERDATASTORE
 diskformat: thin
reclaimPolicy: Delete
volumeBindingMode: Immediate</pre></li><li class="listitem"><p class="simpara">
								Create a <code class="literal">PersistentVolumeClaims</code> object using the following YAML:
							</p><pre class="programlisting language-yaml">kind: PersistentVolumeClaim
apiVersion: v1
metadata:
 name: test-pvc
 namespace: openshift-config
 annotations:
   volume.beta.kubernetes.io/storage-provisioner: kubernetes.io/vsphere-volume
 finalizers:
   - kubernetes.io/pvc-protection
spec:
 accessModes:
   - ReadWriteOnce
 resources:
   requests:
    storage: 10Gi
 storageClassName: vsphere-sc
 volumeMode: Filesystem</pre></li></ol></div><p class="simpara">
						For instructions, see <a class="link mimir-link-warn" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.15/html-single/storage/index#about_dynamic-provisioning" title="This content is not included in Mimir.">Dynamic provisioning</a> in the OpenShift Container Platform documentation. To troubleshoot a <code class="literal">PersistentVolumeClaims</code> object, navigate to <span class="strong strong"><strong>Storage</strong></span> → <span class="strong strong"><strong>PersistentVolumeClaims</strong></span> in the <span class="strong strong"><strong>Administrator</strong></span> perspective of the OpenShift Container Platform web console.
					</p></li></ol></div></section></section><section class="chapter" id="installing-on-oci"><div class="titlepage"><div><div><h1 class="title">Chapter 15. Optional: Installing on Oracle Cloud Infrastructure (OCI)</h1></div></div></div><p>
			From OpenShift Container Platform 4.14 and later versions, you can use the Assisted Installer to install a cluster on Oracle Cloud Infrastructure by using infrastructure that you provide. Oracle Cloud Infrastructure provides services that can meet your needs for regulatory compliance, performance, and cost-effectiveness. You can access OCI Resource Manager configurations to provision and configure OCI resources.
		</p><p>
			This section is a summary of the steps required in the Assisted Installer web console to support the integration with Oracle Cloud Infrastructure. It does not document the steps to be performed in Oracle Cloud Infrastructure, nor does it cover the integration between the two platforms. For a complete and comprehensive procedure, see <a class="link mimir-link-warn-approx" href="/products/#letter-O" title="Mimir does not include this page, but the link has been rewritten to point to the nearest parent document.">Using the Assisted Installer to install a cluster on OCI</a>.
		</p><section class="section" id="oci_generating-discovery-iso_installing-on-oci"><div class="titlepage"><div><div><h2 class="title">15.1. Generating an OCI-compatible discovery ISO image</h2></div></div></div><p>
				Generate the discovery ISO image in Assisted Installer by completing the required steps. You must upload the image to the Oracle Cloud Infrastructure before you install OpenShift Container Platform on Oracle Cloud Infrastructure.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You created a child compartment and an object storage bucket on Oracle Cloud Infrastructure. See <a class="link mimir-link-warn-approx" href="/products/#letter-O" title="Mimir does not include this page, but the link has been rewritten to point to the nearest parent document.">Creating OCI resources and services</a> in the OpenShift Container Platform documentation.
					</li><li class="listitem">
						You meet the requirements necessary for installing a cluster. For details, see <a class="link" href="#prerequisites" title="Chapter 2. Prerequisites">Prerequisites</a>.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Log in to the <a class="link mimir-link-warn" href="https://console.redhat.com/openshift" title="Mimir does not include content from: console.redhat.com">Red Hat Hybrid Cloud Console</a>.
					</li><li class="listitem">
						Click <span class="strong strong"><strong>Create cluster</strong></span>.
					</li><li class="listitem">
						On the <span class="strong strong"><strong>Cluster type</strong></span> page, click the <span class="strong strong"><strong>Datacenter</strong></span> tab.
					</li><li class="listitem">
						In the <span class="strong strong"><strong>Assisted Installer</strong></span> section, click <span class="strong strong"><strong>Create cluster</strong></span>.
					</li><li class="listitem"><p class="simpara">
						On the <span class="strong strong"><strong>Cluster Details</strong></span> page, complete the following fields:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
								In the <span class="strong strong"><strong>Cluster name</strong></span> field, specify the name of your cluster, such as <code class="literal">ocidemo</code>.
							</li><li class="listitem">
								In the <span class="strong strong"><strong>Base domain</strong></span> field, specify the base domain of the cluster, such as <code class="literal">splat-oci.devcluster.openshift.com</code>. Take the base domain from OCI after creating a compartment and a zone.
							</li><li class="listitem">
								In the <span class="strong strong"><strong>OpenShift version</strong></span> field, specify OpenShift 4.15 or a later version.
							</li><li class="listitem">
								In the <span class="strong strong"><strong>CPU architecture</strong></span> field, specify <code class="literal">x86_64</code> or <code class="literal">Arm64</code>.
							</li><li class="listitem">
								From the <span class="strong strong"><strong>Integrate with external partner platforms</strong></span> list, select <span class="strong strong"><strong>Oracle Cloud Infrastructure</strong></span>. The <span class="strong strong"><strong>Include custom manifests</strong></span> checkbox is automatically selected.
							</li></ol></div></li><li class="listitem">
						Click <span class="strong strong"><strong>Next</strong></span>.
					</li><li class="listitem">
						On the <span class="strong strong"><strong>Operators</strong></span> page, click <span class="strong strong"><strong>Next</strong></span>.
					</li><li class="listitem"><p class="simpara">
						On the <span class="strong strong"><strong>Host Discovery</strong></span> page, perform the following actions:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
								Click <span class="strong strong"><strong>Add host</strong></span> to display a dialog box.
							</li><li class="listitem">
								For the <span class="strong strong"><strong>SSH public key</strong></span> field, upload a public SSH key from your local system. You can generate an SSH key pair with <code class="literal">ssh-keygen</code>.
							</li><li class="listitem">
								Click <span class="strong strong"><strong>Generate Discovery ISO</strong></span> to generate the discovery image ISO file.
							</li><li class="listitem">
								Download the file to your local system. You will then upload the file to the bucket in Oracle Cloud Infrastructure as an Object.
							</li></ol></div></li></ol></div></section><section class="section" id="oci-assigning-roles-and-manifests_installing-on-oci"><div class="titlepage"><div><div><h2 class="title">15.2. Assigning node roles and custom manifests</h2></div></div></div><p>
				After you provision Oracle Cloud Infrastructure (OCI) resources and upload OpenShift Container Platform custom manifest configuration files to OCI, you must complete the remaining cluster installation steps on the Assisted Installer before you can create an instance OCI.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You created a resource stack on OCI, and the stack includes the custom manifest configuration files and OCI Resource Manager configuration resources. For details, see <a class="link mimir-link-warn-approx" href="/products/#letter-O" title="Mimir does not include this page, but the link has been rewritten to point to the nearest parent document.">Downloading manifest files and deployment resources</a> in the OpenShift Container Platform documentation.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						From the <a class="link mimir-link-warn" href="https://console.redhat.com/openshift" title="Mimir does not include content from: console.redhat.com">Red Hat Hybrid Cloud Console</a>, go to the <span class="strong strong"><strong>Host discovery</strong></span> page.
					</li><li class="listitem">
						Under the <span class="strong strong"><strong>Role</strong></span> column, assign a node role, either <span class="strong strong"><strong>Control plane node</strong></span> or <span class="strong strong"><strong>Worker</strong></span>, for each targeted hostname. Click <span class="strong strong"><strong>Next</strong></span>.
					</li><li class="listitem">
						Accept the default settings for the <span class="strong strong"><strong>Storage</strong></span> and <span class="strong strong"><strong>Networking</strong></span> pages.
					</li><li class="listitem">
						Click <span class="strong strong"><strong>Next</strong></span> to go to the <span class="strong strong"><strong>Custom manifests</strong></span> page.
					</li><li class="listitem">
						In the <span class="strong strong"><strong>Folder</strong></span> field, select <code class="literal">manifests</code>.
					</li><li class="listitem">
						In the <span class="strong strong"><strong>File name</strong></span> field, enter a value such as <code class="literal">oci-ccm.yml</code>.
					</li><li class="listitem">
						In the <span class="strong strong"><strong>Content</strong></span> section, click <span class="strong strong"><strong>Browse</strong></span>. Select the CCM manifest located in <code class="literal">custom_ manifest/manifests/oci-ccm.yml</code>.
					</li><li class="listitem"><p class="simpara">
						Click <span class="strong strong"><strong>Add another manifest</strong></span>. Repeat the same steps for the following manifests provided by Oracle:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								CSI driver manifest: <code class="literal">custom_ manifest/manifests/oci-csi.yml</code>.
							</li><li class="listitem">
								CCM machine configuration: <code class="literal">custom_ manifest/openshift/machineconfig-ccm.yml</code>.
							</li><li class="listitem">
								CSI driver machine configuration: <code class="literal">custom_ manifest/openshift/machineconfig-csi.yml</code>.
							</li></ul></div></li><li class="listitem">
						Complete the <span class="strong strong"><strong>Review and create</strong></span> step to create your OpenShift Container Platform cluster on OCI.
					</li><li class="listitem">
						Click <span class="strong strong"><strong>Install cluster</strong></span> to finalize the cluster installation.
					</li></ol></div></section></section><section class="chapter" id="assembly_troubleshooting"><div class="titlepage"><div><div><h1 class="title">Chapter 16. Troubleshooting</h1></div></div></div><p>
			There are cases where the Assisted Installer cannot begin the installation or the cluster fails to install properly. In these events, it is helpful to understand the likely failure modes as well as how to troubleshoot the failure.
		</p><section class="section" id="troubleshooting-discovery-iso-issues"><div class="titlepage"><div><div><h2 class="title">16.1. Troubleshooting discovery ISO issues</h2></div></div></div><p>
				The Assisted Installer uses an ISO image to run an agent that registers the host to the cluster and performs hardware and network validations before attempting to install OpenShift. You can follow these procedures to troubleshoot problems related to the host discovery.
			</p><p>
				Once you start the host with the discovery ISO image, the Assisted Installer discovers the host and presents it in the Assisted Service web console. See <a class="link" href="#assembly_configuring-the-discovery-image" title="Chapter 7. Configuring the discovery image">Configuring the discovery image</a> for additional details.
			</p><section class="section" id="proc_verify-the-discovery-agent-is-running_troubleshooting"><div class="titlepage"><div><div><h3 class="title">16.1.1. Verify the discovery agent is running</h3></div></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have created an infrastructure environment by using the API or have created a cluster by using the web console.
						</li><li class="listitem">
							You booted a host with the Infrastructure Environment discovery ISO and the host failed to register.
						</li><li class="listitem">
							You have SSH access to the host.
						</li><li class="listitem">
							You provided an SSH public key in the "Add hosts" dialog before generating the Discovery ISO so that you can SSH into your machine without a password.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Verify that your host machine is powered on.
						</li><li class="listitem">
							If you selected <span class="strong strong"><strong>DHCP networking</strong></span>, check that the DHCP server is enabled.
						</li><li class="listitem">
							If you selected <span class="strong strong"><strong>Static IP, bridges and bonds</strong></span> networking, check that your configurations are correct.
						</li><li class="listitem"><p class="simpara">
							Verify that you can access your host machine using SSH, a console such as the BMC, or a virtual machine console:
						</p><pre class="programlisting language-terminal">$ ssh core@&lt;host_ip_address&gt;</pre><p class="simpara">
							You can specify private key file by using the <code class="literal">-i</code> parameter if it is not stored in the default directory.
						</p><pre class="programlisting language-terminal">$ ssh -i &lt;ssh_private_key_file&gt; core@&lt;host_ip_address&gt;</pre><p class="simpara">
							If you fail to connect over SSH to the host, the host failed during boot or it failed to configure the network.
						</p><p class="simpara">
							Upon login you should see this message:
						</p><div class="formalpara"><p class="title"><strong>Example login</strong></p><p>
								<span class="inlinemediaobject"><img src="/webassets/avalon/d/Assisted_Installer_for_OpenShift_Container_Platform-2024-Installing_OpenShift_Container_Platform_with_the_Assisted_Installer-en-US/images/90114925b822ae76534f2b850d9ac82e/assisted-iso-login.png" alt="screenshot of ISO login message" /></span>

							</p></div></li></ol></div><p>
					If you are not seeing this message it means that the host did not boot with the Assisted Installer ISO image. Make sure you configured the boot order properly (The host should boot once from the live-ISO).
				</p><p>
					+ . Check the agent service logs:
				</p><p>
					+
				</p><pre class="programlisting language-terminal">$ sudo journalctl -u agent.service</pre><p>
					+ In the following example, the errors indicate there is a network issue:
				</p><p>
					+ .Example agent service log screenshot of agent service log 
					<span class="inlinemediaobject"><img src="/webassets/avalon/d/Assisted_Installer_for_OpenShift_Container_Platform-2024-Installing_OpenShift_Container_Platform_with_the_Assisted_Installer-en-US/images/0e1b5d237dbac68e235fce19c7507ebe/agent-service-log.png" alt="screenshot of agent service log" /></span>

				</p><p>
					+ If there is an error pulling the agent image, check the proxy settings. Verify that the host is connected to the network. You can use <code class="literal">nmcli</code> to get additional information about your network configuration.
				</p></section><section class="section" id="proc_verify-the-discovery-agent-can-access-the-assisted-service_troubleshooting"><div class="titlepage"><div><div><h3 class="title">16.1.2. Verify the agent can access the assisted-service</h3></div></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have created an Infrastructure Environment by using the API or have created a cluster by using the web console.
						</li><li class="listitem">
							You booted a host with the Infrastructure Environment discovery ISO and the host failed to register.
						</li><li class="listitem">
							You verified the discovery agent is running.
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Check the agent logs to verify the agent can access the Assisted Service:
						</p><pre class="programlisting language-terminal">$ sudo journalctl TAG=agent</pre><p class="simpara">
							The errors in the following example indicate that the agent failed to access the Assisted Service.
						</p><div class="formalpara"><p class="title"><strong>Example agent log</strong></p><p>
								<span class="inlinemediaobject"><img src="/webassets/avalon/d/Assisted_Installer_for_OpenShift_Container_Platform-2024-Installing_OpenShift_Container_Platform_with_the_Assisted_Installer-en-US/images/438b410fadf201514feb7199f629d68a/agent-log-failed-to-access-assisted-service.png" alt="screenshot of the agent log failing to access the Assisted Service" /></span>

							</p></div><p class="simpara">
							Check the proxy settings you configured for the cluster. If configured, the proxy must allow access to the Assisted Service URL.
						</p></li></ul></div></section></section><section class="section" id="con_minimal-iso-image_troubleshooting"><div class="titlepage"><div><div><h2 class="title">16.2. Troubleshooting minimal discovery ISO issues</h2></div></div></div><p>
				The minimal ISO image should be used when bandwidth over the virtual media connection is limited. It includes only what is required to boot a host with networking. The majority of the content is downloaded upon boot. The resulting ISO image is about 100MB in size compared to 1GB for the full ISO image.
			</p><section class="section" id="interrupting-boot-before-mount-rootfs_troubleshooting"><div class="titlepage"><div><div><h3 class="title">16.2.1. Troubleshooting minimal ISO boot failure by interrupting the boot process</h3></div></div></div><p>
					If your environment requires static network configuration to access the Assisted Installer service, any issues with that configuration might prevent the minimal ISO from booting properly. If the boot screen shows that the host has failed to download the root file system image, the network might not be configured correctly.
				</p><p>
					You can interrupt the kernel boot early in the bootstrap process, before the root file system image is downloaded. This allows you to access the root console and review the network configurations.
				</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">rootfs</code> download failure</strong></p><p>
						<span class="inlinemediaobject"><img src="/webassets/avalon/d/Assisted_Installer_for_OpenShift_Container_Platform-2024-Installing_OpenShift_Container_Platform_with_the_Assisted_Installer-en-US/images/d1b0aed9c5eba580249c32cb8a4bdd0b/rootfs-download-failure.png" alt="Failed root file system image download" /></span>

					</p></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Add the <code class="literal">.spec.kernelArguments</code> stanza to the <code class="literal">infraEnv</code> object of the cluster you are deploying:
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								For details on modifying an infrastructure environment, see <span class="emphasis"><em>Additional Resources</em></span>.
							</p></div></div><pre class="programlisting language-terminal"># ...
spec:
  clusterRef:
    name: sno1
    namespace: sno1
  cpuArchitecture: x86_64
  ipxeScriptType: DiscoveryImageAlways
  kernelArguments:
  - operation: append
    value: rd.break=initqueue <span id="CO25-1"><!--Empty--></span><span class="callout">1</span>
  nmStateConfigLabelSelector:
    matchLabels:
      nmstate-label: sno1
  pullSecretRef:
    name: assisted-deployment-pull-secret</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO25-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									<code class="literal">rd.break=initqueue</code> interrupts the boot at the <code class="literal">dracut</code> main loop. See <a class="link mimir-link-warn" href="https://access.redhat.com/solutions/2382221" title="This content is not included in Mimir.">rd.break options for debugging kernel boot</a> for details.
								</div></dd></dl></div></li><li class="listitem">
							Wait for the related nodes to reboot automatically and for the boot to stop at the <code class="literal">iniqueue</code> stage, before <code class="literal">rootfs</code> is downloaded. You will be redirected to the root console.
						</li><li class="listitem"><p class="simpara">
							Identify and change the incorrect network configurations. Here are some useful diagnostic commands:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									View system logs by using <code class="literal">journalctl</code>, for example:
								</p><pre class="programlisting language-terminal"># journalctl -p err //Sorts logs by errors
# journalctl -p crit //Sorts logs by critical errors
# journalctl -p warning //Sorts logs by warnings</pre></li><li class="listitem"><p class="simpara">
									View network connection information by using <code class="literal">nmcli</code>, as follows:
								</p><pre class="programlisting language-terminal"># nmcli conn show</pre></li><li class="listitem"><p class="simpara">
									Check the configuration files for incorrect network connections, for example:
								</p><pre class="programlisting language-terminal"># cat /etc/assisted/network/host0/eno3.nmconnection</pre></li></ol></div></li><li class="listitem">
							Press <code class="literal">control+d</code> to resume the bootstrap process. The server downloads <code class="literal">rootfs</code> and completes the process.
						</li><li class="listitem">
							Reopen the <code class="literal">infraEnv</code> object and remove the <code class="literal">.spec.kernelArguments</code> stanza.
						</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="#modifying-an-infrastructure-environment_installing-with-api" title="4.8. Modifying an infrastructure environment">Modifying an infrastructure environment</a>
						</li></ul></div></section></section><section class="section" id="proc_incorrect-boot-order_troubleshooting"><div class="titlepage"><div><div><h2 class="title">16.3. Correcting a host’s boot order</h2></div></div></div><p>
				Once the installation that runs as part of the Discovery Image completes, the Assisted Installer reboots the host. The host must boot from its installation disk to continue forming the cluster. If you have not correctly configured the host’s boot order, it will boot from another disk instead, interrupting the installation.
			</p><p>
				If the host boots the discovery image again, the Assisted Installer will immediately detect this event and set the host’s status to <span class="strong strong"><strong>Installing Pending User Action</strong></span>.  Alternatively, if the Assisted Installer does not detect that the host has booted the correct disk within the allotted time, it will also set this host status.
			</p><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Reboot the host and set its boot order to boot from the installation disk. If you didn’t select an installation disk, the Assisted Installer selected one for you. To view the selected installation disk, click to expand the host’s information in the host inventory, and check which disk has the “Installation disk” role.
					</li></ul></div></section><section class="section" id="proc_partially-successful-installations_troubleshooting"><div class="titlepage"><div><div><h2 class="title">16.4. Rectifying partially-successful installations</h2></div></div></div><p>
				There are cases where the Assisted Installer declares an installation to be successful even though it encountered errors:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						If you requested to install OLM operators and one or more failed to install, log in to the cluster’s console to remediate the failures.
					</li><li class="listitem">
						If you requested to install more than two worker nodes and at least one failed to install, but at least two succeeded, add the failed workers to the installed cluster.
					</li></ul></div></section><section class="section" id="api-connectivity-failure_troubleshooting"><div class="titlepage"><div><div><h2 class="title">16.5. API connectivity failure when adding nodes to a cluster</h2></div></div></div><p>
				When you add a node to an existing cluster as part of Day 2 operations, the node downloads the ignition configuration file from the Day 1 cluster. If the download fails and the node is unable to connect to the cluster, the status of the host in the <span class="strong strong"><strong>Host discovery</strong></span> step changes to <span class="strong strong"><strong>Insufficient</strong></span>. Clicking this status displays the following error message:
			</p><pre class="programlisting language-terminal">The host failed to download the ignition file from &lt;URL&gt;. You must ensure the host can reach the URL. Check your DNS and network configuration or update the IP address or domain used to reach the cluster.

error: ignition file download failed.... no route to host</pre><p>
				There are several possible reasons for the connectivity failure. Here are some recommended actions.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Check the IP address and domain name of the cluster:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
								Click the <span class="strong strong"><strong>set the IP or domain used to reach the cluster</strong></span> hyperlink.
							</li><li class="listitem">
								In the <span class="strong strong"><strong>Update cluster hostname</strong></span> window, enter the correct IP address or domain name for the cluster.
							</li></ol></div></li><li class="listitem">
						Check your DNS settings to ensure that the DNS can resolve the domain that you provided.
					</li><li class="listitem">
						Ensure that port <code class="literal">22624</code> is open in all firewalls.
					</li><li class="listitem"><p class="simpara">
						Check the agent logs of the host to verify that the agent can access the Assisted Service via SSH:
					</p><pre class="programlisting language-terminal">$ sudo journalctl TAG=agent</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							For more details, see <a class="link" href="#proc_verify-the-discovery-agent-can-access-the-assisted-service_troubleshooting" title="16.1.2. Verify the agent can access the assisted-service">Verify the agent can access the Assisted Service</a>.
						</p></div></div></li></ol></div></section></section><div><div xml:lang="en-US" class="legalnotice" id="idm139800925236160"><h1 class="legalnotice">Legal Notice</h1><div class="para">
		Copyright <span class="trademark"><!--Empty--></span>© 2024 Red Hat, Inc.
	</div><div class="para">
		The text of and illustrations in this document are licensed by Red Hat under a Creative Commons Attribution–Share Alike 3.0 Unported license ("CC-BY-SA"). An explanation of CC-BY-SA is available at <a class="uri mimir-link-warn" href="http://creativecommons.org/licenses/by-sa/3.0/" title="Mimir does not include content from: creativecommons.org">http://creativecommons.org/licenses/by-sa/3.0/</a>. In accordance with CC-BY-SA, if you distribute this document or an adaptation of it, you must provide the URL for the original version.
	</div><div class="para">
		Red Hat, as the licensor of this document, waives the right to enforce, and agrees not to assert, Section 4d of CC-BY-SA to the fullest extent permitted by applicable law.
	</div><div class="para">
		Red Hat, Red Hat Enterprise Linux, the Shadowman logo, the Red Hat logo, JBoss, OpenShift, Fedora, the Infinity logo, and RHCE are trademarks of Red Hat, Inc., registered in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Linux</span>® is the registered trademark of Linus Torvalds in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Java</span>® is a registered trademark of Oracle and/or its affiliates.
	</div><div class="para">
		<span class="trademark">XFS</span>® is a trademark of Silicon Graphics International Corp. or its subsidiaries in the United States and/or other countries.
	</div><div class="para">
		<span class="trademark">MySQL</span>® is a registered trademark of MySQL AB in the United States, the European Union and other countries.
	</div><div class="para">
		<span class="trademark">Node.js</span>® is an official trademark of Joyent. Red Hat is not formally related to or endorsed by the official Joyent Node.js open source or commercial project.
	</div><div class="para">
		The <span class="trademark">OpenStack</span>® Word Mark and OpenStack logo are either registered trademarks/service marks or trademarks/service marks of the OpenStack Foundation, in the United States and other countries and are used with the OpenStack Foundation's permission. We are not affiliated with, endorsed or sponsored by the OpenStack Foundation, or the OpenStack community.
	</div><div class="para">
		All other trademarks are the property of their respective owners.
	</div></div></div></div></body>

        
        
    </div>
    

</div> 


                            </bdo>
                        </main>
                    </div>
                </main>
            </div>
        
            <!--#include virtual="/includes/footer/index.html" -->
        </div>
    </body>
</html>
